% \newpage
% \chapter*{Conclusion}
% \addcontentsline{toc}{chapter}{Conclusion}
The main objective of this thesis has been to investigate Bayesian neural networks sampled from the exact posterior as a substitute for direct calculations of next-to-leading order cross sections in quantum field theory. We argued for the necessity of bypassing the computationally expensive direct calculations with Bayesian regression because of the need for an estimate of the uncertainty of the predicted cross sections. This was due to the role the uncertainties play in the statistical exclusion of regions in parameter spaces of possible Beyond the Standard Model theories. The neural network model was chosen for its universal function approximation property. We provided an overview of Bayesian machine learning and its relation to classical machine learning with a focus on regression tasks. We sought to use Markov chain Monte Carlo methods to sample from the exact posterior of neural networks and thus delved into the theory behind these methods for continuous sample spaces, building our way through the Metropolis-Hastings methods all the way up to the advanced class of samplers used in this thesis, namely Hamiltonian Monte Carlo. We discussed the shortcomings of Hamiltonian Monte Carlo with a fixed trajectory length and its need for tedious hand-tuning via preliminary diagnostic tests. This motivated the exploration of adaptive Hamiltonian Monte Carlo techniques to adapt the trajectory length with the No-U-Turn sampler for the number of Leapfrog steps and a dual-averaging scheme for the step size used in the Leapfrog integration component. 

Through numerical experiments, we demonstrated that trained BNNs can significantly reduce the time spent computing cross sections. We found that the time spent on such computations were roughly evenly divided between loading the models in from memory and performing the actual forward pass in the neural networks to compute the predictive distribution, its sample mean and sample variance. Moreover, we found that the training employed on GPUs with XLA compilation can result in a significant reduction in training time compared to training on CPUs.
We investigated the empirical posterior distribution of the sampled weights and found them to be multi-modal, consistent with claims in the literature, sowing doubt of the reliability of Bayesian inference using surrogate models. We explored how various hyperparameters used during training of BNNs affect their predictive performance. We found evidence suggesting that a moderate amount of warm-up steps and pretraining positively impacts the performance of the trained models but that an excessive amount exacerbated it. With the vast set of different configurations one can use with BNNs though, these results may not be generalizable to different forms of architectures than the ones we have used and more extensive investigations can be carried out. Some of the configurations we explored were found to produce reliable uncertainty estimates and performed well in the space of targets it was trained for. Finally, we explored the predictive distributions of a trained BNN model and showcased an example of a good predictive distribution and a predictive distribution which missed the mark entirely. We showed that the BNN model underperformed relative to a Normal distribution where less than 96 \% of the targets resided within $\pm 2\sigma$ of the sample mean of the distributions. 

Although we have progressed our understanding of the training of BNNs by drawing samples from the exact posterior with MCMC samplers like HMC and NUTS, there are several question which we have not answered. We propose the following problems to be addressed in the future.
\begin{enumerate}
    \item \textbf{The Convergence Properties of the Markov Chain}. In chapter \ref{chap:mcmc}, we noted that the standard metric to estimate that a Markov chain has converged to its stationary distribution were by use of the scale reduction factor $\hat{R}$. Such convergence statistics is not measured or reported in this thesis. Computing $\hat{R}$ for neural network posteriors is complicated by the non-identifiability of neural networks. Several different neural networks sampled from the different regions of sample space may produce the same predictions, which makes assessing the convergence by studying the elements of the Markov chain itself challenging. Instead we propose the use of $\hat{R}$ computed on the predictions by using the samples in the Markov chain. This analysis has been performed in \cite{google_bnn_posteriors} where they study the resulting Markov chain in function space. For a specific input $x$, one can run several independent chains that all compute their own predictive distribution. The $\hat{R}$ metric can then be calculated from the resulting set of distributions to identify potential non-convergence.
    \item \textbf{Training of BNNs on Larger Datasets}. In this thesis, we have focused on a fairly small dataset of $\sim 15000$ datapoints. At no point have we investigated the added computational expense from computing the potential energy function and its gradient in HMC and NUTS as a function of the number of datapoints it needs to be evaluated for. If NLO cross section estimation is to be used with BNNs on larger datasets, the effect it has on the hyperparameters used during training is likely necessary to be reinvestigated. The analysis performed in this thesis should at least give information on what hyperparameters that are worth exploring. Training time will likely be much longer but the predictive performance of the trained BNN may become more robust.
    \item \textbf{Sampling Larger Models}. Our analysis has been dealing with a fairly small number of sampled neural networks per model. In each case, we have drawn 1000 neural networks which collectively represented the full BNN model. The number of parameters the models had, spanned from a few hundred to the order of a hundred thousand. A thousand samples drawn from the posterior is a pretty low number owed to the computational expense needed to generate them. For the most complex models trained, the training time could exceed 24 hours on a GPU. Thus drawing more samples by running longer Markov chains can be exceedingly expensive. The potential upside is that the MCMC estimators and the predictive distributions will likely produce better results if more samples are drawn. An important consideration to reduce the training time is to increase the complexity of neural networks by adding many small layers and create deep networks as opposed to shallow networks with many nodes per layer. The reason for this is that applying shallow ``wide'' networks to a large dataset simultaneously during training may create temporary objects which are too large to fit into cache and registers which results in additional time spent transferring strips of memory, in which case the GPU may sit idle during large portions of the training.
    \item \textbf{The Effect of Thinning}. We have operated with a fixed number of samples skipped between each drawn network. This means that we have performed no analysis of the correlation between successively drawn samples but instead worked with a heuristic that appeared to produce good results. Investigating the \textit{lag}-$l$ autocorrelation of successive neural network samples can give valuable information from a practical perspective. Although drawing more samples may be beneficial for the calculation of MCMC estimators, it is not so if the samples are heavily correlated. Both samplers used in this thesis generate successive samples with low correlation in simple cases studied in the literature \cite{nuts,neal2011} but with the complexity of the BNN posterior, this may require a larger amount of thinning. Performing preliminary runs to estimate how correlated successive samples are will help reduce the necessary amount of samples needed to be drawn to obtain good statistics from the MCMC estimators. It will also help the practitioner to minimize the amount of thinning and avoid wasting computational resources.
    \item \textbf{The Effect of the Multi-modality of the Posterior on the Predictive Distributions}. Although we demonstrated the multi-modality of the posterior distribution of BNNs, we did not investigate its effect on the predictive distribution. After all, it is the predictive distribution we really care about in practice. Due to how computationally expensive it is to sample from the exact posterior, a thorough comparison of sampling from the exact posterior should be compared and contrasted with the use of surrogate distributions for the BNNs parameters with respect to the quality of the predictive distributions they produce.
    \item \textbf{Other Potential Energy Functions}. In our investigation we have used a Gaussian prior for each neural network parameter and the same likelihood function for each model. It is possible that modifying the potential energy function, either by choosing different priors or modifying the likelihood function, that the training process can be improved. It has been suggested that the effect of the chosen priors may yield a measurable impact on the predictive distribution although it may not be particularly noticable from studing the posterior distribution of weights \cite{google_bnn_posteriors}.
    \item \textbf{Deep Ensembles}. Deep ensembles has been shown to yield a better fidelity of the Bayesian predictive distribution on par with the ones produced by HMC, outperforming the surrogate distributions typically employed in the literature \cite{google_bnn_posteriors}. And this can potentially be achieved at a significant reduction of the computational cost of sampling from the true posterior using HMC or NUTS. 
    \item \textbf{Multi-GPU Training with HMC}. In this thesis, within the framework we used, we were confined to run the sampling on a single GPU device. Investigating the possibility of running multiple independent Markov chains on several GPUs simultaneously in an asynchronous fashion can potentially speed up the training of BNNs with HMC significantly by allowing for many long chains to sample independently of each other. The quality of the sampled chains from the posterior is likely to improve by generating more than a single chain. One possible framework to adopt for this is Jax \cite{jax}, a machine learning research library developed and used in-house by the Deep Mind research team at Google Research. It allows for automatic mapping of a Python function to several physical devices (such as multiple GPUs), just-in-time compilation for GPU devices which when run on NVIDIA GPUs support XLA compilation. Moreover, it provides its own framework for automatic differentiation. Thus it may be a viable platform to develop more research oriented machine learning models than the more strict frameworks provided by TensorFlow and its extensions. Due to the complex control flow introduced by the NUTS sampler, it is not as well suited for multi-GPU sampling but other adaptive schemes such as ChEES-HMC was recently proposed to adaptively set the trajectory length without the same limitation \cite{chees-hmc}.
    \item \textbf{Approximate the Prior From the Empirical Distribution for Online Training}. A major drawback to drawing samples from the exact distribution is that we in practice must discard old models when new data becomes available. This is unfortunate as we obviously know \textit{something} about the weights inferred from the dataset prior to the arrival of the hypothetical new data. We are closely guarding a set of them which we have drawn, informed by the ``old'' data. In order to increase the viability of sampling from the exact posterior, investigating ways to obtain approximate unnormalized densities that approximate the empirical distributions can be fruitful for the following reason. The potential energy function used in HMC and NUTS \textit{requires} the ability to evaluate a log prior given an arbitrary input of weights. Thus inferring a function that approximates the empirical distribution will yield an \textit{informed} prior that can be evaluated to draw new samples given new data. This can greatly increase the viability of the methods as old samples can inform the generation of new ones. 
\end{enumerate}