% \newpage
% \chapter*{Conclusion}
% \addcontentsline{toc}{chapter}{Conclusion}
The main objective of this thesis has been to investigate Bayesian neural networks sampled from the exact posterior as a substitute for direct calculations of next-to-leading order cross sections in quantum field theory. We argued for the necessity of bypassing the computationally expensive direct calculations with Bayesian regression because of the need for an estimate of the uncertainty of the predicted cross sections due to the role the uncertainty plays in the statistical exclusion of regions in parameter spaces of possible Beyond the Standard Model theories. The neural network model was chosen for its universal function approximation property. We provided an overview of Bayesian machine learning and its relation to classical machine learning with a focus on regression tasks. We sought to use Markov chain Monte Carlo methods to sample from the exact posterior of neural networks and thus delved into the theory behind these methods for continuous sample spaces, building our way through the Metropolis-Hastings methods all the way up to the advanced class of samplers used in this thesis, namely Hamiltonian Monte Carlo. We discussed the shortcomings up Hamiltonian Monte Carlo with a fixed trajectory length and its need for tedious hand-tuning via preliminary diagnostic tests. This motivated the exploration of adaptive Hamiltonian Monte Carlo techniques to adapt the trajectory length with the No-U-Turn sampler for the number of Leapfrog steps and a dual-averaging scheme for the step size used in the Leapfrog integration component. 

Through numerical experiments, we demonstrated that trained BNNs can significantly reduce the time spent computing cross sections. We found that the time spent on such computations were roughly evenly divided between loading the models in from memory and performing the actual forward pass in the neural networks to compute the predictive distribution, its sample mean and sample variance. Moreover, we found that the training employed on GPUs with XLA compilation can result in a significant reduction in training time.
We investigated the posterior distribution of the true sampled weights and found them to be multi-modal, consistent with claims in the literature, sowing doubt of the reliability of Bayesian inference using surrogate models. We explored how various hyperparameters used during training of BNNs affect their predictive performance. We found evidence suggesting that a moderate amount of warm-up steps and pretraining positively impacts the performance of the trained models but that an excessive amount exacerbated it. With the vast set of different configurations one can use with BNNs though, these results may not be generalizable to different forms of architectures than the ones we have used and more extensive investigations can be carried out. Finally, we explored the predictive distributions of a trained BNN model and showcased an example of a good predictive distribution and a predictive distribution which missed the mark entirely. We showed that the BNN model underperformed relative to a Normal distribution where less than 96 \% of the targets resided within $\pm 2\sigma$ of the sample mean of the distributions. 

Although we have progressed our understanding of the training of BNNs by drawing sampling from the exact posterior with MCMC samplers like HMC and NUTS, there are several question which we have not answered. We propose the following problems to be addressed in the future.
\begin{enumerate}
    \item \textbf{The Convergence Properties of the Markov Chain}. In chapter \ref{chap:mcmc}, we noted that the standard metric to estimate that a Markov chain has converged to its stationary distribution were by use of the scale reduction factor $\hat{R}$. Such convergence statistics is not measured or reported in this thesis. Computing $\hat{R}$ for neural network posteriors is complicated by the non-identifiability of neural networks. Several different neural networks sampled from the same sample space may produce the same predictions, which makes assessing the convergence by studying the elements of the Markov chain itself challenging. Instead we propose the use of $\hat{R}$ computed on the predictions by using the samples in the Markov chain. 
    \item \textbf{Training of BNNs on Larger Datasets}. In this thesis, we have focused on a fairly small dataset of $\sim 16000$ datapoints. At no points have we investigated the added computational expense from computing the potential energy function and its gradient in HMC and NUTS as a function of number of datapoints it needs to be evaluated for. If NLO cross section estimation is to be used with BNNs on larger datasets, the effect it has on the hyperparameters used during training is likely necessary to be redone. The analysis performed in this thesis should at least give information on what hyperparameters that are worth exploring. Training time will likely be much longer but the predictive performance of the trained BNN may become more robust.
    \item \textbf{Sampling Larger Models}. Our analysis has been dealing with a fairly small number of sampled neural networks per model. In each case, we have drawn 1000 neural networks which collectively represented the full BNN model. The number of parameters the models had, spanned from a few hundred to a hundred thousand. A thousand samples drawn from the posterior is a pretty low number owed to the computational expense needed to generate them. The MCMC estimators and the predictive distributions will likely produce better results if more samples are drawn.
    \item \textbf{The Effect of Thinning}. We have operated with a fixed number of sampled skipped between each drawn network. This means that we have performed no analysis of the correlation between successively drawn samples but instead worked with a heuristic that appeared to produce good results. Investigating the \textit{lag}-$l$ autocorrelation successive neural network samples can give valuable information from a practical perspective. Although drawing more samples may be beneficial for the calculation of MCMC estimators, it is not so if the samples are heavily correlated. Both samplers used in this thesis generate successive samples with low correlation in simple cases studied in the literature \cite{nuts,neal2011} but with the complexity of the BNN posterior, this may require a larger amount of thinning. Performing preliminary runs to estimate how correlated succesive samples are will help reduce the necessary amount of sampled needed to be drawn to obtain good statistics from the MCMC estimators. It will also help the practitioner to minimize the amount of thinning and avoid wasting computational resources.
    \item \textbf{The Effect of the Multi-modality of the Posterior on the Predictive Distributions}. Althoug we demonstrated the multi-modality of the posterior distribution of BNNs, we did not investigate its effect on the predictive distribution. After all, it is the predictive distribution we really care about in practice. Due to how computationally expensive it is to sample from the exact posterior, a thorough comparison of sampling from the exact posterior should be compared and contrasted with the use of surrogate distributions for the BNNs parameters. 
    \item \textbf{Other Potential Energy Functions}. In our investigation we have used a Gaussian prior for each neural network parameter and the same likelihood function for each model. It is possible that modifying the potential energy function, either by choosing different priors or modifying the likelihood function, that the training process can be improved. It has been suggested that the effect of the chosen priors may yield a measurable impact on the predictive distribution although it may not be particularly noticable from studing the posterior distribution of weights \cite{google_bnn_posteriors}.
    \item \textbf{Deep Ensembles}. Deep ensembles has been shown to yield a better fidelity of the Bayesian predictive distribution on par with the ones produced by HMC, outperforming the surrogate distributions typically employed in the literature \cite{google_bnn_posteriors}. And this can be achieved at a significant reduction of the computational cost of sampling from the true posterior using HMC or NUTS.
    \item \textbf{Multi-GPU Training with HMC}. In this thesis, within the framework used in this thesis, we were confined to run the sampling on a single GPU device. Investigating the possibility of running multiple independent Markov chains on several GPUs simultaneously in an asynchronous fashion can potentially speed up the training of BNNs with HMC and NUTS significantly by allowing for many shorter chains that sample independently of each other. The quality of the sampled chains from the posterior is likely to improve by generating more than a single chain. One possible framework to adopt for this is Jax \cite{jax} which allow for automatic mapping of a Python function to several physical devices (such as multiple GPUs). It also supports just-in-time compilation for GPU devices which when run on NVIDIA GPUs support XLA compilation. Moreover, it provides its own framework for automatic differentiation. Thus it provides a viable platform to carry out more research oriented coding that the more strict frameworks provided by TensorFlow and its extensions.
\end{enumerate}