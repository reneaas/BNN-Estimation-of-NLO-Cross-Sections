\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


The Standard Model of particle physics is a remarkably successful theory explaining the fundamental particles of nature and their interactions. Yet, there exist a vast number of observations gathered in collider experiments which the Standard Model cannot explain. 
This has led physicists to propose several extensions to the theory.
These extensions, collectively called Beyond the Standard Model theories, predict the existence of new phenomena that may explain the observed data. But the investigation of the extended theories needs a high accuracy in its computed predictions. Direct calculation of these demands an excessive computational cost which significantly hampers the search for new physics. The use of machine learning to 
circumvent this obstacle has steadily increased over the last years with hopes of speeding up the search. The use of modern machine learning models such as deep learning has been widely used for classification tasks but the use of machine learning models to perform regression tasks in high-energy physics has only recently been employed to speed up quantum field theory calculations that would otherwise be intractible by direct calculation. An example of this effort is through evaluation of higher-order cross sections using Gaussian processes \cite{xsec}. 

Classical regression is not enough, however. A crucial aspect of regression tasks in high-energy physics is an estimation of the uncertainty of predictions which are needed to properly evaluate a new physics model by propagation of the uncertainty through proper inference models. While deep neural networks are ubiquitously employed to solve regression problems in the real world, they suffer the need for an excessive amount of data to serve as robust and reliable tools for predicting unknowns, which can be a major drawback of the model class for smaller sets of data. However, neural networks are universal function approximators and serve as an ideal model class for regression tasks. Bayesian inference of its parameters offers an approach of obtaining a distribution of its weights which allow for computation of predictions and yield corresponding uncertainties. The most widely used method for inferring weights of neural networks through the Bayesian framework is to parameterize a surrogate distribution for its weights which are used to approximate its true distribution. The approach has been widely investigated because of its natural integration into popular machine learning frameworks such as TensorFlow and PyTorch with the goal of spending approximately the same amount of time adjusting its parameters as training of classical neural networks takes. The potential weakness is of course its approximation of the exact distribution of weights. 

In this thesis, we propose using Hamiltonian Monte Carlo and its derivatives to infer neural network parameters from its exact distribution. The method is well known to be computationally expensive for large datasets but in the search for physics beyond the Standard Model, data is a scarce resource, a scenario in which these more accurate methods may shine. Bayesian inference using Hamiltonian Monte Carlo to sample from the exact distribution is considered challenging at best, as neural networks suffer from unidentifiability. The model class is what is known as over-parameterized. The two most common classes are weight-space symmetry and scaling symmetry. The first symmetry refers to the case where two layers can be permuted and still produce the same prediction. The second symmetry arise when using non-linear function that obey $\sigma(\alpha x) = \alpha\sigma(x)$. The second symmetry can be removed entirely by avoidance of non-linear function of this form but the first symmetry is an unavoidable one. Thus many equivalent parameterizations exist which manifest itself as a multi-modal distribution than can be notoriously difficult to infer parameters from.

In this thesis, we will investigate the computational costs related to inferring neural networks parameters from its exact distribution using Hamiltonian Monte Carlo. We will explore the nature of its distribution, its predictive performance and its ability to yield reliable uncertainty estimate. This will be investigated, treating is as a possible substitute for direct calculations in high-energy physics in hopes it will aid the seach for new physics beyond the Standard Model.

% \subsubsection*{Outline of the Thesis}
In chapter \ref{chap:physics_problem}, we explore the extensive computational cost needed to compute next-to-leading-order cross sections and how Bayesian regression models can serve as a viable substitute for direct calculation of these. In chapter \ref{chap:bayesian_ml}, we survey the notion of Bayesian machine learning and how one in general constructs a probabilistic model using Bayes' theorem. In chapter \ref{chap:mcmc} we provide an overview of important ideas for Monte Carlo Markov chains (MCMC) in continuous sample spaces. In chapter \ref{chap:hmc} we build upon this and show how to construct the first main sampler used in this called Hamiltonian Monte Carlo. In chapter \ref{chap:no_u_turn_sampler} we explore ways to dynamically tune parameters used in the sampler to avoid tedious hand-tuning. In chapter \ref{chap:bnn}, we bring all these topics together, culminating in a training algorithm for Bayesian neural networks using MCMC samplers to sample directly from the exact posterior of the probabilistic model. In chapter \ref{chap:numerical_experiments}, we explain the remainder of the methodology and investigate problems such as computational cost, performance on various hardware platforms, reliability of predictions and uncertainty measurements and tuning of the training of Bayesian neural networks.
