\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


The Standard Model of particle physics is a remarkably successful theory explaining the fundamental particles of nature and their interactions. It accounts for the constituent building blocks of all everyday phenomena on Earth. Yet, there exist a vast number of observations gathered in collider experiments which the Standard Model cannot account for. 
This has led physicists to propose several extensions to the theory.
These extensions, collectively called Beyond the Standard Model theories, predict the existence of new phenomena that may explain the observed data. The investigation of the extended theories needs a high accuracy in its computed predictions. Direct calculation of these demands an excessive computational cost which significantly hampers the search for new physics. The use of machine learning to 
circumvent this obstacle has steadily increased over the last years with hopes of speeding up the search. The use of modern machine learning models such as deep learning has been widely used for classification tasks but the use of machine learning models to perform regression tasks in high-energy physics has only recently been employed to speed up quantum field theory calculations that would otherwise be intractible by direct calculation. An example of this effort is through evaluation of higher-order cross sections using Gaussian processes \cite{xsec}. 

Classical regression is insufficent, however. A crucial aspect of regression tasks in high-energy physics is an estimation of the uncertainty in predictions which are needed to properly evaluate a new physics model by propagation of the uncertainty through proper inference models. While deep neural networks are ubiquitously employed to solve regression problems in the real world, they suffer the need for an excessive amount of data to serve as robust and reliable tools for predicting unknowns, which can be a major drawback of the model class for smaller sets of data. However, neural networks are universal function approximators and serve as an ideal model class for regression tasks, especially in the case where the underlying relationship one attempts to regress is difficult to discern from first principles. Bayesian inference of its parameters offers an approach of obtaining a distribution of its parameters which allow for computation of predictions and yield corresponding uncertainty estimates. The most widely used method for inferring parameters of neural networks through the Bayesian framework is to parameterize a surrogate distribution for its weights which are used to approximate its true distribution. The approach has spawned a popular research area because of its natural integration into popular machine learning frameworks such as TensorFlow and PyTorch with the goal of spending approximately the same amount of time adjusting its parameters as it does for classical neural networks. The potential weakness is of course its approximation of the exact distribution of weights. 

In this thesis, we propose using Hamiltonian Monte Carlo and its derivatives to infer neural network parameters from its exact distribution. It is a class of Markov chain Monte Carlo (MCMC) methods for continuous sample spaces. It is well known to be computationally expensive for large datasets but in the search for physics beyond the Standard Model, data is a scarce resource, a scenario in which a more accurate approach to inference may shine. Bayesian inference using Hamiltonian Monte Carlo to sample from the exact distribution is considered challenging at best, as neural networks suffer from unidentifiability. The model class is what is known as over-parameterized. This gives rise to multiple equivalent parameterizations that all yield the same predictions which has the unfortunate consequence of potentially producing multi-modal distributions with many regions that all yield the same effective predictions. Although Hamiltonian Monte Carlo is considered a state-of-the-art sampling method for continuous sample spaces, it needs hand-tuning to achieve good results. To handle this, we will explore adaptive Hamiltonian Monte Carlo methods to automatically perform tuning on the fly.

The main objective of this thesis is to investigate the viablity of substituting direct calculation of next-to-leading order cross sections in quantum field theory with Bayesian neural networks drawn from its exact distribution of parameters.
To this end, we will investigate the computational cost of the inference using Hamiltonian Monte Carlo and adaptive extensions of it. A central point of interest is the actual time needed to infer parameters on modern computing hardware like CPUs and GPUs to evaluate the feasibility of the methods. The computational cost of computing predictions of inferred models is a related and equally important question which we will consider. The distribution of neural network parameters are reported to be multi-modal \cite{google_bnn_posteriors}, a feature we will investigate. Due to the need for reliable uncertainty estimates when predictions are fed through proper inference models, we will investigate the predictive performance of Bayesian neural networks and the quality of the uncertainty estimates they yield. Inference of neural network parameters require the specification of a large number of hyperparameters, the effect of which may be highly dependent on the underlying data used. We will therefore investigate the effect these have on computational cost and predictive performance.

\subsubsection*{Outline of the Thesis}
In chapter \ref{chap:physics_problem}, we will discuss the extensive computational cost needed to compute next-to-leading order cross sections in quantum field theory and how Bayesian regression models can serve as a viable substitute for direct calculation of these. 
In chapter \ref{chap:bayesian_ml}, we will give an overview of machine learning for regression tasks and a formulation of it from a Bayesian perspective. We will discuss how one in general constructs a probabilistic model using Bayes' theorem which all together culminates to the notion of Bayesian machine learning.
In chapter \ref{chap:mcmc}, we provide an overview of important ideas for Monte Carlo Markov chains in continuous sample spaces including the Metropolis-Hastings and Gibbs samplers which form the basis for Hamiltonian Monte Carlo. In chapter \ref{chap:hmc} we will present Hamiltonian dynamics and discuss how we can construct the basic Hamiltonian Monte Carlo sampler. 
In chapter \ref{chap:no_u_turn_sampler} we explore ways to dynamically tune parameters used in the sampler to avoid tedious hand-tuning and automatically tune them on the fly. In chapter \ref{chap:bnn}, we will survey the neural network model before we bring all the topics together, culminating in a training algorithm for Bayesian neural networks using the MCMC samplers to draw parameters directly from the exact distribution of the model. In chapter \ref{chap:methodology}, we will discuss the dataset we apply the methods to, the preparation of the data and present the metrics we will use to evaluate the performance of the inferred models. In chapter \ref{chap:numerical_experiments}, we will present the results of our numerical experiments and discuss their implications. In chapter \ref{chap:conclusion}, we will present our final thoughts on the methods and suggestions for future topics of investigation.


\begin{comment}
    The two most common classes are weight-space symmetry and scaling symmetry. The first symmetry refers to the case where two layers can be permuted and still produce the same prediction. The second symmetry arise when using non-linear function that obey $\sigma(\alpha x) = \alpha\sigma(x)$. The second symmetry can be removed entirely by avoidance of non-linear function of this form but the first symmetry is an unavoidable one. Thus many equivalent parameterizations exist which manifest itself as a multi-modal distribution that can be notoriously difficult to infer parameters from.
\end{comment}