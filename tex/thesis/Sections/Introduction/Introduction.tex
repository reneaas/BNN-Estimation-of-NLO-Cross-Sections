\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Motivation, context and problem.


\begin{itemize}
    \item Contrast variational inference to Bayesian inference with MCMC methods.
\end{itemize}   



\subsubsection*{Outline of the Thesis}
In chapter \ref{chap:physics_problem}, we explore the extensive computational cost needed to compute next-to-leading-order cross sections and how Bayesian regression models can serve as a viable substitute for direct calculation of these. In chapter \ref{chap:bayesian_ml}, we survey the notion of Bayesian machine learning and how one in general constructs a probabilistic model using Bayes' theorem. In chapter \ref{chap:mcmc} we provide an overview of important ideas for Monte Carlo Markov chains (MCMC) in continuous sample spaces. In chapter \ref{chap:hmc} we build upon this and show how to construct the first main sampler used in this called Hamiltonian Monte Carlo. In chapter \ref{chap:no_u_turn_sampler} we explore ways to dynamically tune parameters used in the sampler to avoid tedious hand-tuning. In chapter \ref{chap:bnn}, we bring all these topics together, culminating in a training algorithm for Bayesian neural networks using MCMC samplers to sample directly from the exact posterior of the probabilistic model. In chapter \ref{chap:numerical_experiments}, we explain the remainder of the methodology and investigate problems such as computational cost, performance on various hardware platforms, reliability of predictions and uncertainty measurements and tuning of the training of Bayesian neural networks.
