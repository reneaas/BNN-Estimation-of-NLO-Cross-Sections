\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


The Standard Model of particle physics is a remarkably successful theory explaining the fundamental particles of nature and their interactions. Yet, there exist a vast number of observations gathered in collider experiments which the Standard Model cannot explain. 
This has led physicists to propose several extensions to the theory.
These extensions, collectively called Beyond the Standard Model theories, predict the existence of new phenomena that may explain the observed data. The investigation of the extended theories needs a high accuracy in its computed predictions. Direct calculation of these demands an excessive computational cost which significantly hampers the search for new physics. The use of machine learning to 
circumvent this obstacle has steadily increased over the last years with hopes of speeding up the search. The use of modern machine learning models such as deep learning has been widely used for classification tasks but the use of machine learning models to perform regression tasks in high-energy physics has only recently been employed to speed up quantum field theory calculations that would otherwise be intractible by direct calculation. An example of this effort is through evaluation of higher-order cross sections using Gaussian processes \cite{xsec}. 

Classical regression is sufficent, however. A crucial aspect of regression tasks in high-energy physics is an estimation of the uncertainty in predictions which are needed to properly evaluate a new physics model by propagation of the uncertainty through proper inference models. While deep neural networks are ubiquitously employed to solve regression problems in the real world, they suffer the need for an excessive amount of data to serve as robust and reliable tools for predicting unknowns, which can be a major drawback of the model class for smaller sets of data. However, neural networks are universal function approximators and serve as an ideal model class for regression tasks, especially in the case where the underlying relationship one attempts to regress is difficult to identify from first principles. Bayesian inference of its parameters offers an approach of obtaining a distribution of its parameters which allow for computation of predictions and yield corresponding uncertainties. The most widely used method for inferring parameters of neural networks through the Bayesian framework is to parameterize a surrogate distribution for its weights which are used to approximate its true distribution. The approach has spawned a popular research area because of its natural integration into popular machine learning frameworks such as TensorFlow and PyTorch with the goal of spending approximately the same amount of time adjusting its parameters as training of classical neural networks takes. The potential weakness is of course its approximation of the exact distribution of weights. 

In this thesis, we propose using Hamiltonian Monte Carlo and its derivatives to infer neural network parameters from its exact distribution. It is a class of Markov chain Monte Carlo methods for continous sample spaces. It is well known to be computationally expensive for large datasets but in the search for physics beyond the Standard Model, data is a scarce resource, a scenario in which these more accurate methods may shine. Bayesian inference using Hamiltonian Monte Carlo to sample from the exact distribution is considered challenging at best, as neural networks suffer from unidentifiability. The model class is what is known as over-parameterized. This give rise to multiple equivalent parameterizations that all yield the same predictions. Hamiltonian Monte Carlo is a state-of-the-art sampling method for continuous sample spaces that needs hand-tuning to achieve good results. To handle this, we will explore adaptive Hamiltonian Monte Carlo methods. 

The main objective of this thesis is to investigate the viablity of substituting direct calculation of quantum field theory predictions with Bayesian neural networks sampled from its exact distribution of parameters.
To this ned, we will investigate the computational cost of inferring neural network parameters from its exact distribution using Hamiltonian Monte Carlo and adaptive extensions of the sampler. A central point of interest is the actual time needed to infer parameters on modern computing hardware like CPUs and GPUs to evaluate the feasibility of the methods. The computational cost of computing predictions of inferred models is a related and equally important question which we will consider. The distribution of neural network parameters are reported to be multi-modal \cite{google_bnn_posteriors}, a feature we will investigate. The feature is a result of the over-parameterized nature of the model class and makes the use of surrogate distributions questionable. Due to the need for reliable uncertainty estimates when predictions are fed through proper inference models, we will investigate the predictive performance of Bayesian neural networks and the quality of the uncertainty estimates they yield. Inference of neural network parameters require the specification of large number of hyperparameters, the effect may be highly dependent on the underlying data used. We will therefore investigate the effect these have on computational cost and predictive performance.

\subsubsection*{Outline of the Thesis}
In chapter \ref{chap:physics_problem}, we will discuss the extensive computational cost needed to compute next-to-leading-order cross sections and how Bayesian regression models can serve as a viable substitute for direct calculation of these. In chapter \ref{chap:bayesian_ml}, we will survey the notion of Bayesian machine learning and how one in general constructs a probabilistic model using Bayes' theorem. We will give a general overview of machine learning for regression tasks and how this is formulated from a Bayesian perspective. In chapter \ref{chap:mcmc} we provide an overview of important ideas for Monte Carlo Markov chains (MCMC) in continuous sample spaces including the Metropolis-Hastings and Gibbs sampling which forms the basis for Hamiltonian Monte Carlo. In chapter \ref{chap:hmc} we will present Hamiltonian dynamics and discuss how we can construct the basic Hamiltonian Monte Carlo sampler. 
In chapter \ref{chap:no_u_turn_sampler} we explore ways to dynamically tune parameters used in the sampler to avoid tedious hand-tuning. In chapter \ref{chap:bnn}, we bring all these topics together, culminating in a training algorithm for Bayesian neural networks using the MCMC samplers to sample directly from the exact distribution of the probabilistic model. In chapter \ref{chap:methodology}, we will discuss the dataset we apply the methods to, the preparation of the data and present the metrics we will use to evaluate the performance of the inferred models. In chapter \ref{chap:numerical_experiments}, we will present the results of our numerical experiments and discuss their implications. In chapter \ref{chap:conclusion}, we will present our final thoughts on the methods and suggestions for future topics of investigation.


\begin{comment}
    The two most common classes are weight-space symmetry and scaling symmetry. The first symmetry refers to the case where two layers can be permuted and still produce the same prediction. The second symmetry arise when using non-linear function that obey $\sigma(\alpha x) = \alpha\sigma(x)$. The second symmetry can be removed entirely by avoidance of non-linear function of this form but the first symmetry is an unavoidable one. Thus many equivalent parameterizations exist which manifest itself as a multi-modal distribution that can be notoriously difficult to infer parameters from.
\end{comment}