Hamiltonian Monte Carlo is considered a state-of-the-art sampler that efficiently explores sample space by producing large jumps to successive states with low correlation, 
but suffers the need for manual tuning of the trajectory length $\epsilon L$. 
In this chapter, we will explore improvements that adaptively adjust the trajectory length. This is achieved by means of adapting both the number of Leapfrog steps $L$ using an improved sampler called the \textit{No-U-Turn} (NUTS) sampler, and an adaptive scheme for setting the step size $\epsilon$ using a \textit{dual averaging} algorithm. We will closely follow the treatment in the original paper \cite{nuts} but adapt the notation to be consistent with the rest of this thesis.

We will start off with a discussion on how to adapt the number of Leapfrog steps using NUTS. At a high-level, NUTS starts from an initial state $(q, p)$ and simulates the Hamiltonian dynamics of the system. This is
done in the following way. Leapfrog steps are performed either forwards or backwards in time, first with a single Leapfrog step, then two Leapfrog steps, then followed by four Leapfrog steps and so on. This reiteration of the simulation is performed until the the path traced out starts to double back towards itself. The states traced out can be regarded as a \textit{balanced binary tree} $\mathcal{B}$ where
each node represents a phase-space state produced by the Leapfrog integrator during the simulation. The next state of the Markov chain is sampled at random from these nodes.  

We will end the chapter with the dual averaging scheme for adaptively setting the step size using the Leapfrog integrator. The algorithm is a modified version of a dual averaging scheme presented by Nesterov in \cite{Nesterov2009}.


\section{The No-U-Turn Sampler}
The No-U-Turn sampler generates a set of states we may regard as a balanced binary tree which we represent with the set $\mathcal{B}$. 
We shall explain the way it is built by starting from an initial point and building up the tree gradually before we generalize the procedure. An example of a trajectory generated by NUTS is shown in figure \ref{fig:nuts_trajectory}. 
The initial state $(q, p)$ is defined as the the node of the tree of depth $j = 0$. We sample a direction at random in time, either forwards ($v_0 = 1$) or backwards ($v_0 = -1$) and perform a single Leapfrog step to produce a new state $(q', p')$ using the step size $\epsilon v_0$. This state represents its own little subtree of height $j = 0$ which is to be combined with the initial node to form a tree of height $j = 1$. If $v_0 = 1$, the new node is placed as the right half of the new tree. Conversely, if $v_0 = -1$, the new node is placed as the left half of the new tree. We repeat, but this time we double the number of Leapfrog steps to $L = 2$. We randomly sample the direction once more. If forwards in time ($v_1 = 1$), we initiate the Leapfrog integrator from rightmost node of the current tree (which represents the head of the trajectory). If backwards in time ($v_1 = -1$), we feed the state of the leftmost node to the Leapfrog integrator (which represents the tail of the trajectory) and integrate backwards in time. The new states produced with the Leapfrog integrator becomes the nodes of a subtree of height $j = 1$ which will be combined with the current tree. Again, if $v_1 = 1$, we place the new subtree as the right half of the combined tree. If $v_1 = -1$, it is placed as the left half of the combined tree. 
This procedure is carried out repeatedly. We draw a direction in time at random, and perform twice as many Leapfrog steps as the prior iteration from the rightmost node if forwards in time or the leftmost node if backwards in time to extend the trajectory further. More precisely, given a tree of height $j$, 
\begin{enumerate}
    \item Sample a direction $v_j \sim \text{Uniform}(\{-1, 1\})$ in time. Set the step size in the Leapfrog integrator as $\epsilon \to \epsilon v_j$.
    \item Perform $2^j$ Leapfrog steps from the rightmost node if $v_j = 1$ or from the leftmost node if $v_j = -1$.  
    \item The new generated tree of height $j$ is combined with the current tree of height $j$, producing a combined tree of height $j + 1$. If $v_j = 1$, the newly generated tree becomes the right half of the combined tree. If $v_j = -1$, it becomes the left half of the combined tree.
\end{enumerate}
From a practical perspective, we cannot apply these steps repeatedly \textit{ad-infinitum} of course. At some point, we must stop the doubling of the tree (trajectory) and select a node which will be the phase-space state to take the next place in the Markov chain. How this is solved is what we shall consider next.


\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.075]{figures/drawings/doubling3_with_numbering.png}
    \caption{The figure shows an example of a trajectory generated by the NUTS sampler. The top diagram displays the projection on position space with the momenta drawn in as arrows. The bottom diagram shows the resulting tree. The balanced binary tree structure is drawn in on the trajectory as well as illustrated at the bottom. The numbering displays the order in which the states are generated by Leapfrog integration. The black node is the initial node. The first doubling is forwards in time and yields the rightmost node of the first binary tree. The second doubling is backwards in time and is initiated from the black node, yielding a new tree of height 2 where the left subtree is the new states (the yellow nodes). The next doubling is also backwards in time, and the Leapfrog integrator is initiated from the tail (the leftmost yellow node) for four Leapfrog steps generating a subtree which becomes the left half of the next tree (blue nodes). The final doubling in the figure is forwards in time with $L = 8$ Leapfrog steps are taken from the orange node (which was the leftmost leaf of the tree befoure the final doubling) which yields the green nodes. The figure is a modified version of a diagram in \cite{nuts}.}\label{fig:nuts_trajectory}
\end{figure}

\begin{comment}
    The main benefit HMC introduced over random walk Metropolis is its ability to produce large jumps in parameter space, allowing efficient exploration of the typical set. Doubling back towards other states that are already part of the trajectory is wasteful and removed the benefit HMC introduces in the first place. 
\end{comment}

\subsection{Stopping Conditions and Selection of Candidate States}
We seek a way to stop the doubling procedure that automatically does so when continuing is no longer beneficial from a computational standpoint.
Let us first consider a point we stressed in chapter \ref{chap:hmc}. If Hamilton's equations are solved exactly, the generated trajectory is confined to a hyperplane. Thus for an exact solution, the trajectory would at some point double back onto itself. This will likely also happen when we only approximate the solution with the Leapfrog integrator. Continuing the doubling procedure when the generated trajectory begins to double back on itself  (perform a ``U-turn''), we will revisit regions of parameter space that are already part of the binary tree, thus wasting computational resources. To avoid this, we ought to check during each doubling if such a ``U-turn'' occurs and terminate if it does. Consider an initial state $(q, p)$ taken to be the initial state and $(q', p')$ be a point produced by the Leapfrog integrator (which is treated as a function of time). Then the change in the Euclidean distance between the positions are
\begin{equation}\label{eq:change_in_distance}
    \dv{t}\frac{\norm{q' - q}_2^2}{2} = (q' - q)^T \dv{t}  (q' - q) = (q' - q)^Tp'.
\end{equation}
If eq.~\eqref{eq:change_in_distance} evaluates to a negative number, continuing the simulation for an infinitesimal time $\dd t$ will decrease the distance between the points which is how we can detect an occurence of a ``U-turn''. The way the NUTS sampler does this, is to consider the leftmost and rightmost node of \text{any} subtree of the current tree. Let $(q^+, p^+)$ be the rightmost node and $(q^-, p^-)$ the leftmost node of any subtree. Then if 
\begin{equation}\label{eq:no_u_turn_condition}
    (q^+ - q^-)^Tp^+ < 0 \qq{or} (q^+ - q^-)^Tp^- < 0, 
\end{equation}
is fulfilled for any of the subtrees, it terminates the doubling of the tree. This is the so-called \textit{No-U-Turn condition}. We must consider two distinct cases where the stopping condition in eq.~\eqref{eq:no_u_turn_condition} is met.
\begin{enumerate}
    \item Consider a tree of height $j$. If, during the doubling to create the tree of height $j + 1$, a ``U-turn'' is detected within any of the subtrees of the new tree of height $j$, all of its states are discarded and the tree before the doubling is taken as the final tree. That is, if eq.~\eqref{eq:no_u_turn_condition} is met for any of the subtrees of the new tree, we must discard their states. The reason for this is that if we were to begin the doubling from any of these states, the No-U-Turn condition is met before we can rebuild $\mathcal{B}$ and thus we violate reversibility and inadvertently detailed balance.
    \item Consider now the combined tree after doubling. Naturally, none of the subtrees will satisfy the No-U-Turn condition because
    \begin{enumerate}
        \item The tree before doubling had not triggered the termination of the doubling procedure. Hence, none of its subtrees satisfy eq.~\eqref{eq:no_u_turn_condition}. 
        \item The new tree, which is the other half of the combined tree, did not trigger a termination either so none of its subtrees satisfy eq.~\eqref{eq:no_u_turn_condition}.
    \end{enumerate}
    The only part of the combined tree that can satisfy the No-U-Turn condition at this is point is the rightmost and leftmost nodes of the entire tree. If eq.~\eqref{eq:no_u_turn_condition} is met in this case, we terminate the doubling but no state must necessarily be discarded. After all, since the full tree is built and none of subtrees satisfy the No-U-Turn condition, we can start from any state and find a unique set of directions $\{v_j\}$ from which we can rebuild the entire tree before the No-U-Turn condition is satisfied.
\end{enumerate}

There is another case in which we want to stop the doubling procedure. If at any point, the error of the simulation becomes too large,
the states produced during the doubling process is likely to lie in a low probability region of parameter space. Let $(q', p')$ be any state in the tree (including the initial state) and denote the initial state as $(q, p)$. The doubling is terminated if
\begin{equation}\label{eq:nuts_error_condition}
    H(q', p') - H(q, p) + \log \Lambda \geq \Delta_\text{max},
\end{equation}
where $\Lambda \sim \text{Uniform}(0, 1)$ is sampled in the beginning of the tree building (and is the slice variable used during Metropolis correction to accept or reject a state) and $\Delta_\text{max}$ is a tolerance which the authors of the original paper recommends to be set to $\Delta_\text{max} = 1000$ to allow the tree building to continue if the error introduced by the Leapfrog integrator is moderate. Equation~\eqref{eq:nuts_error_condition} essentially states that if the energy difference becomes too large, we terminate the tree building. The tree produced during this final doubling must be discarded and the final tree becomes the tree prior to doubling. The reasoning is the same as before; we cannot initiate the Leapfrog integrator from the states in this tree and rebuild $\mathcal{B}$ as the stopping condition in eq.~\eqref{eq:nuts_error_condition} is met before the full tree can be rebuilt.

Once the tree $\mathcal{B}$ is built, the NUTS sampler selects a \textit{candidate set} $\mathcal{C}$ from the tree where all of its elements, which we define as \textit{candidate states}, must satisfy
\begin{equation}\label{eq:nuts_acceptance}
    \frac{\pi(q', p')}{\pi(q, p)} = \exp\{-[H(q', p') - H(q, p) ]\} > \Lambda,
\end{equation}
which is the same Metropolis correction that is employed in HMC \cite{nuts_joonha_park}. The next state in the Markov chain is drawn randomly from $\mathcal{C}$. The selected state $(q', p')$ is projected onto $q'$ which is the parameter of interest that is next in line in the Markov chain. We have defined a function {\tt NUTSstep} in algorithm \ref{algo:nuts} which generates the next state $q'$ in the Markov chain given a prior state $q$, a Hamiltonian $H$ and a step size $\epsilon$.

\begin{figure}[H]
	\begin{algorithm}[H]
	\caption{The NUTS Sampler}\label{algo:nuts}
	\begin{algorithmic}
        \Function{{\tt NUTSstep}}{$q, H, \epsilon$}
            \State Sample $p \sim \mathcal{N}(0, I)$
            \State Sample $\Lambda \sim \text{Uniform}(0, 1)$
            \State Set the initial tree $\mathcal{B} \gets \{(q, p)\}$
            \For{$j \geq 1$}
                \State Sample $v_j \sim \text{Uniform}(\{-1, 1\})$
                \If{$v_j = 0$}
                    \State Perform $2^j$ Leapfrog steps from the rightmost node of the current tree. Assign to $\mathcal{B}'$
                \Else
                    \State Perform $2^j$ Leapfrog steps from the leftmost node of the current tree. Assign to $\mathcal{B}'$
                \EndIf
                \If{For \text{any} subtree in $\mathcal{B}'$, eq.~\eqref{eq:no_u_turn_condition} is satisfied}
                    \State Terminate building of tree and discard $\mathcal{B}'$
                \Else{
                    \State $\mathcal{B} \gets \mathcal{B} \cup \mathcal{B}'$
                }
                \EndIf
            \EndFor
        \EndFunction
	\end{algorithmic}
	\end{algorithm}
\end{figure}

\subsection{Computational Cost}
The No-U-Turn sampler introduces additional operations to keep track of whether any of the stopping conditions are met.
Equation~Â \eqref{eq:no_u_turn_condition} requires $2^{j+1} - 2$ evaluations of inner products for a tree of height $j$, two inner products per subtree. 
In addition, eq.~\eqref{eq:nuts_error_condition} requires $2^j - 1$ evaluations of the Hamiltonian, and its gradient must be calculated an equal amount of times to perform Leapfrog integration similar to what is required by HMC. The additional cost of the inner products are, however, neglible for sufficiently complex models and/or large datasets as the evaluation of the Hamiltonian and its gradient will be the dominating computational cost.
Another added computational cost is the memory footprint introduced by storing the balanced binary tree. In its naive form, the memory footprint requires the order $\mathcal{O}(2^j)$ states. A more efficient solution can be found by observing that the uniform distribution over the candidate set $\mathcal{C}$ can be rewritten as 
\begin{equation}\label{eq:uniform_dist_over_C}
    p(q, p| \mathcal{B}, \mathcal{C}) = \frac{1}{\abs{\mathcal{C}}} = \frac{\abs{\mathcal{C}_\text{subtree}}}{\abs{\mathcal{C}}}\frac{1}{\abs{\mathcal{C}_\text{subtree}}},
\end{equation}
where $\abs{\cdot}$ denotes the \textit{cardinality} or the number of elements in the set and $\mathcal{C}_\text{subtree} \subseteq \mathcal{C}$ is the candidate states in a subtree of the subset of the full tree corresponding to the candidate set. Equation~\eqref{eq:uniform_dist_over_C} states that the uniform probability over $\mathcal{C}$ can be rewritten as the probability of selecting a subtree $\mathcal{C}_\text{subtree}$ from $\mathcal{C}$ times the probability of drawing a state at random from that subtree. A tree of height $j$ consists of two subtrees of height $j - 1$. From each subtree for $j > 0$, draw a state $(q, p)$ from each subtree with probability $1 / \abs{\mathcal{C}_\text{subtree}}$ to represent that tree and a give it a weight proportional to how many states of the total candidate set that belonged to that particular subtree. Starting from the initial tree of height $j > 0$, this can be performed during the doubling process for each new subtree that is generated to avoid explicit storage.  The storage requirement is thus brought down to an order of $\mathcal{O}(j)$ position-momentum states, which significantly reduces the memory footprint.

