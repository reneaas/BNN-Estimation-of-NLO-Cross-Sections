\section{Bayesian learning of Neural Networks using Monte Carlo Samplers}
So far, we have discussed neural networks as a model class whilst ignoring the issue of what it really means to do Bayesian learning of neural networks, 
in other words, what it means to \textit{train} BNNs. We have intentionally left it somewhat ambigious what this really means because as it turns out, its meaning can be quite different depending on how Bayesian inference is performed. In this section we will clarify precisely what it means to train BNN using MCMC samplers such as HMC and NUTS. We shall then discuss practical aspects of the training which we shall put to practice in chapter \ref{chap:numerical_experiments}.

\subsection{What \textit{is} Bayesian learning of Neural Networks?}
The way Bayesian learning of neural networks manifest itself depends on the way in which we do Bayesian inference of the probabilistic model. 
We are concerned with inference of model parameters from the posterior using MCMC methods and will therefore obtain samples where each such sample
consist of the weights of an entire neural network. More precisely, if we gather $N$ samples with a chosen sampler, we will obtain $N$ entire neural networks
all sampled from the posterior to explain the observed data. Thus, what we mean by a \textit{trained} BNN in this sense is that we have
sampled a set of neural networks that collectively represent the BNN. 

As we discussed at the end of chapter \ref{chap:bayesian_ml}, 
we are mainly interested in the predictive distribution $p(y|x, W, b)$ of an output $y$ given an input $x$. We can approximate this distribution by constructing an empirical distribution by feeding $x$ through all $N$ sampled neural networks to obtain $N$ predicted targets $\hat{y}$ using eq.~\eqref{eq:predictive_dist_approx}.
The second quantity of interest is expectations of target functions dependent on the model parameters. 
We can approximate any such expectation with an MCMC estimator as in eq.~\eqref{eq:mcmc_estimator} using all $N$ networks to evaluate the target function.


\subsection{The Potential Energy Function of Neural Networks}

We now turn to the Bayesian formulation of the neural network model for use with the samplers used in this thesis. Assume that we have picked an architecture for a neural network and wish to train it in the Bayesian sense.
For both HMC and NUTS, we need only specify a potential energy function for our model. The samplers
take care of the rest. Assume we are dealing with a dataset $D = \{(x^{(i)}, y^{(i)})\}_{i=1}^N$ where all $N$ points are independent and identically distributed.
Equation~\eqref{eq:potential_energy_bayesian} instructs us to specify a prior for the weights of the network, and a likelihood function that depends on the target and the model output, in order
to fully specify the potential energy function.
Common practice is to choose priors that are either Gaussian or Laplacian. We will operate with Gaussian priors, i.e.
\begin{equation}\label{eq:model_priors}
  P(W^\ell) \propto \exp\left(-\frac{\lambda_W}{2}\norm{W^\ell}_2^2\right) \qq{and} \qquad P(b^\ell) \propto \exp\left(-\frac{\lambda_b}{2}\norm{b^\ell}_2^2\right).
\end{equation}
We will not worry too much about the choice of priors as the term in the potential energy function that corresponds to the likelihood will be much larger in practice. The Gaussian priors serve roughly the same purpose as $L^2$-regularization does in classical ML.

The likelihood for regression from eq.~\eqref{eq:likelihood_fn} formulated in terms of a neural network $\hat{f}(x^{(i)};W, b)$ is
\begin{equation}
  p(D|W, b) = \exp \left(-\frac{1}{2\sigma^2}\sum_{i=1}^N\norm{y^{(i)} - \hat{f}(x^{(i)};W, b)}_2^2\right).
\end{equation} 
This is not the only valid choice for a likelihood function but it is the common choice since it can be identified with the Euclidean $L^2$-norm and
itself ``neat'' mathematical properties.

Combining the priors and the likelihood with eq.~\eqref{eq:loss_function_of_posterior} yields the potential energy function
\begin{equation}\label{eq:special_potential_energy}
  \mathcal{L} = \frac{1}{2\sigma^2}\sum_{i=1}^N \norm{y^{(i)} - f(x^{(i)}; W, b)}_2^2 + \frac{\lambda_W}{2} \sum_{\ell=1}^L \norm{W^\ell}_2^2 + \frac{\lambda_b}{2}\sum_{\ell=1}^L \norm{b^\ell}_2^2,  
\end{equation}
up to a constant. As we discussed in chapter \ref{chap:hmc}, the potential energy function also happens to be the typical loss function with $L^2$-regularization used in the classical ML which is why we denote it as $\mathcal{L}$. At this point, we have set up all the machinery we need to train BNNs. Our next topic of discourse is the practice of doing so.

\subsection{Practical Training of Bayesian Neural Networks}

Training BNNs in practice requires us to specify a fairly large number of hyperparameters to obtain a set of models. These are 
\begin{enumerate}
  \item \textbf{Neural network architecture}. We need to specify its number of layers, number of nodes and activation function per layer. 
  Once the BNN is trained, we store this information along with the model for future usage. The stored weights themselves will encode how many layers and nodes the model has but the activation functions must be stored in addition.
  \item \textbf{Number of results}. We must specify how many neural networks we want to sample and store. Because the weights must be stored in its entirety, we are forced to worry about the amount of disk space that is required to do so. For a fixed allocated disk space, we can obviously store a larger set of samples if the model is simple. As complexity increases, the number of samples we can store will necessarily decrease.
  \item \textbf{Number of burn-in steps}. We must decide how long we want to run the MCMC chain before we start storing results. If amount of disk space was no obstacle, this step would be considered entirely optional as we could simply store every single sample and make a thorough analysis of the chain's quality to determine when proper mixing is obtained. In practice, with TensorFlow's framework, we can make a predetermined set of burn-in steps to avoid unnecessary RAM usage.
  \item \textbf{Amount of thinning}. Since successive samples most likely will be correlated, we can specify how many samples we simply skip once we start gathering samples, i.e. after the burn-in period. Again, we could ignore this and do this manually with the chain but doing so becomes a question of amount of available VRAM, RAM and disk space. 
  \item \textbf{Hyperparameters specific to the samplers}. The samplers themselves carry their own hyperparameters. In the case of HMC, we must specify a fixed number of Leapfrog steps $L$. If we use the NUTS sampler, we must specify the maximum tree depth. Moreover, we must determine how much of the computing resources we allocate to adapting the step size used in the Leapfrog integrator.
  \item \textbf{Amount of pretraining}. An attempt to accelerate convergence of the MCMC chain can be achieved by pretraining the neural network using minimization methods with the backpropagation algorithm to bring the weights closer to a minima of the potential energy function (i.e. the loss function used in classical ML). Then the point estimate obtained at the end of the training is used as a starting point for the MCMC chain.
\end{enumerate}

\subsection{Training Algorithm of Bayesian Neural Networks}
In this section we shall turn our attention to an actual training algorithm for BNNs. Assume we pick a sampler $S$ that represents either HMC or NUTS and a specified permuation of the hyperparameters discussed in the last section. In practice we can summarize a training algorithm as follows.
\begin{enumerate}
  \item Initialize the weights of the model from the specified priors, i.e.
  \begin{equation}
    W^\ell \sim p(W^\ell) \qq{and} b^\ell \sim p(b^\ell) \qq{for} \ell=1,\ldots, L.
  \end{equation}
  \item Minimize the potential energy function $\mathcal{L}$ with respect to the weights of the model using an optimizer of your choice to obtain a point estimate for use as the initial state of the Markov chain.
  \item Initialize the Markov chain for a finite set of burn-in steps to achieve mixing using $S$. A proportion of the initial burn-in steps are used for step size adaptation, while the remaining are used for mixing.
  \item Gather samples by applying $S$ repeatedly, replacing the current weights of the model by the ones returned by $S$.  
\end{enumerate}


\begin{comment}
  
With the ingredients discussed hitherto, it is time to consider practical training of BNNs.
We can divide this process in two parts. First, we must choose the architecture of the model, which is tantamount to picking model class. This decision is similar in nature to the training of regular neural networks to obtain point estimates. Second, we must consider the sampling process of the model. There are several considerations to make here such as disk space, length of mixing, amount of thinning and so on.

\subsection{Sampling of Bayesian Neural Networks}
We may divide this procedure into several subcategories:
\begin{enumerate}
  \item Choosing sampler $S$.
  \item Amount of mixing. We must choose the number of burn-in steps to run the sampling process before we actually start gathering sampled NNs.
  \item Amount of thinning. Number of samples to skip per stored sample. 
  \item Number of NNs to sample. Since the way we do Bayesian learning of NNs here actually require us a set of full NNs and store them to disk, we must decide how many samples we want to gather.
\end{enumerate}


\begin{enumerate}
  \item Initialize the weights of the network sampled from the priors. 
  \item An \textit{optional step}: train the network using a minimization algorithm with $\mathcal{L}$ as loss to reach an initial network state in the high probability \textit{density} region of the posterior.
  This is done to reduce the number of burn-in steps needed for the the Markov chain to reach the stationary distribution. However, for sufficiently high-dimensional parameter spaces this 
  may lead to adverse effects since the typical set may not be close to any mode of the posterior.
  \item Perform a finite number of burn-in steps to facilitate mixing by application of $S$ repeatedly.
  \item Sample a set of neural network parameters from the posterior distribution by use of $S$ repeatedly.
\end{enumerate}

\end{comment}