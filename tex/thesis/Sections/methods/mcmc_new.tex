In this chapter, we will discuss fundamental ideas pertaining to \textit{Markov Chain Monte Carlo} (MCMC) methods.
We shall confine the discussion to continuous sample spaces which is the kind needed in this thesis.
We will commence with a discussion of expectation values and an important notion called the \textit{typical set}. 
We will then define and discuss Markov chains and Markov transitions after which we shall
discuss Metropolis-Hastings sampling and its limitations. Finally we will
look at Gibbs sampling.
We will adopt a geometric view where possible to provide a natural transition to Hamiltonian Monte Carlo and the No-U-Turn sampler
in the two following chapters. 
% The treatment will closely follow \cite{conceptual_intro_hmc}.

\section{Expectation Values and the Typical Set}
Consider a \textit{target probability density} $\pi(\theta)$ and an $m$-dimensional sample space $\Theta$ where $\theta \in \Theta$. 
Consider $f(\theta)$ to be an arbitrary smooth
function of $\theta$. The \textit{expectation value} of $f(\theta)$ with respect to the density $\pi(\theta)$ is then defined as
\begin{equation}\label{eq:expval}
    \mathbb{E}_\pi[f] = \int \dd \theta \ \pi(\theta)f(\theta).
\end{equation}
We shall interchangably refer to expectation values simply as \textit{expectations}.
We will call the function $f$ we seek to compute the expectation of as the \textit{target function}.
For all but a few simple densities, evaluation of eq.~\eqref{eq:expval} is impossible analytically. 
To complicate things further, numerical evaluation with numerical integration techniques of the expectation in high-dimensional spaces quickly becomes computationally
infeasible as the dimensionality increases, due to limited computational resources. 
Even worse, we may not even be able to write down the expression of $\pi(\theta)$ explicitly.
Fortunately, it is unlikely that the entire sample space contribute significantly to the expectation.
If we could somehow pick out the points in sample space that \textit{does} contribute, only knowing $\pi(\theta)$ up to a normalization constant, we could
make approximate computations of expectations tractible. 

For most purposes, we are interested in the expectation of more than a single target function. For example, in Bayesian applications,
we are often interested in both the mean and variance of a quantity which introduces the need for several target functions. 
Thus the numerical method should not depend on the target function in question. 
Instead the focus should be laid on the contribution from $\pi(\theta)\dd \theta$ to
the integrand. The objective of MCMC methods is to efficiently 
sample points from regions of sample space where this quantity is non-neglible. This region of sample space
is called the \textit{typical set} \cite{conceptual_intro_hmc}. 

\subsection{The Typical Set}
For simplicity, we can divide a
sample space into three regions with respect to the target density $\pi(\theta)$.
\begin{enumerate}
    \item High-probability density region. These are regions in the neighborhood of a mode of the target density.
    In general, as the dimensionality increases, the contribution from $\pi(\theta) \dd \theta$ becomes neglible here 
    unless the volume in the region is significant enough.
    \item The typical set. This refers to the regions in which $\pi(\theta) \dd \theta$ provides a non-neglible contribution
    to any expectation. This may be thought of as the high-probability region of the sample space
    since $\pi(\theta) \dd \theta$ is proportional to probability of a volume $\dd\theta$ in the neighborhood of $\theta$. 
    \item Low-probability density regions. These are regions far away from any mode of the density. This region, too, will generally
    yield neglible contributions to the integrand even if the volume is large.
\end{enumerate}
Although the notion of a typical set can be formalized precisely, we will intentionally 
operate with this somewhat imprecise definition. For our purposes, it suffices to use it merely as
a conceptual notion to evaluate the quality of the samples generated by an MCMC chain. 
\subsection{The Target Density and Bayesian Applications}
In the chapter on Bayesian ML, we mentioned that we could not compute the evidence term of Bayes' theorem in 
realistic applications and thus were only concerned with a proportionality relationship $p(\theta|D)\propto p(D|\theta)p(\theta)$. Thus any MCMC methods we are interested in cannot require that the $\pi(\theta)$ is normalized to unity. We only require that the density is smooth and that 
\begin{equation}
  0 < \int \dd\theta \ \pi(\theta) < \infty.
\end{equation} 
Sometimes we may refer to the target density as the \textit{target distribution}.
In Bayesian applications, we assume that $\pi(\theta) = p(D|\theta)p(\theta)$ such that $p(\theta|D) \propto \pi(\theta)$.

\section{Markov Chains and Markov Transitions}
Since direct evaluation of eq.~\eqref{eq:expval} in most applications is intractible, we seek to approximately evaluate it
by generating samples $\theta^{(t)}$ from the typical set using \textit{Markov chains}. A Markov chain is a sequence of points $\theta^{(1)}, \theta^{(2)}, ..., \theta^{(n)}$ generated sequentially
using a random map called a \textit{Markov transition}. A Markov transition is a conditional probability density
$T(\theta'|\theta)$ that yields the probability of transition from a point $\theta$ to $\theta'$. The Markov transition is also called
a \textit{Markov kernel} which is a special case of a \textit{transition kernel}. The latter is the term we will adopt because it
is the term used by TensorFlow Probability.

An arbitrary transition kernel is not useful because the generated Markov chain is unlikely to have any relation to the target distribution
of interest. To generate a useful Markov chain, we must use a transition kernel that preserves the target distribution. 
The condition that ensures this is
\begin{equation}\label{eq:detailed_balance}
    \pi(\theta) = \int d\theta' \pi(\theta')T(\theta|\theta').
\end{equation}
The condition is formally called \textit{detailed balance}. The interpretation of the condition is that the Markov chain is reversible.

We can start from any $\theta$ and use the transition kernel to produce a set of new states. The distribution generated by the Markov chain
should be distributed according the target distribution regardless of which point we used to generate the chain from, given a long enough chain. A more important
fact is that as long as this condition is satisfied, the Markov chain will converge to and stay within the typical set.

The standard approach to approximate eq.~\eqref{eq:expval} is then with the MCMC \textit{estimator}
\begin{equation}\label{eq:mcmc_estimator}
    \hat{f}_N = \frac{1}{N}\sum_{t=1}^N f(\theta^{(t)}).
\end{equation}
For large enough $N$, the estimator can be shown to converge to the true expectation such that $\lim_{N\to\infty} \hat{f}_N = \mathbb{E}_\pi[f]$.
Obviously, the knowledge that the estimator will asymptotically converge to the true expectations is of limited use
when restricted to a practical computation in which only a finite chain can be generated. 
We must therefore understand the properties of finite Markov chains so we can efficiently use them to approximate eq.~\eqref{eq:expval}.

\subsection{Ideal Markov Chains}
In order to understand the behaviour of finite Markov chains, we should first 
consider the behaviour of ideal Markov chains.
An ideal Markov chain can be divided into three phases.
\begin{enumerate}
    \item A convergence phase. The Markov chain is initiated from some point $\theta$ and the initially generated sequence
    lies in a region outside the typical set. Estimators evaluated using this part of the sequence are highly biased,
    meaning inclusion of these points will lead to an estimator that lies relatively far away from the true expectation.
    \item An exploration phase. The Markov chain has reached the typical set and begins its first ``traversal'' of it.
    In this phase, estimators will rapidly converge towards the true expectations.
    \item A saturation phase. At this point, the Markov chain has explored most of the typical set and convergence
    of the estimators slow down significantly. 
\end{enumerate}
The ideal evaluation of estimators thus only use the parts of the Markov chain generated in the second and third phase, discarding the
the chain generated in the first phase. The notion of discarding the chain from the first phase is called \textit{burn-in} or \textit{mixing}.
To most efficiently approximate eq.~\eqref{eq:expval}, we should really only use points generated in the exploration phase.
Using points from the saturation phase does not hurt the estimators but yield diminishing returns
with respect to computational resources.

\subsection{Pathologies}
Unfortunately, many target distributions embody typical sets with pathological regions where \textit{any} 
transition kernel that obey eq.~\eqref{eq:detailed_balance} is not sufficient to \textit{efficiently} explore the typical set.
Geometrically, this can be regions in the typical set in which the target distribution rapidly changes.
The pathological regions can be completely ignored by the chain for much of the exploration,
leading to poor convergence and thus biased estimators. However, as long as the transition kernel
satisfies detailed balance, we know for a fact that the estimators \textit{must} converge eventually.
Consequentially, the Markov chain will be stuck near pathological regions for long periods to compensate
before it rapidly explores other parts of the typical set. This behaviour can be repeated, 
which makes estimators oscillate. Regardless of when the MCMC chain is terminated, the estimator will likely be biased due
to this oscillating behaviour.

\subsection{Geometric Ergodicity and Convergence Diagnostics}
Generation of ideal Markov chains is guaranteed if the transition kernel satisfies \textit{geometric ergodicity} \cite{geometric_ergodicity}, a \textit{Central Limit Theorem} for the MCMC estimators.
However, in most cases it is impossible to check that the condition is satisfied. Instead one uses a statistical quantitiy known
as the \textit{potential scale reduction factor} $\hat{R}$ \cite{rhat}. The ideal value is $\hat{R} = 1$. For values far away from this target,
it is unlikely that geometric ergodicity is satisfied. The Rule-of-thumb is to assume convergence
if $\hat{R} < 1.1$ \cite{convergence_diagnostics}.



\section{Metropolis-Hastings}
Construction of a transition kernel that ensures convergence to the typical set of the target distribution is a non-trivial problem in general.
Fortunately, the Metropolis-Hastings algorithm provides a general solution that lets us construct 
a transition kernel with this property \cite{metropolis,metropolis_two}. 
The algorithm consist of two components; a proposal of a new state and a correction step called the \textit{Metropolis correction}.
Given a state $\theta$, we propose a new state $\theta'$ by adding a random perturbation to the initial state.
The correction step rejects a proposed state that moves away from the typical set of the target distribution
and accepts proposals that stay within it.
The proposed state is formally sampled from a \textit{proposal distribution} $q(\theta'|\theta)$.
The probability of accepting the proposed state given the initial state, fittingly called the \textit{acceptance probability}, is 
\begin{equation}\label{eq:general_acceptance_prob}
    a(\theta'|\theta) = \min \left(1, \frac{q(\theta|\theta')\pi(\theta')}{q(\theta'|\theta)\pi (\theta)}\right).
\end{equation}
A particularly neat feature of the acceptance probability in eq.~\eqref{eq:general_acceptance_prob} is that it can be calculated in Bayesian applications because the evidence term cancels out.
 These steps are summarized in algorithm~\ref{algo:general_metropolis}.
\begin{figure}[H]
    \begin{algorithm}[H]
      \caption{Metropolis-Hastings}\label{algo:general_metropolis}
      \begin{algorithmic}
        \Function{{\tt MetropolisHastings}}{$\theta$}
        \State Sample $\theta' \sim q(\theta'|\theta)$
        \State $\displaystyle{a(\theta'|\theta) \leftarrow \min \left(1, \frac{q(\theta|\theta')\pi(\theta')}{q(\theta'|\theta)\pi (\theta)}\right)}$
        \State Sample $u \sim \text{Uniform}(0,1)$.
        \If {$a(\theta'|\theta) \geq u$}
          \State $\theta \leftarrow \theta'$ \Comment{Accept transition}
        \Else
          \State $\theta \leftarrow \theta$  \Comment{Reject transition}
        \EndIf 
        \State \Return $\theta$
        \EndFunction
      \end{algorithmic}
    \end{algorithm}
  \end{figure}

\subsection{The Proposal Distribution}
There are many valid choices of proposal distributions. A common choice is a Gaussian distribution $q(\theta'|\theta) = \mathcal{N}(\theta'|\theta, \Sigma)$,
where $\Sigma$ is the \textit{covariance matrix} of the normal distribution used to generate the perturbation of the initial state.
This is typically chosen to be the identity matrix $\Sigma = I$.

We will refer to the Metropolis-Hastings algorithm with this proprosal distribution as \textit{random walk Metropolis}.
More precisely, this means that a proposed state is given by
\begin{equation}
  \theta' = \theta + \delta,
\end{equation}
where $\delta \sim \mathcal{N}(0, \Sigma)$. This distribution is symmetric such that $q(\theta'|\theta) = q(\theta|\theta')$, implying that the acceptance probability
reduces to
\begin{equation}\label{eq:symmetric_acceptance_prob}
    a(\theta'|\theta) = \min \left(1, \frac{\pi(\theta')}{\pi(\theta)}\right).
\end{equation}
Hence, evaluation of the acceptance probability only require that we evaluate the target distribution at the initial state and the proposed state.

The random walk Metropolis algorithm does suffer from slow convergence to, and exploration of, the typical set in high-dimensional spaces.
This can be understood because of the following. 
As we increase the dimension of the sample space, the volume outside of the typical set becomes increasingly larger than the volume of the typical set itself.
This implies with increasing probability that a random perturbation of an arbitrary initial state will cause the proposed state
to lie outside the typical set for a fixed covariance matrix.
We can compensate
for this flaw by reducing the values of $\Sigma_{ij}$, but this will slow down exploration of the
sample space. The slow exploration also leads to a Markov chain where consecutive
samples embody
a relatively large measure of correlation. 
The quality of the resulting Markov chain tarnishes and successive samples must be discarded in order to properly evaluate eq.~\eqref{eq:expval}.
This process of discarding correlated successive samples in a Markov chain is called \textit{thinning}.
Fortunately, there exists a solution; \textit{gradient-informed} exploration of the sample space, manifested in
the form of \textit{Hamiltonian Monte Carlo}. This algorithm is a special case of a Metropolis-Hastings algorithm
in which the proposal distribution $q(\theta'|\theta)$ is a special one utilizing Hamiltonian dynamics and Gibbs sampling
to produce a new proposal state $\theta'$. This is the topic of the next chapter.

\section{Gibbs Sampling}
The final standard MCMC algorithm we need is the \textit{Gibbs} sampler. It plays a small
part of the sampling in HMC and so we should therefore briefly discuss it.
It is a MCMC sampling method used for multi-variate probability densities, and
so is only meaningful to discuss for $d > 1$ dimensions.
Suppose $\theta^{(t)}$ represents the parameters at iteration $t$. 
The next sample $\theta^{(t+1)}$ in the Markov chain is drawn according to some chosen
conditional distribution $p$ depending on the previous and current sample as follows
\begin{equation}\label{eq:gibbs_sampling}
  \theta_i^{(t+1)} \sim p(\theta_i|\theta_{1}^{(t+1)}, \ldots \theta_{i-1}^{(t+1)}, \theta_{i+1}^{(t)}, \ldots, \theta_{m}^{(t)}).
\end{equation}
We may summarize this as a function in algorithm~\ref{algo:gibbs} which 
given an initial state $\theta^{(t)}$ returns a new state $\theta^{(t+1)}$
sampled according to eq.~\eqref{eq:gibbs_sampling}.
\begin{figure}[H]
  \begin{algorithm}[H]
    \caption{Gibbs sampling}\label{algo:gibbs}
    \begin{algorithmic}
      \Function{{\tt Gibbs}}{$\theta^{(t)}$}
        \For{$i=1,\ldots, d$}
          \State Sample $\theta_i^{(t+1)} \sim p(\theta_i|\theta_{1}^{(t+1)}, \ldots \theta_{i-1}^{(t+1)}, \theta_{i+1}^{(t)}, \ldots, \theta_{m}^{(t)}).$
        \EndFor \\
        \Return $\theta^{(t+1)} = \left(\theta_1^{(t+1)}, \ldots, \theta_m^{(t+1)}\right)$.
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
\end{figure}


