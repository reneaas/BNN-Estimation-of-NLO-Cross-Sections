@book{neal2011,
	doi = {10.1201/b10905},
  
	url = {https://doi.org/10.1201%2Fb10905},
  
	year = 2011,
	month = {may},
  
	publisher = {Chapman and Hall/{CRC}
},
  
	editor = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  
	title = {Handbook of Markov Chain Monte Carlo}
}


@article{ml_for_physicists,
	doi = {10.1016/j.physrep.2019.03.001},
  
	url = {https://doi.org/10.1016%2Fj.physrep.2019.03.001},
  
	year = 2019,
	month = {may},
  
	publisher = {Elsevier {BV}
},
  
	volume = {810},
  
	pages = {1--124},
  
	author = {Pankaj Mehta and Marin Bukov and Ching-Hao Wang and Alexandre G.R. Day and Clint Richardson and Charles K. Fisher and David J. Schwab},
  
	title = {A high-bias, low-variance introduction to Machine Learning for physicists},
  
	journal = {Physics Reports}
}

@Article{Nesterov2009,
author={Nesterov, Yurii},
title={Primal-dual subgradient methods for convex problems},
journal={Mathematical Programming},
year={2009},
month={Aug},
day={01},
volume={120},
number={1},
pages={221-259},
abstract={In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.},
issn={1436-4646},
doi={10.1007/s10107-007-0149-x},
url={https://doi.org/10.1007/s10107-007-0149-x}
}



@ARTICLE{gibbs,
  author={Geman, Stuart and Geman, Donald},
  journal={{I}{E}{E}{E} {T}ransactions on {P}attern {A}nalysis and {M}achine {I}ntelligence}, 
  title={{S}tochastic Relaxation, {G}ibbs {D}istributions, and the {B}ayesian {R}estoration of {I}mages}, 
  year={1984},
  volume={PAMI-6},
  number={6},
  pages={721-741},
  doi={10.1109/TPAMI.1984.4767596},
}

@article{metropolis_two,
    author = {Hastings, W. K.},
    title = "{Monte Carlo sampling methods using Markov chains and their applications}",
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97-109},
    year = {1970},
    month = {04},
    abstract = "{A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}

@misc{conceptual_intro_hmc,
  doi = {10.48550/ARXIV.1701.02434},
  
  url = {https://arxiv.org/abs/1701.02434},
  
  author = {Betancourt, Michael},
  
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Conceptual Introduction to Hamiltonian Monte Carlo},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{geometric_ergodicity,
	doi = {10.1214/154957804100000024},
  
	url = {https://doi.org/10.1214%2F154957804100000024},
  
	year = 2004,
	month = {jan},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {1},
  
	number = {none},
  
	author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
  
	title = {General state space Markov chains and {MCMC} algorithms},
  
	journal = {Probability Surveys}
}


@misc{nuts,
      title={The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}, 
      author={Matthew D. Hoffman and Andrew Gelman},
      year={2011},
      eprint={1111.4246},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@inbook{numerical_recipies,
author={William H. Press et. al},
title={Numerical recipes in C : the art of scientific computing},
year={1992},
publisher={Second edition. Cambridge [Cambridgeshire] ; New York : Cambridge University Press, 1992.},
abstract={xxvi, 994 pages : illustrations ; 25 cm},
note={Includes bibliographical references (pages 926-929) and index.;Also available online.},
pages={824-826},
url={https://search.library.wisc.edu/catalog/999702229702121}
}

@article{metropolis,
author = {Metropolis,Nicholas  and Rosenbluth,Arianna W.  and Rosenbluth,Marshall N.  and Teller,Augusta H.  and Teller,Edward },
title = {{E}quation of {S}tate {C}alculations by {F}ast {C}omputing {M}achines},
journal = {The Journal of Chemical Physics},
volume = {21},
number = {6},
pages = {1087-1092},
year = {1953},
doi = {10.1063/1.1699114},
URL = {https://doi.org/10.1063/1.1699114},
eprint = {https://doi.org/10.1063/1.1699114},
}

@inbook{hmc,
   title={Handbook of Markov Chain Monte Carlo},
   author={Radford Neal},
   ISBN={9780429138508},
   url={http://dx.doi.org/10.1201/b10905},
   eprint={http://dx.doi.org/10.1201/b10905},
   chapter={5},
   DOI={10.1201/b10905},
   publisher={Chapman and Hall/CRC},
   year={2011},
   month={May},
}

@inbook{classical_mechanics,
  author         = {Helbert Goldstein, Charles Poole,  John Safko},
  chapter        = {2,8},
  publisher      = {Addison Wesley},
  title          = {Classical Mechanics, 3rd ed.},
  year           = {2000},
}

@inbook{leapfrog,
  author         = {Christopher M. Bishop},
  chapter        = {11},
  pages          = {551},
  publisher      = {Springer New York},
  title          = {Pattern Recognition and Machine Learning},
  year           = {2006}
}

@book{markov_chains,
  author         = {M. E. J. Newman and G. T. Barkema},
  publisher      = {Oxford University Press},
  title          = {Monte Carlo Methods in Statistical Physics},
  year           = {1991}
}


@article{backprop,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	da = {1986/10/01},
	date-added = {2021-11-22 02:13:05 +0100},
	date-modified = {2021-11-22 02:13:05 +0100},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	title = {Learning representations by back-propagating errors},
	ty = {JOUR},
	url = {https://doi.org/10.1038/323533a0},
	volume = {323},
	year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1038/323533a0}
}



@misc{tf,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{swish,
  author    = {Prajit Ramachandran and
               Barret Zoph and
               Quoc V. Le},
  title     = {Searching for Activation Functions},
  journal   = {CoRR},
  volume    = {abs/1710.05941},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.05941},
  eprinttype = {arXiv},
  eprint    = {1710.05941},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-05941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{relu,
  added-at = {2014-04-01T20:16:10.000+0200},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/256f5ffd25378f109c8cc14394bcfdabb/prlz77},
  booktitle = {AISTATS},
  crossref = {conf/aistats/2011},
  editor = {Gordon, Geoffrey J. and Dunson, David B. and Dud√≠k, Miroslav},
  ee = {http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf},
  interhash = {fbf04ef5079b11118f3f3184b1068d88},
  intrahash = {56f5ffd25378f109c8cc14394bcfdabb},
  keywords = {Bengio Deep Networks Neural Rectifier Relu Sparse},
  pages = {315-323},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  timestamp = {2014-04-01T20:16:10.000+0200},
  title = {Deep Sparse Rectifier Neural Networks.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlrp15.html#GlorotBB11},
  volume = 15,
  year = 2011
}

@inbook{bayes_theorem,
  title     = "Modern Mathematical Statistics with Applications",
  author    = "Devore, Jay L. and Berk, Kenneth N.",
  year      = 2018,
  publisher = "Springer",
  pages = "80",
}

@misc{ADAM,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{rhat,
author = {Andrew Gelman and Donald B. Rubin},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
volume = {7},
journal = {Statistical Science},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {457 -- 472},
keywords = {Bayesian inference, Convergence of stochastic processes, ECM, EM, Gibbs sampler, importance sampling, Metropolis algorithm, multiple imputation, random-effects model, SIR},
year = {1992},
doi = {10.1214/ss/1177011136},
URL = {https://doi.org/10.1214/ss/1177011136}
}


@inbook{convergence_diagnostics,
  title     = "Handbook of Markov Chain Monte Carlo",
  editor         = "Brooks, Steve and Gelman, Andrew and Jonas, Galin L. and Meng, Xiao-Li",
  year      = 2018,
  publisher = "Springer",
  chapter = "6",
}