

@InProceedings{chees-hmc,
  title = 	 { An Adaptive-MCMC Scheme for Setting Trajectory Lengths in Hamiltonian Monte Carlo },
  author =       {Hoffman, Matthew and Radul, Alexey and Sountsov, Pavel},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3907--3915},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/hoffman21a/hoffman21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/hoffman21a.html},
  abstract = 	 { Hamiltonian Monte Carlo (HMC) is a powerful MCMC algorithm based on simulating Hamiltonian dynamics. Its performance depends strongly on choosing appropriate values for two parameters: the step size used in the simulation, and how long the simulation runs for. The step-size parameter can be tuned using standard adaptive-MCMC strategies, but it is less obvious how to tune the simulation-length parameter. The no-U-turn sampler (NUTS) eliminates this problematic simulation-length parameter, but NUTS’s relatively complex control flow makes it difficult to efficiently run many parallel chains on accelerators such as GPUs. NUTS also spends some extra gradient evaluations relative to HMC in order to decide how long to run each iteration without violating detailed balance. We propose ChEES-HMC, a simple adaptive-MCMC scheme for automatically tuning HMC’s simulation-length parameter, which minimizes a proxy for the autocorrelation of the state’s second moments. We evaluate ChEES-HMC and NUTS on many tasks, and find that ChEES-HMC typically yields larger effective sample sizes per gradient evaluation than NUTS does. When running many chains on a GPU, ChEES-HMC can also run significantly more gradient evaluations per second than NUTS, allowing it to quickly provide accurate estimates of posterior expectations. }
}


@misc{jax,
  author = {Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hennigan, Tom and Hessel, Matteo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Martens, Lena and Mikulik, Vladimir and Norman, Tamara and Quan, John and Papamakarios, George and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stokowiec, Wojciech and Viola, Fabio},
  howpublished = {\url{http://github.com/deepmind}},
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  year         = {2020}
}


@article{colliderbit,
    author = "Bal\'azs, Csaba and others",
    collaboration = "GAMBIT",
    title = "{ColliderBit: a GAMBIT module for the calculation of high-energy collider observables and likelihoods}",
    eprint = "1705.07919",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    reportNumber = "gambit-code-2017",
    doi = "10.1140/epjc/s10052-017-5285-8",
    journal = "Eur. Phys. J. C",
    volume = "77",
    number = "11",
    pages = "795",
    year = "2017"
}

@article{colliderbit_old,
	doi = {10.1140/epjc/s10052-017-5285-8},
  
	url = {https://doi.org/10.1140%2Fepjc%2Fs10052-017-5285-8},
  
	year = 2017,
	month = {nov},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {77},
  
	number = {11},
  
	author = {Csaba Bal{\'{a}}zs and   and Andy Buckley and Lars A. Dal and Ben Farmer and Paul Jackson and Abram Krislock and Anders Kvellestad and Daniel Murnane and Antje Putze and Are Raklev and Christopher Rogan and Aldo Saavedra and Pat Scott and Christoph Weniger and Martin White},
  
	title = {{ColliderBit}: a {GAMBIT} module for the calculation of high-energy collider observables and likelihoods},
  
	journal = {The European Physical Journal C}
}

@article{universal_function_approximator,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}


@Article{r2_score,
author={Chicco, Davide
and Warrens, Matthijs J.
and Jurman, Giuseppe},
title={{The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation}},
journal={PeerJ. Computer science},
year={2021},
month={Jul},
day={05},
publisher={PeerJ Inc.},
volume={7},
pages={e623-e623},
keywords={Coefficient of determination; Mean absolute error; Mean square error; Regression; Regression analysis; Regression evaluation; Regression evaluation rates},
abstract={Regression analysis makes up a large part of supervised machine learning, and consists of the prediction of a continuous independent target from a set of other predictor variables. The difference between binary classification and regression is in the target range: in binary classification, the target can have only two values (usually encoded as 0 and 1), while in regression the target can have multiple values. Even if regression analysis has been employed in a huge number of machine learning studies, no consensus has been reached on a single, unified, standard metric to assess the results of the regression itself. Many studies employ the mean square error (MSE) and its rooted variant (RMSE), or the mean absolute error (MAE) and its percentage variant (MAPE). Although useful, these rates share a common drawback: since their values can range between zero and +infinity, a single value of them does not say much about the performance of the regression with respect to the distribution of the ground truth elements. In this study, we focus on two rates that actually generate a high score only if the majority of the elements of a ground truth group has been correctly predicted: the coefficient of determination (also known as R-squared or R (2)) and the symmetric mean absolute percentage error (SMAPE). After showing their mathematical properties, we report a comparison between R (2) and SMAPE in several use cases and in two real medical scenarios. Our results demonstrate that the coefficient of determination (R-squared) is more informative and truthful than SMAPE, and does not have the interpretability limitations of MSE, RMSE, MAE and MAPE. We therefore suggest the usage of R-squared as standard metric to evaluate regression analyses in any scientific domain.},
note={34307865[pmid]},
note={PMC8279135[pmcid]},
note={cs-623[PII]},
issn={2376-5992},
doi={10.7717/peerj-cs.623},
url={https://pubmed.ncbi.nlm.nih.gov/34307865},
url={https://doi.org/10.7717/peerj-cs.623},
language={eng}
}




@article{VEGAS,
title = {A new algorithm for adaptive multidimensional integration},
journal = {Journal of Computational Physics},
volume = {27},
number = {2},
pages = {192-203},
year = {1978},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(78)90004-9},
url = {https://www.sciencedirect.com/science/article/pii/0021999178900049},
author = {G {Peter Lepage}},
abstract = {A new general purpose algorithm for multidimensional integration is described. It is an iterative and adaptive Monte Carlo scheme. The new algorithm is compared with several others currently in use, and shown to be considerably more efficient than all of these for a number of sample integrals of high dimension (n ⪆ 4).}
}

@article{xsec,
    author = "Buckley, Andy and Kvellestad, Anders and Raklev, Are and Scott, Pat and Sparre, Jon Vegard and Van Den Abeele, Jeriek and Vazquez-Holm, Ingrid A.",
    title = "{Xsec: the cross-section evaluation code}",
    eprint = "2006.16273",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    reportNumber = "SAGEX-20-17-E",
    doi = "10.1140/epjc/s10052-020-08635-y",
    journal = "Eur. Phys. J. C",
    volume = "80",
    number = "12",
    pages = "1106",
    year = "2020"
}



@article{prospino,
    author = "Beenakker, W. and Hopker, R. and Spira, M.",
    title = "{PROSPINO: A Program for the production of supersymmetric particles in next-to-leading order QCD}",
    eprint = "hep-ph/9611232",
    archivePrefix = "arXiv",
    month = "11",
    year = "1996"
}

@misc{xla,
title	= {{X}{L}{A} : {C}ompiling {M}achine {L}earning for {P}eak {P}erformance},
author	= {Amit Sabne},
year	= {2020}
}





@book{neal2011,
	doi = {10.1201/b10905},
  
	url = {https://doi.org/10.1201%2Fb10905},
  
	year = 2011,
	month = {may},
  
	publisher = {Chapman and Hall/{CRC}
},
  
	editor = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  
	title = {{H}andbook of {M}arkov {C}hain {M}onte {C}arlo}
}


@article{ml_for_physicists,
	doi = {10.1016/j.physrep.2019.03.001},
  
	url = {https://doi.org/10.1016%2Fj.physrep.2019.03.001},
  
	year = 2019,
	month = {may},
  
	publisher = {Elsevier {BV}
},
  
	volume = {810},
  
	pages = {1--124},
  
	author = {Pankaj Mehta and Marin Bukov and Ching-Hao Wang and Alexandre G.R. Day and Clint Richardson and Charles K. Fisher and David J. Schwab},
  
	title = {A high-bias, low-variance introduction to {M}achine {L}earning for physicists},
  
	journal = {Physics Reports}
}

@Article{Nesterov2009,
author={Nesterov, Yurii},
title={Primal-dual subgradient methods for convex problems},
journal={Mathematical Programming},
year={2009},
month={Aug},
day={01},
volume={120},
number={1},
pages={221-259},
abstract={In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.},
issn={1436-4646},
doi={10.1007/s10107-007-0149-x},
url={https://doi.org/10.1007/s10107-007-0149-x}
}



@ARTICLE{gibbs,
  author={Geman, Stuart and Geman, Donald},
  journal={{I}{E}{E}{E} {T}ransactions on {P}attern {A}nalysis and {M}achine {I}ntelligence}, 
  title={{S}tochastic Relaxation, {G}ibbs {D}istributions, and the {B}ayesian {R}estoration of {I}mages}, 
  year={1984},
  volume={PAMI-6},
  number={6},
  pages={721-741},
  doi={10.1109/TPAMI.1984.4767596},
}

@article{metropolis_two,
    author = {Hastings, W. K.},
    title = "{Monte Carlo sampling methods using Markov chains and their applications}",
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97-109},
    year = {1970},
    month = {04},
    abstract = "{A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}

@article{swish,
  author    = {Prajit Ramachandran and
               Barret Zoph and
               Quoc V. Le},
  title     = {Searching for Activation Functions},
  journal   = {CoRR},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.05941},
  eprinttype = {arXiv},
  eprint    = {1710.05941},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-05941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{google_bnn_posteriors,
  author    = {Pavel Izmailov and
               Sharad Vikram and
               Matthew D. Hoffman and
               Andrew Gordon Wilson},
  title     = {{W}hat {A}re {B}ayesian {N}eural {N}etwork {P}osteriors {R}eally {L}ike?},
  journal   = {CoRR},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.14421},
  eprinttype = {arXiv},
  eprint    = {2104.14421},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-14421.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nuts_joonha_park,
  author = {Park, Joonha and Atchadé, Yves F.},
  title = {{Markov Chain Monte Carlo Algorithms with Sequential Proposals}},
  year = {2019},
  url = {https://arxiv.org/abs/1907.06544},
  eprinttype = {arXiv},
  eprint = {1907.06544},
}

@article{tf_old,
  author       = {Tensorflow Developers},
  title        = {TensorFlow},
  month        = may,
  year         = 2022,
  note         = {{Specific TensorFlow versions can be found in the 
                   "Versions" list on the right side of this
                   page.<br>See the full list of authors <a href="htt
                   ps://github.com/tensorflow/tensorflow/graphs/contr
                   ibutors">on GitHub</a>.}},
  publisher    = {Zenodo},
  version      = {v2.8.2},
  doi          = {10.5281/zenodo.6574269},
  url          = {https://doi.org/10.5281/zenodo.6574269},
  eprint = {10.5281/zenodo.6574269},
  
}

@misc{tf,
  author = {{Tensorflow Developers, Google}},
  title = {TensorFlow},
  howpublished = {\url{https://doi.org/10.5281/zenodo.6574269}},
  year = {2022},
  month = may,
}


@misc{rene_alexander_ask_2022_6800167,
  author       = {René Alexander Ask},
  howpublished = {\url{https://doi.org/10.5281/zenodo.6800167}},
  title        = {{Bayesian neural network implementation in Tensorflow}},
  year         = {2022},
  month = jul,
}


@article{tfp,
  author = {Lao, Junpeng and Suter, Christopher and Langmore, Ian and Chimisov, Cyril and Saxena, Ashish and Sountsov, Pavel and Moore, Dave and Saurous, Rif A. and Hoffman, Matthew D. and Dillon, Joshua V.},
  title = {tfp.mcmc: Modern Markov Chain Monte Carlo Tools Built for Modern Hardware},
  year = {2020},
  url = {https://arxiv.org/abs/2002.01184},
  eprinttype = {arXiv},
  eprint = {2002.01184},
  publisher = {arXiv},
}

@article{conceptual_intro_hmc,
  author = {Betancourt, Michael},
  title = {A Conceptual Introduction to Hamiltonian Monte Carlo},
  year = {2017},
  url = {https://arxiv.org/abs/1701.02434},
  eprinttype = {arXiv},
  eprint = {1701.02434}
}




@article{geometric_ergodicity,
	doi = {10.1214/154957804100000024},
  
	url = {https://doi.org/10.1214%2F154957804100000024},
  
	year = 2004,
	month = {jan},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {1},
  
	number = {none},
  
	author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
  
	title = {General state space Markov chains and {MCMC} algorithms},
  
	journal = {Probability Surveys}
}


@article{nuts,
  author  = {Matthew D. Hoffman and Andrew Gelman},
  title   = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {47},
  pages   = {1593--1623},
  url     = {http://jmlr.org/papers/v15/hoffman14a.html}
}


@inbook{numerical_recipies,
author={William H. Press et. al},
title={Numerical recipes in C : the art of scientific computing},
year={1992},
publisher={Second edition. Cambridge [Cambridgeshire] ; New York : Cambridge University Press, 1992.},
abstract={xxvi, 994 pages : illustrations ; 25 cm},
note={Includes bibliographical references (pages 926-929) and index.;Also available online.},
pages={824-826},
url={https://search.library.wisc.edu/catalog/999702229702121}
}

@article{metropolis,
author = {Metropolis,Nicholas  and Rosenbluth,Arianna W.  and Rosenbluth,Marshall N.  and Teller,Augusta H.  and Teller,Edward },
title = {{E}quation of {S}tate {C}alculations by {F}ast {C}omputing {M}achines},
journal = {The Journal of Chemical Physics},
volume = {21},
number = {6},
pages = {1087-1092},
year = {1953},
doi = {10.1063/1.1699114},
URL = {https://doi.org/10.1063/1.1699114},
eprint = {https://doi.org/10.1063/1.1699114},
}

@inbook{hmc,
   title={Handbook of Markov Chain Monte Carlo},
   author={Radford Neal},
   ISBN={9780429138508},
   url={http://dx.doi.org/10.1201/b10905},
   eprint={http://dx.doi.org/10.1201/b10905},
   chapter={5},
   DOI={10.1201/b10905},
   publisher={Chapman and Hall/CRC},
   year={2011},
   month={May},
}

@inbook{classical_mechanics,
  author         = {Helbert Goldstein, Charles Poole,  John Safko},
  chapter        = {2,8},
  publisher      = {Addison Wesley},
  title          = {Classical Mechanics, 3rd ed.},
  year           = {2000},
}

@inbook{leapfrog,
  author         = {Christopher M. Bishop},
  chapter        = {11},
  pages          = {551},
  publisher      = {Springer New York},
  title          = {Pattern Recognition and Machine Learning},
  year           = {2006}
}

@book{markov_chains,
  author         = {M. E. J. Newman and G. T. Barkema},
  publisher      = {Oxford University Press},
  title          = {Monte Carlo Methods in Statistical Physics},
  year           = {1991}
}


@article{backprop,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	da = {1986/10/01},
	date-added = {2021-11-22 02:13:05 +0100},
	date-modified = {2021-11-22 02:13:05 +0100},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	title = {Learning representations by back-propagating errors},
	ty = {JOUR},
	url = {https://doi.org/10.1038/323533a0},
	volume = {323},
	year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1038/323533a0}
}


@misc{tf_old,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@inproceedings{relu,
  added-at = {2014-04-01T20:16:10.000+0200},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/256f5ffd25378f109c8cc14394bcfdabb/prlz77},
  booktitle = {AISTATS},
  crossref = {conf/aistats/2011},
  editor = {Gordon, Geoffrey J. and Dunson, David B. and Dudík, Miroslav},
  ee = {http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf},
  interhash = {fbf04ef5079b11118f3f3184b1068d88},
  intrahash = {56f5ffd25378f109c8cc14394bcfdabb},
  keywords = {Bengio Deep Networks Neural Rectifier Relu Sparse},
  pages = {315-323},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  timestamp = {2014-04-01T20:16:10.000+0200},
  title = {Deep Sparse Rectifier Neural Networks.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlrp15.html#GlorotBB11},
  volume = 15,
  year = 2011
}

@inbook{bayes_theorem,
  title     = "Modern Mathematical Statistics with Applications",
  author    = "Devore, Jay L. and Berk, Kenneth N.",
  year      = 2018,
  publisher = "Springer",
  pages = "80",
}

@inproceedings{ADAM,
    author = "Kingma, Diederik P. and Ba, Jimmy",
    title = "{Adam: A Method for Stochastic Optimization}",
    eprint = "1412.6980",
    archivePrefix = "arXiv",
    primaryClass = "cs.LG",
    month = "12",
    year = "2014"
}


@article{rhat,
author = {Andrew Gelman and Donald B. Rubin},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
volume = {7},
journal = {Statistical Science},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {457 -- 472},
keywords = {Bayesian inference, Convergence of stochastic processes, ECM, EM, Gibbs sampler, importance sampling, Metropolis algorithm, multiple imputation, random-effects model, SIR},
year = {1992},
doi = {10.1214/ss/1177011136},
URL = {https://doi.org/10.1214/ss/1177011136}
}


@inbook{convergence_diagnostics,
  title     = "Handbook of Markov Chain Monte Carlo",
  editor    = "Brooks, Steve and Gelman, Andrew and Jonas, Galin L. and Meng, Xiao-Li",
  year      = 2018,
  publisher = "Springer",
  chapter = "6",
}