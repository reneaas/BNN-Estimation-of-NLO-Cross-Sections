@article{colliderbit,
	doi = {10.1140/epjc/s10052-017-5285-8},
  
	url = {https://doi.org/10.1140%2Fepjc%2Fs10052-017-5285-8},
  
	year = 2017,
	month = {nov},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {77},
  
	number = {11},
  
	author = {Csaba Bal{\'{a}}zs and   and Andy Buckley and Lars A. Dal and Ben Farmer and Paul Jackson and Abram Krislock and Anders Kvellestad and Daniel Murnane and Antje Putze and Are Raklev and Christopher Rogan and Aldo Saavedra and Pat Scott and Christoph Weniger and Martin White},
  
	title = {{ColliderBit}: a {GAMBIT} module for the calculation of high-energy collider observables and likelihoods},
  
	journal = {The European Physical Journal C}
}

@misc{universal_function_approximator,
  doi = {10.48550/ARXIV.2102.10993},
  
  url = {https://arxiv.org/abs/2102.10993},
  
  author = {Nishijima, Takato},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Universal Approximation Theorem for Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@Article{r2_score,
author={Chicco, Davide
and Warrens, Matthijs J.
and Jurman, Giuseppe},
title={The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation},
journal={PeerJ. Computer science},
year={2021},
month={Jul},
day={05},
publisher={PeerJ Inc.},
volume={7},
pages={e623-e623},
keywords={Coefficient of determination; Mean absolute error; Mean square error; Regression; Regression analysis; Regression evaluation; Regression evaluation rates},
abstract={Regression analysis makes up a large part of supervised machine learning, and consists of the prediction of a continuous independent target from a set of other predictor variables. The difference between binary classification and regression is in the target range: in binary classification, the target can have only two values (usually encoded as 0 and 1), while in regression the target can have multiple values. Even if regression analysis has been employed in a huge number of machine learning studies, no consensus has been reached on a single, unified, standard metric to assess the results of the regression itself. Many studies employ the mean square error (MSE) and its rooted variant (RMSE), or the mean absolute error (MAE) and its percentage variant (MAPE). Although useful, these rates share a common drawback: since their values can range between zero and +infinity, a single value of them does not say much about the performance of the regression with respect to the distribution of the ground truth elements. In this study, we focus on two rates that actually generate a high score only if the majority of the elements of a ground truth group has been correctly predicted: the coefficient of determination (also known as R-squared or R (2)) and the symmetric mean absolute percentage error (SMAPE). After showing their mathematical properties, we report a comparison between R (2) and SMAPE in several use cases and in two real medical scenarios. Our results demonstrate that the coefficient of determination (R-squared) is more informative and truthful than SMAPE, and does not have the interpretability limitations of MSE, RMSE, MAE and MAPE. We therefore suggest the usage of R-squared as standard metric to evaluate regression analyses in any scientific domain.},
note={34307865[pmid]},
note={PMC8279135[pmcid]},
note={cs-623[PII]},
issn={2376-5992},
doi={10.7717/peerj-cs.623},
url={https://pubmed.ncbi.nlm.nih.gov/34307865},
url={https://doi.org/10.7717/peerj-cs.623},
language={eng}
}



@misc{nuts_joonha_park,
  doi = {10.48550/ARXIV.1907.06544},
  
  url = {https://arxiv.org/abs/1907.06544},
  
  author = {Park, Joonha and Atchadé, Yves F.},
  
  keywords = {Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Markov chain Monte Carlo algorithms with sequential proposals},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{VEGAS,
title = {A new algorithm for adaptive multidimensional integration},
journal = {Journal of Computational Physics},
volume = {27},
number = {2},
pages = {192-203},
year = {1978},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(78)90004-9},
url = {https://www.sciencedirect.com/science/article/pii/0021999178900049},
author = {G {Peter Lepage}},
abstract = {A new general purpose algorithm for multidimensional integration is described. It is an iterative and adaptive Monte Carlo scheme. The new algorithm is compared with several others currently in use, and shown to be considerably more efficient than all of these for a number of sample integrals of high dimension (n ⪆ 4).}
}



@article{xsec,
	doi = {10.1140/epjc/s10052-020-08635-y},
  
	url = {https://doi.org/10.1140%2Fepjc%2Fs10052-020-08635-y},
  
	year = 2020,
	month = {dec},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {80},
  
	number = {12},
  
	author = {Andy Buckley and Anders Kvellestad and Are Raklev and Pat Scott and Jon Vegard Sparre and Jeriek Van den Abeele and Ingrid A. Vazquez-Holm},
  
	title = {Xsec: the cross-section evaluation code},
  
	journal = {The European Physical Journal C}
}

@misc{prospino,
  doi = {10.48550/ARXIV.HEP-PH/9611232},
  
  url = {https://arxiv.org/abs/hep-ph/9611232},
  
  author = {Beenakker, W. and Hoepker, R. and Spira, M.},
  
  keywords = {High Energy Physics - Phenomenology (hep-ph), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {PROSPINO: A Program for the Production of Supersymmetric Particles in Next-to-leading Order QCD},
  
  publisher = {arXiv},
  
  year = {1996},
  
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}


@article{google_bnn_posteriors,
  author    = {Pavel Izmailov and
               Sharad Vikram and
               Matthew D. Hoffman and
               Andrew Gordon Wilson},
  title     = {{W}hat {A}re {B}ayesian {N}eural {N}etwork {P}osteriors {R}eally {L}ike?},
  journal   = {CoRR},
  volume    = {abs/2104.14421},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.14421},
  eprinttype = {arXiv},
  eprint    = {2104.14421},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-14421.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{xla,
title	= {{X}{L}{A} : {C}ompiling {M}achine {L}earning for {P}eak {P}erformance},
author	= {Amit Sabne},
year	= {2020}
}


@book{neal2011,
	doi = {10.1201/b10905},
  
	url = {https://doi.org/10.1201%2Fb10905},
  
	year = 2011,
	month = {may},
  
	publisher = {Chapman and Hall/{CRC}
},
  
	editor = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  
	title = {{H}andbook of {M}arkov {C}hain {M}onte {C}arlo}
}


@article{ml_for_physicists,
	doi = {10.1016/j.physrep.2019.03.001},
  
	url = {https://doi.org/10.1016%2Fj.physrep.2019.03.001},
  
	year = 2019,
	month = {may},
  
	publisher = {Elsevier {BV}
},
  
	volume = {810},
  
	pages = {1--124},
  
	author = {Pankaj Mehta and Marin Bukov and Ching-Hao Wang and Alexandre G.R. Day and Clint Richardson and Charles K. Fisher and David J. Schwab},
  
	title = {A high-bias, low-variance introduction to {M}achine {L}earning for physicists},
  
	journal = {Physics Reports}
}

@Article{Nesterov2009,
author={Nesterov, Yurii},
title={Primal-dual subgradient methods for convex problems},
journal={Mathematical Programming},
year={2009},
month={Aug},
day={01},
volume={120},
number={1},
pages={221-259},
abstract={In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.},
issn={1436-4646},
doi={10.1007/s10107-007-0149-x},
url={https://doi.org/10.1007/s10107-007-0149-x}
}



@ARTICLE{gibbs,
  author={Geman, Stuart and Geman, Donald},
  journal={{I}{E}{E}{E} {T}ransactions on {P}attern {A}nalysis and {M}achine {I}ntelligence}, 
  title={{S}tochastic Relaxation, {G}ibbs {D}istributions, and the {B}ayesian {R}estoration of {I}mages}, 
  year={1984},
  volume={PAMI-6},
  number={6},
  pages={721-741},
  doi={10.1109/TPAMI.1984.4767596},
}

@article{metropolis_two,
    author = {Hastings, W. K.},
    title = "{Monte Carlo sampling methods using Markov chains and their applications}",
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97-109},
    year = {1970},
    month = {04},
    abstract = "{A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}

@misc{conceptual_intro_hmc,
  doi = {10.48550/ARXIV.1701.02434},
  
  url = {https://arxiv.org/abs/1701.02434},
  
  author = {Betancourt, Michael},
  
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{A} {C}onceptual {I}ntroduction to {H}amiltonian {M}onte {C}arlo},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{geometric_ergodicity,
	doi = {10.1214/154957804100000024},
  
	url = {https://doi.org/10.1214%2F154957804100000024},
  
	year = 2004,
	month = {jan},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {1},
  
	number = {none},
  
	author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
  
	title = {General state space Markov chains and {MCMC} algorithms},
  
	journal = {Probability Surveys}
}


@misc{nuts,
      title={The {N}o-{U}-{T}urn {S}ampler: {A}daptively {S}etting {P}ath {L}engths in {H}amiltonian {M}onte {C}arlo}, 
      author={Matthew D. Hoffman and Andrew Gelman},
      year={2011},
      eprint={1111.4246},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@inbook{numerical_recipies,
author={William H. Press et. al},
title={Numerical recipes in C : the art of scientific computing},
year={1992},
publisher={Second edition. Cambridge [Cambridgeshire] ; New York : Cambridge University Press, 1992.},
abstract={xxvi, 994 pages : illustrations ; 25 cm},
note={Includes bibliographical references (pages 926-929) and index.;Also available online.},
pages={824-826},
url={https://search.library.wisc.edu/catalog/999702229702121}
}

@article{metropolis,
author = {Metropolis,Nicholas  and Rosenbluth,Arianna W.  and Rosenbluth,Marshall N.  and Teller,Augusta H.  and Teller,Edward },
title = {{E}quation of {S}tate {C}alculations by {F}ast {C}omputing {M}achines},
journal = {The Journal of Chemical Physics},
volume = {21},
number = {6},
pages = {1087-1092},
year = {1953},
doi = {10.1063/1.1699114},
URL = {https://doi.org/10.1063/1.1699114},
eprint = {https://doi.org/10.1063/1.1699114},
}

@inbook{hmc,
   title={Handbook of Markov Chain Monte Carlo},
   author={Radford Neal},
   ISBN={9780429138508},
   url={http://dx.doi.org/10.1201/b10905},
   eprint={http://dx.doi.org/10.1201/b10905},
   chapter={5},
   DOI={10.1201/b10905},
   publisher={Chapman and Hall/CRC},
   year={2011},
   month={May},
}

@inbook{classical_mechanics,
  author         = {Helbert Goldstein, Charles Poole,  John Safko},
  chapter        = {2,8},
  publisher      = {Addison Wesley},
  title          = {Classical Mechanics, 3rd ed.},
  year           = {2000},
}

@inbook{leapfrog,
  author         = {Christopher M. Bishop},
  chapter        = {11},
  pages          = {551},
  publisher      = {Springer New York},
  title          = {Pattern Recognition and Machine Learning},
  year           = {2006}
}

@book{markov_chains,
  author         = {M. E. J. Newman and G. T. Barkema},
  publisher      = {Oxford University Press},
  title          = {Monte Carlo Methods in Statistical Physics},
  year           = {1991}
}


@article{backprop,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	da = {1986/10/01},
	date-added = {2021-11-22 02:13:05 +0100},
	date-modified = {2021-11-22 02:13:05 +0100},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	title = {Learning representations by back-propagating errors},
	ty = {JOUR},
	url = {https://doi.org/10.1038/323533a0},
	volume = {323},
	year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1038/323533a0}
}



@misc{tf,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{swish,
  author    = {Prajit Ramachandran and
               Barret Zoph and
               Quoc V. Le},
  title     = {Searching for Activation Functions},
  journal   = {CoRR},
  volume    = {abs/1710.05941},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.05941},
  eprinttype = {arXiv},
  eprint    = {1710.05941},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-05941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{relu,
  added-at = {2014-04-01T20:16:10.000+0200},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/256f5ffd25378f109c8cc14394bcfdabb/prlz77},
  booktitle = {AISTATS},
  crossref = {conf/aistats/2011},
  editor = {Gordon, Geoffrey J. and Dunson, David B. and Dudík, Miroslav},
  ee = {http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf},
  interhash = {fbf04ef5079b11118f3f3184b1068d88},
  intrahash = {56f5ffd25378f109c8cc14394bcfdabb},
  keywords = {Bengio Deep Networks Neural Rectifier Relu Sparse},
  pages = {315-323},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  timestamp = {2014-04-01T20:16:10.000+0200},
  title = {Deep Sparse Rectifier Neural Networks.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlrp15.html#GlorotBB11},
  volume = 15,
  year = 2011
}

@inbook{bayes_theorem,
  title     = "Modern Mathematical Statistics with Applications",
  author    = "Devore, Jay L. and Berk, Kenneth N.",
  year      = 2018,
  publisher = "Springer",
  pages = "80",
}

@misc{ADAM,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{rhat,
author = {Andrew Gelman and Donald B. Rubin},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
volume = {7},
journal = {Statistical Science},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {457 -- 472},
keywords = {Bayesian inference, Convergence of stochastic processes, ECM, EM, Gibbs sampler, importance sampling, Metropolis algorithm, multiple imputation, random-effects model, SIR},
year = {1992},
doi = {10.1214/ss/1177011136},
URL = {https://doi.org/10.1214/ss/1177011136}
}


@inbook{convergence_diagnostics,
  title     = "Handbook of Markov Chain Monte Carlo",
  editor         = "Brooks, Steve and Gelman, Andrew and Jonas, Galin L. and Meng, Xiao-Li",
  year      = 2018,
  publisher = "Springer",
  chapter = "6",
}