Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Wilson2020,
abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility.},
archivePrefix = {arXiv},
arxivId = {2002.08791},
author = {Wilson, Andrew Gordon and Izmailov, Pavel},
eprint = {2002.08791},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/NeurIPS-2020-bayesian-deep-learning-and-a-probabilistic-perspective-of-generalization-Paper.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{Bayesian deep learning and a probabilistic perspective of generalization}},
volume = {2020-Decem},
year = {2020}
}
@article{Mullachery2018,
abstract = {This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems},
author = {Mullachery, Vikram. and Khera, Aniruddh and Husain, Amir},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/1801.07710.pdf:pdf},
issn = {0104-6500},
journal = {arXiv preprint arXiv:1801.07710},
pages = {1--16},
title = {{A study of Bayesian Neural Networks}},
year = {2018}
}
@article{Kronheim2022,
abstract = {TensorBNN is a new package based on TensorFlow that implements Bayesian inference for modern neural network models. The posterior density of neural network model parameters is represented as a point cloud sampled using Hamiltonian Monte Carlo. The TensorBNN package leverages TensorFlow's architecture and its ability to use modern graphics processing units in both the training and prediction stages.},
archivePrefix = {arXiv},
arxivId = {2009.14393},
author = {Kronheim, B. S. and Kuchera, M. P. and Prosper, H. B.},
doi = {10.1016/j.cpc.2021.108168},
eprint = {2009.14393},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/bnn_tensorflow.pdf:pdf},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Bayesian neural networks,Hamiltonian Monte Carlo,Machine learning,TensorFlow},
pages = {1--20},
title = {{TensorBNN: Bayesian inference for neural networks using TensorFlow}},
volume = {270},
year = {2022}
}
@article{Kononenko1989,
abstract = {A neural network that uses the basic Hebbian learning rule and the Bayesian combination function is defined. Analogously to Hopfield's neural network, the convergence for the Bayesian neural network that asynchronously updates its neurons' states is proved. The performance of the Bayesian neural network in four medical domains is compared with various classification methods. The Bayesian neural network uses more sophisticated combination function than Hopfield's neural network and uses more economically the available information. The "naive" Bayesian classifier typically outperforms the basic Bayesian neural network since iterations in network make too many mistakes. By restricting the number of iterations and increasing the number of fixed points the network performs better than the naive Bayesian classifier. The Bayesian neural network is designed to learn very quickly and incrementally. {\textcopyright} 1989 Springer-Verlag.},
author = {Kononenko, I.},
doi = {10.1007/BF00200801},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/2106.13594.pdf:pdf},
issn = {03401200},
journal = {Biological Cybernetics},
number = {5},
pages = {361--370},
title = {{Bayesian neural networks}},
volume = {61},
year = {1989}
}
@article{MARTIN1998,
abstract = {I provide a pedagogical introduction to supersymmetry. The level of discussion is aimed at readers who are familiar with the Standard Model and quantum field theory, but who have had little or no prior exposure to supersymmetry. Topics covered include: motivations for supersymmetry, the construction of supersymmetric Lagrangians, superspace and superfields, soft supersymmetry-breaking interactions, the Minimal Supersymmetric Standard Model (MSSM), R-parity and its consequences, the origins of supersymmetry breaking, the mass spectrum of the MSSM, decays of supersymmetric particles, experimental signals for supersymmetry, and some extensions of the minimal framework.},
archivePrefix = {arXiv},
arxivId = {hep-ph/9709356},
author = {MARTIN, STEPHEN P.},
doi = {10.1142/9789812839657_0001},
eprint = {9709356},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/primer_to_supersymmetry.pdf:pdf},
number = {January},
pages = {1--98},
primaryClass = {hep-ph},
title = {{a Supersymmetry Primer}},
year = {1998}
}
@article{Polson2017,
abstract = {Deep learning is a form of machine learning for nonlinear high dimensional pattern matching and prediction. By taking a Bayesian probabilistic perspective, we provide a number of insights into more efficient algorithms for optimisation and hyper-parameter tuning. Traditional high-dimensional data reduction techniques, such as principal component analysis (PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regression (PPR) are all shown to be shallow learners. Their deep learning counterparts exploit multiple deep layers of data reduction which provide predictive performance gains. Stochastic gradient descent (SGD) training optimisation and Dropout (DO) regularization provide estimation and variable selection. Bayesian regularization is central to finding weights and connections in networks to optimize the predictive bias-variance trade-off. To illustrate our methodology, we provide an analysis of international bookings on Airbnb. Finally, we conclude with directions for future research.},
archivePrefix = {arXiv},
arxivId = {1706.00473},
author = {Polson, Nicholas G. and Sokolov, Vadim},
doi = {10.1214/17-BA1082},
eprint = {1706.00473},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/1706.00473.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Artificial Intelligence,Bayesian hierarchical models,Deep learning,LSTM models,Machine learning,Pattern matching,Prediction,TensorFlow},
number = {4},
pages = {1275--1304},
title = {{Deep learning: A Bayesian perspective}},
volume = {12},
year = {2017}
}
@article{Jospin2020,
abstract = {Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian Neural Networks, i.e. Stochastic Artificial Neural Networks trained using Bayesian methods.},
archivePrefix = {arXiv},
arxivId = {2007.06823},
author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
eprint = {2007.06823},
file = {:Users/reneaas/Documents/skole/master/thesis/articles/hands_on_bnn.pdf:pdf},
title = {{Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users}},
url = {http://arxiv.org/abs/2007.06823},
year = {2020}
}

@misc{tf,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}
