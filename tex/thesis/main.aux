\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\babel@aux{UKenglish}{}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Physics Problem}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Computation of Beyond the Standard Model Cross Sections}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Formulation of Machine Learning}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bayesian_ml}{{2}{5}{Bayesian Formulation of Machine Learning}{chapter.2}{}}
\newlabel{chap:bayesian_ml@cref}{{[chapter][2][]2}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Core of Machine Learning}{5}{section.2.1}\protected@file@percent }
\newlabel{eq:model_assumption}{{2.1}{5}{The Core of Machine Learning}{equation.2.1.1}{}}
\newlabel{eq:model_assumption@cref}{{[equation][1][2]2.1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Loss Functions}{5}{subsection.2.1.1}\protected@file@percent }
\newlabel{eq:rss}{{2.2}{5}{Loss Functions}{equation.2.1.2}{}}
\newlabel{eq:rss@cref}{{[equation][2][2]2.2}{[1][5][]5}}
\newlabel{eq:mse}{{2.3}{5}{Loss Functions}{equation.2.1.3}{}}
\newlabel{eq:mse@cref}{{[equation][3][2]2.3}{[1][5][]5}}
\citation{ADAM}
\citation{bayes_theorem}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Regularization}{6}{subsection.2.1.2}\protected@file@percent }
\newlabel{eq:loss_l2_reg}{{2.4}{6}{Regularization}{equation.2.1.4}{}}
\newlabel{eq:loss_l2_reg@cref}{{[equation][4][2]2.4}{[1][6][]6}}
\newlabel{eq:loss_l1_reg}{{2.5}{6}{Regularization}{equation.2.1.5}{}}
\newlabel{eq:loss_l1_reg@cref}{{[equation][5][2]2.5}{[1][6][]6}}
\newlabel{eq:loss_fn}{{2.6}{6}{Regularization}{equation.2.1.6}{}}
\newlabel{eq:loss_fn@cref}{{[equation][6][2]2.6}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Optimization}{6}{subsection.2.1.3}\protected@file@percent }
\newlabel{eq:optimal_param}{{2.8}{6}{Optimization}{equation.2.1.8}{}}
\newlabel{eq:optimal_param@cref}{{[equation][8][2]2.8}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayes' theorem}{6}{section.2.2}\protected@file@percent }
\citation{ml_for_physicists}
\newlabel{eq:bayes_theorem}{{2.10}{7}{Bayes' theorem}{equation.2.2.10}{}}
\newlabel{eq:bayes_theorem@cref}{{[equation][10][2]2.10}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Bayesian Framework for Machine Learning}{7}{section.2.3}\protected@file@percent }
\newlabel{eq:mle}{{2.11}{7}{Bayesian Framework for Machine Learning}{equation.2.3.11}{}}
\newlabel{eq:mle@cref}{{[equation][11][2]2.11}{[1][7][]7}}
\newlabel{eq:map}{{2.12}{7}{Bayesian Framework for Machine Learning}{equation.2.3.12}{}}
\newlabel{eq:map@cref}{{[equation][12][2]2.12}{[1][7][]7}}
\newlabel{eq:likelihood_fn}{{2.17}{8}{Bayesian Framework for Machine Learning}{equation.2.3.17}{}}
\newlabel{eq:likelihood_fn@cref}{{[equation][17][2]2.17}{[1][7][]8}}
\newlabel{eq:gaussian_prior}{{2.19}{8}{Bayesian Framework for Machine Learning}{equation.2.3.19}{}}
\newlabel{eq:gaussian_prior@cref}{{[equation][19][2]2.19}{[1][8][]8}}
\newlabel{eq:posterior_function_of_loss}{{2.22}{8}{Bayesian Framework for Machine Learning}{equation.2.3.22}{}}
\newlabel{eq:posterior_function_of_loss@cref}{{[equation][22][2]2.22}{[1][8][]8}}
\newlabel{eq:loss_function_of_posterior}{{2.24}{8}{Bayesian Framework for Machine Learning}{equation.2.3.24}{}}
\newlabel{eq:loss_function_of_posterior@cref}{{[equation][24][2]2.24}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Bayesian Inference}{8}{section.2.4}\protected@file@percent }
\newlabel{eq:predictive_distribution}{{2.25}{9}{Bayesian Inference}{equation.2.4.25}{}}
\newlabel{eq:predictive_distribution@cref}{{[equation][25][2]2.25}{[1][9][]9}}
\newlabel{eq:predictive_dist_approx}{{2.26}{9}{Bayesian Inference}{equation.2.4.26}{}}
\newlabel{eq:predictive_dist_approx@cref}{{[equation][26][2]2.26}{[1][9][]9}}
\newlabel{eq:bayesian_expval}{{2.27}{9}{Bayesian Inference}{equation.2.4.27}{}}
\newlabel{eq:bayesian_expval@cref}{{[equation][27][2]2.27}{[1][9][]9}}
\citation{conceptual_intro_hmc}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Markov Chain Monte Carlo}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:mcmc}{{3}{11}{Markov Chain Monte Carlo}{chapter.3}{}}
\newlabel{chap:mcmc@cref}{{[chapter][3][]3}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Expectation Values and the Typical Set}{11}{section.3.1}\protected@file@percent }
\newlabel{eq:expval}{{3.1}{11}{Expectation Values and the Typical Set}{equation.3.1.1}{}}
\newlabel{eq:expval@cref}{{[equation][1][3]3.1}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Typical Set}{11}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}The Target Density and Bayesian Applications}{12}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Markov Chains and Markov Transitions}{12}{section.3.2}\protected@file@percent }
\newlabel{eq:detailed_balance}{{3.3}{12}{Markov Chains and Markov Transitions}{equation.3.2.3}{}}
\newlabel{eq:detailed_balance@cref}{{[equation][3][3]3.3}{[1][12][]12}}
\newlabel{eq:mcmc_estimator}{{3.4}{12}{Markov Chains and Markov Transitions}{equation.3.2.4}{}}
\newlabel{eq:mcmc_estimator@cref}{{[equation][4][3]3.4}{[1][12][]12}}
\citation{geometric_ergodicity}
\citation{rhat}
\citation{convergence_diagnostics}
\citation{metropolis,metropolis_two}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Ideal Markov Chains}{13}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Pathologies}{13}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Geometric Ergodicity and Convergence Diagnostics}{13}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Metropolis-Hastings}{13}{section.3.3}\protected@file@percent }
\newlabel{eq:general_acceptance_prob}{{3.5}{14}{Metropolis-Hastings}{equation.3.3.5}{}}
\newlabel{eq:general_acceptance_prob@cref}{{[equation][5][3]3.5}{[1][14][]14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Metropolis-Hastings\relax }}{14}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:general_metropolis}{{3.1}{14}{Metropolis-Hastings\relax }{figure.caption.7}{}}
\newlabel{algo:general_metropolis@cref}{{[algorithm][1][3]3.1}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The Proposal Distribution}{14}{subsection.3.3.1}\protected@file@percent }
\newlabel{eq:symmetric_acceptance_prob}{{3.7}{14}{The Proposal Distribution}{equation.3.3.7}{}}
\newlabel{eq:symmetric_acceptance_prob@cref}{{[equation][7][3]3.7}{[1][14][]14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Gibbs Sampling}{15}{section.3.4}\protected@file@percent }
\newlabel{eq:gibbs_sampling}{{3.8}{15}{Gibbs Sampling}{equation.3.4.8}{}}
\newlabel{eq:gibbs_sampling@cref}{{[equation][8][3]3.8}{[1][15][]15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces Gibbs sampling\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{algo:gibbs}{{3.2}{15}{Gibbs sampling\relax }{figure.caption.8}{}}
\newlabel{algo:gibbs@cref}{{[algorithm][2][3]3.2}{[1][15][]15}}
\citation{classical_mechanics}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Hamiltonian Monte Carlo}{17}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hmc}{{4}{17}{Hamiltonian Monte Carlo}{chapter.4}{}}
\newlabel{chap:hmc@cref}{{[chapter][4][]4}{[1][17][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hamiltonian Dynamics}{17}{section.4.1}\protected@file@percent }
\newlabel{sec:hamiltonian_dynamics}{{4.1}{17}{Hamiltonian Dynamics}{section.4.1}{}}
\newlabel{sec:hamiltonian_dynamics@cref}{{[section][1][4]4.1}{[1][17][]17}}
\newlabel{eq:hamiltons_eqs}{{4.1}{17}{Hamiltonian Dynamics}{equation.4.1.1}{}}
\newlabel{eq:hamiltons_eqs@cref}{{[equation][1][4]4.1}{[1][17][]17}}
\newlabel{eq:hamiltonian}{{4.2}{17}{Hamiltonian Dynamics}{equation.4.1.2}{}}
\newlabel{eq:hamiltonian@cref}{{[equation][2][4]4.2}{[1][17][]17}}
\citation{leapfrog}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Leapfrog integration}{18}{subsection.4.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Leapfrog Integration\relax }}{18}{figure.caption.9}\protected@file@percent }
\newlabel{algo:leapfrog}{{4.1}{18}{Leapfrog Integration\relax }{figure.caption.9}{}}
\newlabel{algo:leapfrog@cref}{{[algorithm][1][4]4.1}{[1][18][]18}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Vectorized Leapfrog Integration\relax }}{19}{figure.caption.10}\protected@file@percent }
\newlabel{algo:vec_leapfrog}{{4.2}{19}{Vectorized Leapfrog Integration\relax }{figure.caption.10}{}}
\newlabel{algo:vec_leapfrog@cref}{{[algorithm][2][4]4.2}{[1][18][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating a Proposal State}{19}{section.4.2}\protected@file@percent }
\newlabel{eq:canonical_coordinate}{{4.5}{19}{Generating a Proposal State}{equation.4.2.5}{}}
\newlabel{eq:canonical_coordinate@cref}{{[equation][5][4]4.5}{[1][19][]19}}
\newlabel{eq:potential_energy}{{4.6}{19}{Generating a Proposal State}{equation.4.2.6}{}}
\newlabel{eq:potential_energy@cref}{{[equation][6][4]4.6}{[1][19][]19}}
\newlabel{eq:K_classical_physics}{{4.9}{19}{Generating a Proposal State}{equation.4.2.9}{}}
\newlabel{eq:K_classical_physics@cref}{{[equation][9][4]4.9}{[1][19][]19}}
\newlabel{eq:canonical_p}{{4.10}{19}{Generating a Proposal State}{equation.4.2.10}{}}
\newlabel{eq:canonical_p@cref}{{[equation][10][4]4.10}{[1][19][]19}}
\newlabel{eq:full_canonical}{{4.11}{20}{Generating a Proposal State}{equation.4.2.11}{}}
\newlabel{eq:full_canonical@cref}{{[equation][11][4]4.11}{[1][20][]20}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces Hamiltonian Monte Carlo\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{algo:hmc}{{4.3}{21}{Hamiltonian Monte Carlo\relax }{figure.caption.11}{}}
\newlabel{algo:hmc@cref}{{[algorithm][3][4]4.3}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Potential Energy Function in Bayesian Machine Learning Applications}{21}{section.4.3}\protected@file@percent }
\newlabel{eq:potential_energy_bayesian}{{4.15}{21}{The Potential Energy Function in Bayesian Machine Learning Applications}{equation.4.3.15}{}}
\newlabel{eq:potential_energy_bayesian@cref}{{[equation][15][4]4.15}{[1][21][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Limitations of Hamiltonian Monte Carlo}{21}{section.4.4}\protected@file@percent }
\citation{nuts}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}The No-U-Turn Sampler}{23}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:no_u_turn_sampler}{{5}{23}{The No-U-Turn Sampler}{chapter.5}{}}
\newlabel{chap:no_u_turn_sampler@cref}{{[chapter][5][]5}{[1][23][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Modifying Hamiltonian Monte Carlo}{23}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Generation of States and the Stopping Criterion}{24}{subsection.5.1.1}\protected@file@percent }
\newlabel{eq:stopping_criterion1}{{5.6}{24}{Generation of States and the Stopping Criterion}{equation.5.1.6}{}}
\newlabel{eq:stopping_criterion1@cref}{{[equation][6][5]5.6}{[1][24][]24}}
\newlabel{eq:stopping_criterion2}{{5.7}{25}{Generation of States and the Stopping Criterion}{equation.5.1.7}{}}
\newlabel{eq:stopping_criterion2@cref}{{[equation][7][5]5.7}{[1][25][]25}}
\newlabel{eq:stop}{{5.8}{25}{Generation of States and the Stopping Criterion}{equation.5.1.8}{}}
\newlabel{eq:stop@cref}{{[equation][8][5]5.8}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Selecting Candidate Points}{25}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}A Naive Implementation of the No-U-Turn Sampler}{25}{section.5.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces Helper function used in the Naive NUTS implementation\relax }}{26}{figure.caption.12}\protected@file@percent }
\newlabel{algo:build_tree}{{5.1}{26}{Helper function used in the Naive NUTS implementation\relax }{figure.caption.12}{}}
\newlabel{algo:build_tree@cref}{{[algorithm][1][5]5.1}{[1][26][]26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.2}{\ignorespaces The naive NUTS sampler\relax }}{27}{figure.caption.13}\protected@file@percent }
\newlabel{algo:nuts_naive}{{5.2}{27}{The naive NUTS sampler\relax }{figure.caption.13}{}}
\newlabel{algo:nuts_naive@cref}{{[algorithm][2][5]5.2}{[1][26][]27}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}An Efficient Implementation of the No-U-Turn Sampler}{27}{section.5.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.3}{\ignorespaces Helper function used in the efficient NUTS implementation\relax }}{28}{figure.caption.14}\protected@file@percent }
\newlabel{algo:build_tree_efficient}{{5.3}{28}{Helper function used in the efficient NUTS implementation\relax }{figure.caption.14}{}}
\newlabel{algo:build_tree_efficient@cref}{{[algorithm][3][5]5.3}{[1][28][]28}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.4}{\ignorespaces The efficient NUTS sampler\relax }}{29}{figure.caption.15}\protected@file@percent }
\newlabel{algo:efficient_nuts}{{5.4}{29}{The efficient NUTS sampler\relax }{figure.caption.15}{}}
\newlabel{algo:efficient_nuts@cref}{{[algorithm][4][5]5.4}{[1][28][]29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Dual-Averaging Step Size Adaptation}{29}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}NUTS with Dual-Averaging Step Size Adaptation}{29}{section.5.5}\protected@file@percent }
\citation{tf}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bayesian Neural Networks}{31}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bnn}{{6}{31}{Bayesian Neural Networks}{chapter.6}{}}
\newlabel{chap:bnn@cref}{{[chapter][6][]6}{[1][31][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Neural Networks}{31}{section.6.1}\protected@file@percent }
\newlabel{sec:neural_networks}{{6.1}{31}{Neural Networks}{section.6.1}{}}
\newlabel{sec:neural_networks@cref}{{[section][1][6]6.1}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Basic Mathematical Structure}{31}{subsection.6.1.1}\protected@file@percent }
\citation{backprop}
\citation{ml_for_physicists}
\newlabel{eq:nn_forward_pass}{{6.2}{32}{Basic Mathematical Structure}{equation.6.1.2}{}}
\newlabel{eq:nn_forward_pass@cref}{{[equation][2][6]6.2}{[1][31][]32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Backpropagation}{32}{subsection.6.1.2}\protected@file@percent }
\newlabel{eq:backprop1}{{6.3}{32}{Backpropagation}{equation.6.1.3}{}}
\newlabel{eq:backprop1@cref}{{[equation][3][6]6.3}{[1][32][]32}}
\newlabel{eq:backprop2}{{6.4}{32}{Backpropagation}{equation.6.1.4}{}}
\newlabel{eq:backprop2@cref}{{[equation][4][6]6.4}{[1][32][]32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.1}{\ignorespaces Backpropagation: Forward pass\relax }}{32}{figure.caption.16}\protected@file@percent }
\newlabel{algo:forward_pass}{{6.1}{32}{Backpropagation: Forward pass\relax }{figure.caption.16}{}}
\newlabel{algo:forward_pass@cref}{{[algorithm][1][6]6.1}{[1][32][]32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.2}{\ignorespaces Backpropagation: Backward pass\relax }}{33}{figure.caption.17}\protected@file@percent }
\newlabel{algo:backward_pass}{{6.2}{33}{Backpropagation: Backward pass\relax }{figure.caption.17}{}}
\newlabel{algo:backward_pass@cref}{{[algorithm][2][6]6.2}{[1][32][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Regularization in Neural Networks}{33}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Activation Functions}{33}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Sigmoid and Tanh}{33}{subsection.6.2.1}\protected@file@percent }
\newlabel{eq:sigmoid}{{6.9}{33}{Sigmoid and Tanh}{equation.6.2.9}{}}
\newlabel{eq:sigmoid@cref}{{[equation][9][6]6.9}{[1][33][]33}}
\citation{swish}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}ReLU}{34}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Swish}{34}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bayesian learning of Neural Networks using Monte Carlo Samplers}{34}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}What \textit  {is} Bayesian learning of Neural Networks?}{34}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}The Potential Energy Function of Neural Networks}{35}{subsection.6.3.2}\protected@file@percent }
\newlabel{eq:model_priors}{{6.13}{35}{The Potential Energy Function of Neural Networks}{equation.6.3.13}{}}
\newlabel{eq:model_priors@cref}{{[equation][13][6]6.13}{[1][35][]35}}
\newlabel{eq:special_potential_energy}{{6.15}{35}{The Potential Energy Function of Neural Networks}{equation.6.3.15}{}}
\newlabel{eq:special_potential_energy@cref}{{[equation][15][6]6.15}{[1][35][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Practical Training of Bayesian Neural Networks}{35}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Training Algorithm of Bayesian Neural Networks}{36}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Numerical Experiments}{37}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:numerical_experiments}{{7}{37}{Numerical Experiments}{chapter.7}{}}
\newlabel{chap:numerical_experiments@cref}{{[chapter][7][]7}{[1][37][]37}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}The Dataset}{37}{section.7.1}\protected@file@percent }
\newlabel{sec:dataset}{{7.1}{37}{The Dataset}{section.7.1}{}}
\newlabel{sec:dataset@cref}{{[section][1][7]7.1}{[1][37][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Data Generation}{37}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Data Scaling and Transformations}{37}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Performance Metrics}{37}{section.7.2}\protected@file@percent }
\newlabel{seq:perf_metrics}{{7.2}{37}{Performance Metrics}{section.7.2}{}}
\newlabel{seq:perf_metrics@cref}{{[section][2][7]7.2}{[1][37][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Relative Error}{37}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Standardized Residuals}{38}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{38}{section.7.3}\protected@file@percent }
\newlabel{sec:results}{{7.3}{38}{Results}{section.7.3}{}}
\newlabel{sec:results@cref}{{[section][3][7]7.3}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Benchmarks of Hyperparameters}{38}{subsection.7.3.1}\protected@file@percent }
\newlabel{subsec:benchmarks}{{7.3.1}{38}{Benchmarks of Hyperparameters}{subsection.7.3.1}{}}
\newlabel{subsec:benchmarks@cref}{{[subsection][1][7,3]7.3.1}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.1}Baseline Model}{38}{subsubsection.7.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.2}Pretraining}{38}{subsubsection.7.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.3}Burn-in length}{38}{subsubsection.7.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.4}Number of model parameters}{38}{subsubsection.7.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Neutralino-Neutralino Cross Sections}{38}{subsection.7.3.2}\protected@file@percent }
\newlabel{subsec:neuralino_experiments}{{7.3.2}{38}{Neutralino-Neutralino Cross Sections}{subsection.7.3.2}{}}
\newlabel{subsec:neuralino_experiments@cref}{{[subsection][2][7,3]7.3.2}{[1][38][]38}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces  The table shows the training configuration used to sample the models listed in table~\ref  {tab:deep_models}. \relax }}{38}{table.caption.18}\protected@file@percent }
\newlabel{tab:NN_mse_scores}{{7.1}{38}{The table shows the training configuration used to sample the models listed in table~\ref {tab:deep_models}. \relax }{table.caption.18}{}}
\newlabel{tab:NN_mse_scores@cref}{{[table][1][7]7.1}{[1][38][]38}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces  The table shows the models used in this section. For each model, 1000 sampled networks are used to generate each result shown in this section. The architecture describe number of nodes per layer. For each hidden layer, the same activation function is used. The final layer uses an identity function. \relax }}{38}{table.caption.19}\protected@file@percent }
\newlabel{tab:deep_models}{{7.2}{38}{The table shows the models used in this section. For each model, 1000 sampled networks are used to generate each result shown in this section. The architecture describe number of nodes per layer. For each hidden layer, the same activation function is used. The final layer uses an identity function. \relax }{table.caption.19}{}}
\newlabel{tab:deep_models@cref}{{[table][2][7]7.2}{[1][38][]38}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The figure shows histograms of the standardized residuals computed for different BNNs. The normal distribution is drawn as dotted line. \relax }}{39}{figure.caption.20}\protected@file@percent }
\newlabel{fig:standardized_residual}{{7.1}{39}{The figure shows histograms of the standardized residuals computed for different BNNs. The normal distribution is drawn as dotted line. \relax }{figure.caption.20}{}}
\newlabel{fig:standardized_residual@cref}{{[figure][1][7]7.1}{[1][38][]39}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion}{41}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{8}{43}{Conclusion}{chapter*.21}{}}
\newlabel{chap:conclusion@cref}{{[chapter][8][]8}{[1][43][]43}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{44}{chapter*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendices}{45}{section*.22}\protected@file@percent }
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{Appendix A}{47}{appendix*.23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Appendix 1 title }{47}{section.Alph0.1}\protected@file@percent }
\newlabel{sec:appendix_1_label}{{A.1}{47}{Appendix 1 title}{section.Alph0.1}{}}
\newlabel{sec:appendix_1_label@cref}{{[subappendix][1][2147483647,0]A.1}{[1][47][]47}}
\bibcite{ADAM}{{1}{}{{}}{{}}}
\bibcite{bayes_theorem}{{2}{}{{}}{{}}}
\bibcite{ml_for_physicists}{{3}{}{{}}{{}}}
\bibcite{conceptual_intro_hmc}{{4}{}{{}}{{}}}
\bibcite{geometric_ergodicity}{{5}{}{{}}{{}}}
\bibcite{rhat}{{6}{}{{}}{{}}}
\bibcite{convergence_diagnostics}{{7}{}{{}}{{}}}
\bibcite{metropolis}{{8}{}{{}}{{}}}
\bibcite{metropolis_two}{{9}{}{{}}{{}}}
\bibcite{classical_mechanics}{{10}{}{{}}{{}}}
\bibcite{leapfrog}{{11}{}{{}}{{}}}
\bibcite{nuts}{{12}{}{{}}{{}}}
\bibcite{tf}{{13}{}{{}}{{}}}
\bibcite{backprop}{{14}{}{{}}{{}}}
\bibcite{swish}{{15}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{63}
