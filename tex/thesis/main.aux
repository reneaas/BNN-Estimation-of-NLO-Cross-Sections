\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\babel@aux{UKenglish}{}
\citation{nuts}
\citation{xsec}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Physics Problem}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:physics_problem}{{1}{3}{The Physics Problem}{chapter.1}{}}
\newlabel{chap:physics_problem@cref}{{[chapter][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Computation of Beyond the Standard Model Cross Sections}{3}{section.1.1}\protected@file@percent }
\newlabel{eq:event_equation}{{1.1}{3}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.1}{}}
\newlabel{eq:event_equation@cref}{{[equation][1][1]1.1}{[1][3][]3}}
\newlabel{eq:total_events_decomp}{{1.2}{3}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.2}{}}
\newlabel{eq:total_events_decomp@cref}{{[equation][2][1]1.2}{[1][3][]3}}
\citation{colliderbit}
\citation{prospino}
\citation{xsec}
\newlabel{eq:general_event_eq}{{1.3}{4}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.3}{}}
\newlabel{eq:general_event_eq@cref}{{[equation][3][1]1.3}{[1][3][]4}}
\newlabel{eq:poisson_likelihood}{{1.4}{4}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.4}{}}
\newlabel{eq:poisson_likelihood@cref}{{[equation][4][1]1.4}{[1][4][]4}}
\newlabel{eq:xi_width}{{1.5}{4}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.5}{}}
\newlabel{eq:xi_width@cref}{{[equation][5][1]1.5}{[1][4][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Bayesian Regression as a Substitute}{4}{section.1.2}\protected@file@percent }
\citation{universal_function_approximator}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Formulation of Machine Learning}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bayesian_ml}{{2}{7}{Bayesian Formulation of Machine Learning}{chapter.2}{}}
\newlabel{chap:bayesian_ml@cref}{{[chapter][2][]2}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Core of Machine Learning}{7}{section.2.1}\protected@file@percent }
\newlabel{eq:model_assumption}{{2.1}{7}{The Core of Machine Learning}{equation.2.1.1}{}}
\newlabel{eq:model_assumption@cref}{{[equation][1][2]2.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Loss Functions}{7}{subsection.2.1.1}\protected@file@percent }
\newlabel{eq:rss}{{2.2}{7}{Loss Functions}{equation.2.1.2}{}}
\newlabel{eq:rss@cref}{{[equation][2][2]2.2}{[1][7][]7}}
\newlabel{eq:mse}{{2.3}{7}{Loss Functions}{equation.2.1.3}{}}
\newlabel{eq:mse@cref}{{[equation][3][2]2.3}{[1][7][]7}}
\citation{ADAM}
\citation{bayes_theorem}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Regularization}{8}{subsection.2.1.2}\protected@file@percent }
\newlabel{eq:loss_l2_reg}{{2.4}{8}{Regularization}{equation.2.1.4}{}}
\newlabel{eq:loss_l2_reg@cref}{{[equation][4][2]2.4}{[1][8][]8}}
\newlabel{eq:loss_l1_reg}{{2.5}{8}{Regularization}{equation.2.1.5}{}}
\newlabel{eq:loss_l1_reg@cref}{{[equation][5][2]2.5}{[1][8][]8}}
\newlabel{eq:loss_fn}{{2.6}{8}{Regularization}{equation.2.1.6}{}}
\newlabel{eq:loss_fn@cref}{{[equation][6][2]2.6}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Optimization}{8}{subsection.2.1.3}\protected@file@percent }
\newlabel{eq:optimal_param}{{2.8}{8}{Optimization}{equation.2.1.8}{}}
\newlabel{eq:optimal_param@cref}{{[equation][8][2]2.8}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayes' theorem}{8}{section.2.2}\protected@file@percent }
\citation{ml_for_physicists}
\newlabel{eq:bayes_theorem}{{2.10}{9}{Bayes' theorem}{equation.2.2.10}{}}
\newlabel{eq:bayes_theorem@cref}{{[equation][10][2]2.10}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Bayesian Framework for Machine Learning}{9}{section.2.3}\protected@file@percent }
\newlabel{eq:mle}{{2.11}{9}{Bayesian Framework for Machine Learning}{equation.2.3.11}{}}
\newlabel{eq:mle@cref}{{[equation][11][2]2.11}{[1][9][]9}}
\newlabel{eq:map}{{2.12}{9}{Bayesian Framework for Machine Learning}{equation.2.3.12}{}}
\newlabel{eq:map@cref}{{[equation][12][2]2.12}{[1][9][]9}}
\newlabel{eq:likelihood_fn}{{2.17}{10}{Bayesian Framework for Machine Learning}{equation.2.3.17}{}}
\newlabel{eq:likelihood_fn@cref}{{[equation][17][2]2.17}{[1][9][]10}}
\newlabel{eq:gaussian_prior}{{2.19}{10}{Bayesian Framework for Machine Learning}{equation.2.3.19}{}}
\newlabel{eq:gaussian_prior@cref}{{[equation][19][2]2.19}{[1][10][]10}}
\newlabel{eq:posterior_function_of_loss}{{2.22}{10}{Bayesian Framework for Machine Learning}{equation.2.3.22}{}}
\newlabel{eq:posterior_function_of_loss@cref}{{[equation][22][2]2.22}{[1][10][]10}}
\newlabel{eq:loss_function_of_posterior}{{2.24}{10}{Bayesian Framework for Machine Learning}{equation.2.3.24}{}}
\newlabel{eq:loss_function_of_posterior@cref}{{[equation][24][2]2.24}{[1][10][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Bayesian Inference}{10}{section.2.4}\protected@file@percent }
\newlabel{eq:predictive_distribution}{{2.25}{11}{Bayesian Inference}{equation.2.4.25}{}}
\newlabel{eq:predictive_distribution@cref}{{[equation][25][2]2.25}{[1][11][]11}}
\newlabel{eq:predictive_dist_approx}{{2.26}{11}{Bayesian Inference}{equation.2.4.26}{}}
\newlabel{eq:predictive_dist_approx@cref}{{[equation][26][2]2.26}{[1][11][]11}}
\newlabel{eq:bayesian_expval}{{2.27}{11}{Bayesian Inference}{equation.2.4.27}{}}
\newlabel{eq:bayesian_expval@cref}{{[equation][27][2]2.27}{[1][11][]11}}
\citation{conceptual_intro_hmc}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Markov Chain Monte Carlo}{13}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:mcmc}{{3}{13}{Markov Chain Monte Carlo}{chapter.3}{}}
\newlabel{chap:mcmc@cref}{{[chapter][3][]3}{[1][13][]13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Expectation Values and the Typical Set}{13}{section.3.1}\protected@file@percent }
\newlabel{eq:expval}{{3.1}{13}{Expectation Values and the Typical Set}{equation.3.1.1}{}}
\newlabel{eq:expval@cref}{{[equation][1][3]3.1}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Typical Set}{13}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}The Target Density and Bayesian Applications}{14}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Markov Chains and Markov Transitions}{14}{section.3.2}\protected@file@percent }
\newlabel{eq:detailed_balance}{{3.3}{14}{Markov Chains and Markov Transitions}{equation.3.2.3}{}}
\newlabel{eq:detailed_balance@cref}{{[equation][3][3]3.3}{[1][14][]14}}
\newlabel{eq:mcmc_estimator}{{3.4}{14}{Markov Chains and Markov Transitions}{equation.3.2.4}{}}
\newlabel{eq:mcmc_estimator@cref}{{[equation][4][3]3.4}{[1][14][]14}}
\citation{geometric_ergodicity}
\citation{rhat}
\citation{convergence_diagnostics}
\citation{metropolis,metropolis_two}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Ideal Markov Chains}{15}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Pathologies}{15}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Geometric Ergodicity and Convergence Diagnostics}{15}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Metropolis-Hastings}{15}{section.3.3}\protected@file@percent }
\newlabel{eq:general_acceptance_prob}{{3.5}{16}{Metropolis-Hastings}{equation.3.3.5}{}}
\newlabel{eq:general_acceptance_prob@cref}{{[equation][5][3]3.5}{[1][16][]16}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Metropolis-Hastings\relax }}{16}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:general_metropolis}{{3.1}{16}{Metropolis-Hastings\relax }{figure.caption.8}{}}
\newlabel{algo:general_metropolis@cref}{{[algorithm][1][3]3.1}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The Proposal Distribution}{16}{subsection.3.3.1}\protected@file@percent }
\newlabel{eq:symmetric_acceptance_prob}{{3.7}{16}{The Proposal Distribution}{equation.3.3.7}{}}
\newlabel{eq:symmetric_acceptance_prob@cref}{{[equation][7][3]3.7}{[1][16][]16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Gibbs Sampling}{17}{section.3.4}\protected@file@percent }
\newlabel{eq:gibbs_sampling}{{3.8}{17}{Gibbs Sampling}{equation.3.4.8}{}}
\newlabel{eq:gibbs_sampling@cref}{{[equation][8][3]3.8}{[1][17][]17}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces Gibbs sampling\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{algo:gibbs}{{3.2}{17}{Gibbs sampling\relax }{figure.caption.9}{}}
\newlabel{algo:gibbs@cref}{{[algorithm][2][3]3.2}{[1][17][]17}}
\citation{classical_mechanics}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Hamiltonian Monte Carlo}{19}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hmc}{{4}{19}{Hamiltonian Monte Carlo}{chapter.4}{}}
\newlabel{chap:hmc@cref}{{[chapter][4][]4}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hamiltonian Dynamics}{19}{section.4.1}\protected@file@percent }
\newlabel{sec:hamiltonian_dynamics}{{4.1}{19}{Hamiltonian Dynamics}{section.4.1}{}}
\newlabel{sec:hamiltonian_dynamics@cref}{{[section][1][4]4.1}{[1][19][]19}}
\newlabel{eq:hamiltons_eqs}{{4.1}{19}{Hamiltonian Dynamics}{equation.4.1.1}{}}
\newlabel{eq:hamiltons_eqs@cref}{{[equation][1][4]4.1}{[1][19][]19}}
\newlabel{eq:hamiltonian}{{4.2}{19}{Hamiltonian Dynamics}{equation.4.1.2}{}}
\newlabel{eq:hamiltonian@cref}{{[equation][2][4]4.2}{[1][19][]19}}
\citation{leapfrog}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Leapfrog integration}{20}{subsection.4.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Leapfrog Integration\relax }}{20}{figure.caption.10}\protected@file@percent }
\newlabel{algo:leapfrog}{{4.1}{20}{Leapfrog Integration\relax }{figure.caption.10}{}}
\newlabel{algo:leapfrog@cref}{{[algorithm][1][4]4.1}{[1][20][]20}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Vectorized Leapfrog Integration\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{algo:vec_leapfrog}{{4.2}{21}{Vectorized Leapfrog Integration\relax }{figure.caption.11}{}}
\newlabel{algo:vec_leapfrog@cref}{{[algorithm][2][4]4.2}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating a Proposal State}{21}{section.4.2}\protected@file@percent }
\newlabel{eq:canonical_coordinate}{{4.5}{21}{Generating a Proposal State}{equation.4.2.5}{}}
\newlabel{eq:canonical_coordinate@cref}{{[equation][5][4]4.5}{[1][21][]21}}
\newlabel{eq:potential_energy}{{4.6}{21}{Generating a Proposal State}{equation.4.2.6}{}}
\newlabel{eq:potential_energy@cref}{{[equation][6][4]4.6}{[1][21][]21}}
\newlabel{eq:K_classical_physics}{{4.9}{21}{Generating a Proposal State}{equation.4.2.9}{}}
\newlabel{eq:K_classical_physics@cref}{{[equation][9][4]4.9}{[1][21][]21}}
\newlabel{eq:canonical_p}{{4.10}{21}{Generating a Proposal State}{equation.4.2.10}{}}
\newlabel{eq:canonical_p@cref}{{[equation][10][4]4.10}{[1][21][]21}}
\newlabel{eq:full_canonical}{{4.11}{22}{Generating a Proposal State}{equation.4.2.11}{}}
\newlabel{eq:full_canonical@cref}{{[equation][11][4]4.11}{[1][22][]22}}
\newlabel{eq:hmc_acceptance}{{4.12}{22}{Generating a Proposal State}{equation.4.2.12}{}}
\newlabel{eq:hmc_acceptance@cref}{{[equation][12][4]4.12}{[1][22][]22}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces Hamiltonian Monte Carlo\relax }}{23}{figure.caption.12}\protected@file@percent }
\newlabel{algo:hmc}{{4.3}{23}{Hamiltonian Monte Carlo\relax }{figure.caption.12}{}}
\newlabel{algo:hmc@cref}{{[algorithm][3][4]4.3}{[1][22][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Potential Energy Function in Bayesian Machine Learning Applications}{23}{section.4.3}\protected@file@percent }
\newlabel{eq:potential_energy_bayesian}{{4.15}{23}{The Potential Energy Function in Bayesian Machine Learning Applications}{equation.4.3.15}{}}
\newlabel{eq:potential_energy_bayesian@cref}{{[equation][15][4]4.15}{[1][23][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Limitations of Hamiltonian Monte Carlo}{23}{section.4.4}\protected@file@percent }
\citation{nuts}
\citation{Nesterov2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adaptive Hamiltonian Monte Carlo}{25}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:no_u_turn_sampler}{{5}{25}{Adaptive Hamiltonian Monte Carlo}{chapter.5}{}}
\newlabel{chap:no_u_turn_sampler@cref}{{[chapter][5][]5}{[1][25][]25}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The No-U-Turn Sampler}{25}{section.5.1}\protected@file@percent }
\citation{nuts}
\citation{nuts}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The figure shows an example of a trajectory generated by the NUTS sampler. The top diagram displays the projection onto position space with the momenta drawn in as arrows. The bottom diagram shows the resulting balanced binary tree. The tree structure is drawn onto the trajectory as well. The numbering displays the order in which the states are generated by Leapfrog integration. The black node is the initial node. The first doubling is forwards in time and yields the rightmost node of the first binary tree. The second doubling is backwards in time and is initiated from the black node, yielding a new tree of height 2 where the left subtree is the new states (the yellow nodes). The next doubling is also backwards in time, and the Leapfrog integrator is initiated from the tail (the leftmost yellow node) performing four Leapfrog steps generating a subtree which becomes the left half of the next tree (blue nodes). The final doubling in the figure is forwards in time with $L = 8$ Leapfrog steps taken from the orange node (which was the rightmost leaf of the tree before the final doubling) which yields the green nodes. The figure is a modified version of a diagram in \cite  {nuts}.\relax }}{26}{figure.caption.13}\protected@file@percent }
\newlabel{fig:nuts_trajectory}{{5.1}{26}{The figure shows an example of a trajectory generated by the NUTS sampler. The top diagram displays the projection onto position space with the momenta drawn in as arrows. The bottom diagram shows the resulting balanced binary tree. The tree structure is drawn onto the trajectory as well. The numbering displays the order in which the states are generated by Leapfrog integration. The black node is the initial node. The first doubling is forwards in time and yields the rightmost node of the first binary tree. The second doubling is backwards in time and is initiated from the black node, yielding a new tree of height 2 where the left subtree is the new states (the yellow nodes). The next doubling is also backwards in time, and the Leapfrog integrator is initiated from the tail (the leftmost yellow node) performing four Leapfrog steps generating a subtree which becomes the left half of the next tree (blue nodes). The final doubling in the figure is forwards in time with $L = 8$ Leapfrog steps taken from the orange node (which was the rightmost leaf of the tree before the final doubling) which yields the green nodes. The figure is a modified version of a diagram in \cite {nuts}.\relax }{figure.caption.13}{}}
\newlabel{fig:nuts_trajectory@cref}{{[figure][1][5]5.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Stopping Conditions and Selection of Candidate States}{26}{subsection.5.1.1}\protected@file@percent }
\newlabel{eq:change_in_distance}{{5.1}{27}{Stopping Conditions and Selection of Candidate States}{equation.5.1.1}{}}
\newlabel{eq:change_in_distance@cref}{{[equation][1][5]5.1}{[1][27][]27}}
\newlabel{eq:no_u_turn_condition}{{5.2}{27}{Stopping Conditions and Selection of Candidate States}{equation.5.1.2}{}}
\newlabel{eq:no_u_turn_condition@cref}{{[equation][2][5]5.2}{[1][27][]27}}
\newlabel{eq:nuts_error_condition}{{5.3}{27}{Stopping Conditions and Selection of Candidate States}{equation.5.1.3}{}}
\newlabel{eq:nuts_error_condition@cref}{{[equation][3][5]5.3}{[1][27][]27}}
\citation{nuts_joonha_park}
\newlabel{eq:nuts_acceptance}{{5.4}{28}{Stopping Conditions and Selection of Candidate States}{equation.5.1.4}{}}
\newlabel{eq:nuts_acceptance@cref}{{[equation][4][5]5.4}{[1][28][]28}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces The NUTS Sampler\relax }}{28}{figure.caption.14}\protected@file@percent }
\newlabel{algo:nuts}{{5.1}{28}{The NUTS Sampler\relax }{figure.caption.14}{}}
\newlabel{algo:nuts@cref}{{[algorithm][1][5]5.1}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Computational Cost}{28}{subsection.5.1.2}\protected@file@percent }
\newlabel{eq:uniform_dist_over_C}{{5.5}{29}{Computational Cost}{equation.5.1.5}{}}
\newlabel{eq:uniform_dist_over_C@cref}{{[equation][5][5]5.5}{[1][28][]29}}
\citation{tf}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bayesian Neural Networks}{31}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bnn}{{6}{31}{Bayesian Neural Networks}{chapter.6}{}}
\newlabel{chap:bnn@cref}{{[chapter][6][]6}{[1][31][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Neural Networks}{31}{section.6.1}\protected@file@percent }
\newlabel{sec:neural_networks}{{6.1}{31}{Neural Networks}{section.6.1}{}}
\newlabel{sec:neural_networks@cref}{{[section][1][6]6.1}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Basic Mathematical Structure}{31}{subsection.6.1.1}\protected@file@percent }
\citation{backprop}
\citation{ml_for_physicists}
\newlabel{eq:nn_forward_pass}{{6.2}{32}{Basic Mathematical Structure}{equation.6.1.2}{}}
\newlabel{eq:nn_forward_pass@cref}{{[equation][2][6]6.2}{[1][31][]32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Backpropagation}{32}{subsection.6.1.2}\protected@file@percent }
\newlabel{eq:backprop1}{{6.3}{32}{Backpropagation}{equation.6.1.3}{}}
\newlabel{eq:backprop1@cref}{{[equation][3][6]6.3}{[1][32][]32}}
\newlabel{eq:backprop2}{{6.5}{32}{Backpropagation}{equation.6.1.5}{}}
\newlabel{eq:backprop2@cref}{{[equation][5][6]6.5}{[1][32][]32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.1}{\ignorespaces Backpropagation: Forward pass\relax }}{33}{figure.caption.15}\protected@file@percent }
\newlabel{algo:forward_pass}{{6.1}{33}{Backpropagation: Forward pass\relax }{figure.caption.15}{}}
\newlabel{algo:forward_pass@cref}{{[algorithm][1][6]6.1}{[1][32][]33}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.2}{\ignorespaces Backpropagation: Backward pass\relax }}{33}{figure.caption.16}\protected@file@percent }
\newlabel{algo:backward_pass}{{6.2}{33}{Backpropagation: Backward pass\relax }{figure.caption.16}{}}
\newlabel{algo:backward_pass@cref}{{[algorithm][2][6]6.2}{[1][33][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Regularization in Neural Networks}{33}{subsection.6.1.3}\protected@file@percent }
\citation{swish}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Activation Functions}{34}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Sigmoid and Tanh}{34}{subsection.6.2.1}\protected@file@percent }
\newlabel{eq:sigmoid}{{6.10}{34}{Sigmoid and Tanh}{equation.6.2.10}{}}
\newlabel{eq:sigmoid@cref}{{[equation][10][6]6.10}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}ReLU}{34}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Swish}{34}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bayesian learning of Neural Networks using Monte Carlo Samplers}{34}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}What \textit  {is} Bayesian learning of Neural Networks?}{35}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}The Potential Energy Function of Neural Networks}{35}{subsection.6.3.2}\protected@file@percent }
\newlabel{eq:model_priors}{{6.14}{35}{The Potential Energy Function of Neural Networks}{equation.6.3.14}{}}
\newlabel{eq:model_priors@cref}{{[equation][14][6]6.14}{[1][35][]35}}
\newlabel{eq:special_potential_energy}{{6.16}{35}{The Potential Energy Function of Neural Networks}{equation.6.3.16}{}}
\newlabel{eq:special_potential_energy@cref}{{[equation][16][6]6.16}{[1][35][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Practical Training of Bayesian Neural Networks}{36}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:practical_bnn}{{6.3.3}{36}{Practical Training of Bayesian Neural Networks}{subsection.6.3.3}{}}
\newlabel{sec:practical_bnn@cref}{{[subsection][3][6,3]6.3.3}{[1][35][]36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Training Algorithm of Bayesian Neural Networks}{36}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}The Dataset and Methodology}{39}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{7}{39}{The Dataset and Methodology}{chapter.7}{}}
\newlabel{chap:methodology@cref}{{[chapter][7][]7}{[1][39][]39}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}The Dataset}{39}{section.7.1}\protected@file@percent }
\newlabel{sec:dataset}{{7.1}{39}{The Dataset}{section.7.1}{}}
\newlabel{sec:dataset@cref}{{[section][1][7]7.1}{[1][39][]39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}The Features and Targets}{39}{subsection.7.1.1}\protected@file@percent }
\newlabel{eq:neutralino_feat}{{7.1}{39}{The Features and Targets}{equation.7.1.1}{}}
\newlabel{eq:neutralino_feat@cref}{{[equation][1][7]7.1}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The values of the cross sections $\sigma _{\tilde  {\chi }_1^0 \tilde  {\chi }_1^0}$ are shown projected onto the axis of masses $m_{\tilde  {\chi }_1^0}$. The data is taken from the training data. \relax }}{40}{figure.caption.17}\protected@file@percent }
\newlabel{fig:dataset_masses}{{7.1}{40}{The values of the cross sections $\sigma _{\tilde {\chi }_1^0 \tilde {\chi }_1^0}$ are shown projected onto the axis of masses $m_{\tilde {\chi }_1^0}$. The data is taken from the training data. \relax }{figure.caption.17}{}}
\newlabel{fig:dataset_masses@cref}{{[figure][1][7]7.1}{[1][40][]40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Data Transformations}{40}{subsection.7.1.2}\protected@file@percent }
\newlabel{sec:data_transform}{{7.1.2}{40}{Data Transformations}{subsection.7.1.2}{}}
\newlabel{sec:data_transform@cref}{{[subsection][2][7,1]7.1.2}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The values of the cross sections $\sigma _{\tilde  {\chi }_1^0 \tilde  {\chi }_1^0}$ are shown projected onto the axes of mixing angles $N_{1j}$ for $j = 1, 2, 3, 4$. The data is taken from the training data. \relax }}{41}{figure.caption.18}\protected@file@percent }
\newlabel{fig:dataset_mixing_angles}{{7.2}{41}{The values of the cross sections $\sigma _{\tilde {\chi }_1^0 \tilde {\chi }_1^0}$ are shown projected onto the axes of mixing angles $N_{1j}$ for $j = 1, 2, 3, 4$. The data is taken from the training data. \relax }{figure.caption.18}{}}
\newlabel{fig:dataset_mixing_angles@cref}{{[figure][2][7]7.2}{[1][40][]41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Data Splitting}{41}{subsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Training Methodology}{42}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Implementation}{42}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Performance Metrics}{42}{subsection.7.2.2}\protected@file@percent }
\newlabel{sec:perf_metrics}{{7.2.2}{42}{Performance Metrics}{subsection.7.2.2}{}}
\newlabel{sec:perf_metrics@cref}{{[subsection][2][7,2]7.2.2}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2.1}Coefficent of Determination}{42}{subsubsection.7.2.2.1}\protected@file@percent }
\newlabel{eq:r2_score}{{7.4}{42}{Coefficent of Determination}{equation.7.2.4}{}}
\newlabel{eq:r2_score@cref}{{[equation][4][7]7.4}{[1][42][]42}}
\citation{r2_score}
\citation{rhat}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2.2}Standardized Residuals}{43}{subsubsection.7.2.2.2}\protected@file@percent }
\newlabel{eq:standardized_residuals}{{7.6}{43}{Standardized Residuals}{equation.7.2.6}{}}
\newlabel{eq:standardized_residuals@cref}{{[equation][6][7]7.6}{[1][43][]43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Potential Scale Reduction}{43}{subsection.7.2.3}\protected@file@percent }
\citation{convergence_diagnostics}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Numerical Experiments}{45}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:numerical_experiments}{{8}{45}{Numerical Experiments}{chapter.8}{}}
\newlabel{chap:numerical_experiments@cref}{{[chapter][8][]8}{[1][45][]45}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Training Procedure and Selection of Models and Hyperparameters}{45}{section.8.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces  The table shows a selection of models that is used for benchmarking purposes in this chapter. For each model, 1000 sampled networks were sampled to collectively represent each BNN model. We performed 1000 pretraining epochs with a batch size of 32 using the ADAM optimizer. We used 2500 warm-up steps (80\% adaptation steps first, followed by 20\% burn-in steps). For every sampled network, we skipped 10 samples. The kernel used for each model was the NUTS kernel with a maximum of $L = 4096$ Leapfrog steps. The number of nodes per layer is shown in the ``Layers'' column. \relax }}{46}{table.caption.19}\protected@file@percent }
\newlabel{tab:deep_models}{{8.1}{46}{The table shows a selection of models that is used for benchmarking purposes in this chapter. For each model, 1000 sampled networks were sampled to collectively represent each BNN model. We performed 1000 pretraining epochs with a batch size of 32 using the ADAM optimizer. We used 2500 warm-up steps (80\% adaptation steps first, followed by 20\% burn-in steps). For every sampled network, we skipped 10 samples. The kernel used for each model was the NUTS kernel with a maximum of $L = 4096$ Leapfrog steps. The number of nodes per layer is shown in the ``Layers'' column. \relax }{table.caption.19}{}}
\newlabel{tab:deep_models@cref}{{[table][1][8]8.1}{[1][45][]46}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Results and Discussion}{46}{section.8.2}\protected@file@percent }
\newlabel{sec:results}{{8.2}{46}{Results and Discussion}{section.8.2}{}}
\newlabel{sec:results@cref}{{[section][2][8]8.2}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Computational Performance}{46}{subsection.8.2.1}\protected@file@percent }
\citation{xla}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1.1}CPU v. GPU Training Performance}{47}{subsubsection.8.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The figure on top shows the relative wall clock time used per generated sample using $L = 512$ Leapfrog steps with the HMC sampler, as a function of number of hidden nodes in the hidden layer with an architechture 5-$n$-1, where $n$ represents the number of nodes. The relative wall clock time is computed as the wall clock time used by the CPU divided by the wall clock time used by the GPU. The figure on the bottom shows the absolute wall clock time per generated sample measured on the GPU for the same case. The red dots indicate the actual measured points. The CPU measurements are done using an 8-core M1 CPU (Apple Silicon). The GPU measurements are made on an NVIDIA Tesla P100 GPU. \relax }}{48}{figure.caption.20}\protected@file@percent }
\newlabel{fig:relative_performance}{{8.1}{48}{The figure on top shows the relative wall clock time used per generated sample using $L = 512$ Leapfrog steps with the HMC sampler, as a function of number of hidden nodes in the hidden layer with an architechture 5-$n$-1, where $n$ represents the number of nodes. The relative wall clock time is computed as the wall clock time used by the CPU divided by the wall clock time used by the GPU. The figure on the bottom shows the absolute wall clock time per generated sample measured on the GPU for the same case. The red dots indicate the actual measured points. The CPU measurements are done using an 8-core M1 CPU (Apple Silicon). The GPU measurements are made on an NVIDIA Tesla P100 GPU. \relax }{figure.caption.20}{}}
\newlabel{fig:relative_performance@cref}{{[figure][1][8]8.1}{[1][47][]48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1.2}Prediction Time}{49}{subsubsection.8.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces The figure shows the average prediction time given up to several simultaneous inputs $x$ using the models in table \ref  {tab:deep_models}. The wall clock time of the executions shown are measured in milliseconds and are averaged over 1000 trials per case. The measured wall clock time includes computation of the sample mean and sample error of the predictive distribution produced by the BNN models. The dots indicate the actual measured values. The colored graphs indicate how many simultaneous input points that were used. The measurements were done using an 8-core M1 CPU (Apple Silicon). \relax }}{49}{figure.caption.21}\protected@file@percent }
\newlabel{fig:prediction_time}{{8.2}{49}{The figure shows the average prediction time given up to several simultaneous inputs $x$ using the models in table \ref {tab:deep_models}. The wall clock time of the executions shown are measured in milliseconds and are averaged over 1000 trials per case. The measured wall clock time includes computation of the sample mean and sample error of the predictive distribution produced by the BNN models. The dots indicate the actual measured values. The colored graphs indicate how many simultaneous input points that were used. The measurements were done using an 8-core M1 CPU (Apple Silicon). \relax }{figure.caption.21}{}}
\newlabel{fig:prediction_time@cref}{{[figure][2][8]8.2}{[1][49][]49}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The figure shows the average prediction time using the built-in GPU on an M1 Apple Silicon system-on-chip to compute a prediction given a up to several simultaneous inputs $x$ using the models in table \ref  {tab:deep_models}. The measured wall clock time is given in milliseconds and is averaged over 1000 trials. The measured time includes computation of the sample mean and sample error of the predictive distribution produced by the BNN models. The dots indicate the actual measured values. The colored graphs indicate the number of simultaneous input points used in each case. \relax }}{50}{figure.caption.22}\protected@file@percent }
\newlabel{fig:prediction_time_gpu}{{8.3}{50}{The figure shows the average prediction time using the built-in GPU on an M1 Apple Silicon system-on-chip to compute a prediction given a up to several simultaneous inputs $x$ using the models in table \ref {tab:deep_models}. The measured wall clock time is given in milliseconds and is averaged over 1000 trials. The measured time includes computation of the sample mean and sample error of the predictive distribution produced by the BNN models. The dots indicate the actual measured values. The colored graphs indicate the number of simultaneous input points used in each case. \relax }{figure.caption.22}{}}
\newlabel{fig:prediction_time_gpu@cref}{{[figure][3][8]8.3}{[1][50][]50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1.3}Loading Times}{50}{subsubsection.8.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces  The figure shows the histograms of measured loading times (wall clock) in seconds using the models in table \ref  {tab:deep_models}. The measurements were made on an M1 Apple Silicon system-on-chip. The time measurements consist of 1000 measurements for each model. \relax }}{51}{figure.caption.23}\protected@file@percent }
\newlabel{fig:loading_times}{{8.4}{51}{The figure shows the histograms of measured loading times (wall clock) in seconds using the models in table \ref {tab:deep_models}. The measurements were made on an M1 Apple Silicon system-on-chip. The time measurements consist of 1000 measurements for each model. \relax }{figure.caption.23}{}}
\newlabel{fig:loading_times@cref}{{[figure][4][8]8.4}{[1][51][]51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Posterior Distribution of Weights}{51}{subsection.8.2.2}\protected@file@percent }
\newlabel{eq:surrogate_dist}{{8.2}{51}{Posterior Distribution of Weights}{equation.8.2.2}{}}
\newlabel{eq:surrogate_dist@cref}{{[equation][2][8]8.2}{[1][51][]51}}
\citation{google_bnn_posteriors}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces The figure shows the projection of the kernel density estimation of the empirical distribution onto two-dimensional subplanes of the posterior distribution. The figure on the top left shows the plane spanned by $(W_{2,5}^1, W_{2,6}^1)$. The figure on the top right shows the distribution in the plane spanned by $(W_{11,4}^3, W_{8,1}^1)$. The figure on the bottom left shows the distribution in the plane spanned by $(b_6^2, b_1^4)$. The figure on the bottom right shows the distribution spanned by the plane $(b_{11}^3, b_1^6)$. The weights used are the ones pertaining to ``model 4'' in table \ref  {tab:deep_models}. \relax }}{52}{figure.caption.24}\protected@file@percent }
\newlabel{fig:posterior_weights}{{8.5}{52}{The figure shows the projection of the kernel density estimation of the empirical distribution onto two-dimensional subplanes of the posterior distribution. The figure on the top left shows the plane spanned by $(W_{2,5}^1, W_{2,6}^1)$. The figure on the top right shows the distribution in the plane spanned by $(W_{11,4}^3, W_{8,1}^1)$. The figure on the bottom left shows the distribution in the plane spanned by $(b_6^2, b_1^4)$. The figure on the bottom right shows the distribution spanned by the plane $(b_{11}^3, b_1^6)$. The weights used are the ones pertaining to ``model 4'' in table \ref {tab:deep_models}. \relax }{figure.caption.24}{}}
\newlabel{fig:posterior_weights@cref}{{[figure][5][8]8.5}{[1][52][]52}}
\citation{nuts}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Benchmarks of Hyperparameters}{53}{subsection.8.2.3}\protected@file@percent }
\newlabel{subsec:benchmarks}{{8.2.3}{53}{Benchmarks of Hyperparameters}{subsection.8.2.3}{}}
\newlabel{subsec:benchmarks@cref}{{[subsection][3][8,2]8.2.3}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3.1}The Effect of Number of Warm-up Steps}{53}{subsubsection.8.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces  The figure shows the computed $R^2$-scores in both log space and target space as a function number of warm-up steps (20\% burn-in and 80\% adaptation) achieved with HMC and NUTS. The architecture of the BNN model used is 5-20-20-1 with $\tanh (x)$ used as the activation function in the hidden layers. We performed 2500 pretraning steps with a batch size of 32 using the ADAM optimizer. In total 1000 neural networks were sampled with 10 steps between each stored sample. When HMC was used, we ran with a fixed number of Leapfrog steps $L = 512$. When the NUTS sampler was used, we allowed for a maximum of $L = 4096$ Leapfrog steps (a maximum tree depth of $12$). \relax }}{54}{figure.caption.25}\protected@file@percent }
\newlabel{fig:r2_score_vs_burn_in_steps}{{8.6}{54}{The figure shows the computed $R^2$-scores in both log space and target space as a function number of warm-up steps (20\% burn-in and 80\% adaptation) achieved with HMC and NUTS. The architecture of the BNN model used is 5-20-20-1 with $\tanh (x)$ used as the activation function in the hidden layers. We performed 2500 pretraning steps with a batch size of 32 using the ADAM optimizer. In total 1000 neural networks were sampled with 10 steps between each stored sample. When HMC was used, we ran with a fixed number of Leapfrog steps $L = 512$. When the NUTS sampler was used, we allowed for a maximum of $L = 4096$ Leapfrog steps (a maximum tree depth of $12$). \relax }{figure.caption.25}{}}
\newlabel{fig:r2_score_vs_burn_in_steps@cref}{{[figure][6][8]8.6}{[1][53][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The figure shows the standardized residuals computed on the test set. The model architechture used is a model with layers 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In the top figure, we have used the HMC sampler with a fixed number of Leapfrog steps $L = 512$. In the bottom figure, we have used the NUTS sampler with a maximum tree depth of $12$ corresponding to a maximum of $L = 2^{12} = 4096$ Leapfrog steps. The remaining important hyperparameters were 2500 pretraining epochs with a batch size of 32 using the ADAM optimizer. In total 1000 neural networks were sampled in each case with a thinning-amount of 10 steps between each sample. The colors indicate how many warm-up steps that were used. The dotted line is the standard Normal distribution. \relax }}{55}{figure.caption.26}\protected@file@percent }
\newlabel{fig:standardized_residuals_vs_burn_in_steps}{{8.7}{55}{The figure shows the standardized residuals computed on the test set. The model architechture used is a model with layers 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In the top figure, we have used the HMC sampler with a fixed number of Leapfrog steps $L = 512$. In the bottom figure, we have used the NUTS sampler with a maximum tree depth of $12$ corresponding to a maximum of $L = 2^{12} = 4096$ Leapfrog steps. The remaining important hyperparameters were 2500 pretraining epochs with a batch size of 32 using the ADAM optimizer. In total 1000 neural networks were sampled in each case with a thinning-amount of 10 steps between each sample. The colors indicate how many warm-up steps that were used. The dotted line is the standard Normal distribution. \relax }{figure.caption.26}{}}
\newlabel{fig:standardized_residuals_vs_burn_in_steps@cref}{{[figure][7][8]8.7}{[1][54][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The figure shows the average number of Leapfrog steps $L$ as a function of number of warm-up steps used by the NUTS sampler when sampling the models shown in the bottom of figure \ref  {fig:standardized_residuals_vs_burn_in_steps}. We have included a few more measurements to showcase how fluctuating the average number can be. \relax }}{56}{figure.caption.27}\protected@file@percent }
\newlabel{fig:avg_leapfrog_steps_vs_burn_in}{{8.8}{56}{The figure shows the average number of Leapfrog steps $L$ as a function of number of warm-up steps used by the NUTS sampler when sampling the models shown in the bottom of figure \ref {fig:standardized_residuals_vs_burn_in_steps}. We have included a few more measurements to showcase how fluctuating the average number can be. \relax }{figure.caption.27}{}}
\newlabel{fig:avg_leapfrog_steps_vs_burn_in@cref}{{[figure][8][8]8.8}{[1][56][]56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3.2}The Effect of Pretraining}{56}{subsubsection.8.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces The figure shows the computed $R^2$-scores of a model with the architecture 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In this case the varying number of the number of epochs run with pretraining starting from 32 all the way up to 8192. The batch size used was 32 with the ADAM optimizer. The number of warm-up steps was 1000 (200 of which were burn-in steps and 800 were adaptation steps). We fixed the Leapfrog steps to $L = 512$ using the HMC sampler. As usual we sampled 1000 neural networks with 10 steps between each sample. \relax }}{57}{figure.caption.28}\protected@file@percent }
\newlabel{fig:r2_score_vs_pretraining}{{8.9}{57}{The figure shows the computed $R^2$-scores of a model with the architecture 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In this case the varying number of the number of epochs run with pretraining starting from 32 all the way up to 8192. The batch size used was 32 with the ADAM optimizer. The number of warm-up steps was 1000 (200 of which were burn-in steps and 800 were adaptation steps). We fixed the Leapfrog steps to $L = 512$ using the HMC sampler. As usual we sampled 1000 neural networks with 10 steps between each sample. \relax }{figure.caption.28}{}}
\newlabel{fig:r2_score_vs_pretraining@cref}{{[figure][9][8]8.9}{[1][56][]57}}
\citation{ml_for_physicists}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces The figure shows the standardized residuals of a model with the architecture 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In this case the varying number of the number of epochs run with pretraining starting from 32 all the way up to 8192. The batch size used was 32, the number of warm-up steps was 1000 (200 of which were burn-in steps and 800 were adaptation steps). We fixed the Leapfrog steps to $L = 512$ using the HMC sampler. The ADAM optimizer was used for the pretraining phase. As usual we sampled 1000 neural networks with 10 steps between each sample. The colors indicate the number of pretraining epochs performed. The dotted line is the standard Normal distribution. \relax }}{58}{figure.caption.29}\protected@file@percent }
\newlabel{fig:std_residual_vs_pretraining}{{8.10}{58}{The figure shows the standardized residuals of a model with the architecture 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In this case the varying number of the number of epochs run with pretraining starting from 32 all the way up to 8192. The batch size used was 32, the number of warm-up steps was 1000 (200 of which were burn-in steps and 800 were adaptation steps). We fixed the Leapfrog steps to $L = 512$ using the HMC sampler. The ADAM optimizer was used for the pretraining phase. As usual we sampled 1000 neural networks with 10 steps between each sample. The colors indicate the number of pretraining epochs performed. The dotted line is the standard Normal distribution. \relax }{figure.caption.29}{}}
\newlabel{fig:std_residual_vs_pretraining@cref}{{[figure][10][8]8.10}{[1][57][]58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3.3}Effect of Number of Parameters}{58}{subsubsection.8.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces  The figure shows the $R^2$-score computed on the training and test data as a function of number of nodes $n$ in the hidden layer of models with architechture 5-$n$-1, yielding a total of $5n + 1$ parameters. The hidden layer activation used was $\tanh (x)$. The models were trained with 1000 warm-up steps (20\% burn-in and 80\% adaptation), gathering 1000 neural networks with 10 steps between each sample. We used 2500 pretraining epochs with a batch size of 32. When using the HMC sampler, we fixed the number of Leapfrog steps to $L = 512$. When using NUTS, we set a maximum of $L = 4096$ Leapfrog steps. \relax }}{59}{figure.caption.30}\protected@file@percent }
\newlabel{fig:r2_scores_vs_num_params}{{8.11}{59}{The figure shows the $R^2$-score computed on the training and test data as a function of number of nodes $n$ in the hidden layer of models with architechture 5-$n$-1, yielding a total of $5n + 1$ parameters. The hidden layer activation used was $\tanh (x)$. The models were trained with 1000 warm-up steps (20\% burn-in and 80\% adaptation), gathering 1000 neural networks with 10 steps between each sample. We used 2500 pretraining epochs with a batch size of 32. When using the HMC sampler, we fixed the number of Leapfrog steps to $L = 512$. When using NUTS, we set a maximum of $L = 4096$ Leapfrog steps. \relax }{figure.caption.30}{}}
\newlabel{fig:r2_scores_vs_num_params@cref}{{[figure][11][8]8.11}{[1][59][]59}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces  The figure shows the standardized residuals of models with an architecture 5-$n$-1 with $\tanh (x)$ as the hidden layer activation. The models were trained with 1000 warm-up steps (20\% burn-in and 80\% adaptation), drawing 1000 neural networks with 10 steps between each drawn sample. We used 2500 pretraining epochs with a batch size of 32 using the ADAM optimizer. The figure on top shows results of models trained with the HMC sampler where we fixed the number of Leapfrog steps to $L = 512$. The figure on the bottom shows the results of models trained with NUTS using a maximum of $L = 4096$ Leapfrog steps. The black dotted line shows the standard Normal distribution drawn in. \relax }}{60}{figure.caption.31}\protected@file@percent }
\newlabel{fig:standardized_residual_vs_params}{{8.12}{60}{The figure shows the standardized residuals of models with an architecture 5-$n$-1 with $\tanh (x)$ as the hidden layer activation. The models were trained with 1000 warm-up steps (20\% burn-in and 80\% adaptation), drawing 1000 neural networks with 10 steps between each drawn sample. We used 2500 pretraining epochs with a batch size of 32 using the ADAM optimizer. The figure on top shows results of models trained with the HMC sampler where we fixed the number of Leapfrog steps to $L = 512$. The figure on the bottom shows the results of models trained with NUTS using a maximum of $L = 4096$ Leapfrog steps. The black dotted line shows the standard Normal distribution drawn in. \relax }{figure.caption.31}{}}
\newlabel{fig:standardized_residual_vs_params@cref}{{[figure][12][8]8.12}{[1][59][]60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Predictive Distributions}{61}{subsection.8.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces  The figure shows the predictive distribution estimated by use of model 3 in table \ref  {tab:deep_models} for two randomly chosen points from the test set. The red line shows the true target and the black line shows the predicted sample mean obtained from the distribution. The figure on top demonstrates a case where the sample mean is approximately the same as the target, while the figure at the bottom demonstrates a case where the true target lies entirely outside the predictive distrbution. \relax }}{61}{figure.caption.32}\protected@file@percent }
\newlabel{fig:predictive_distributions}{{8.13}{61}{The figure shows the predictive distribution estimated by use of model 3 in table \ref {tab:deep_models} for two randomly chosen points from the test set. The red line shows the true target and the black line shows the predicted sample mean obtained from the distribution. The figure on top demonstrates a case where the sample mean is approximately the same as the target, while the figure at the bottom demonstrates a case where the true target lies entirely outside the predictive distrbution. \relax }{figure.caption.32}{}}
\newlabel{fig:predictive_distributions@cref}{{[figure][13][8]8.13}{[1][61][]61}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces  The figure shows the predictive distribution estimated by use of model 3 in table \ref  {tab:deep_models} computed on all datapoints in the training, validation and test data. The black line shows the 95\% line. The crosses indicate measured data with training data shown in blue, validation data shown in orange and test data shown in green. The $y$-axis shows the percentage of all targets lie on the interval $[\mu - k\sigma , \mu + k\sigma ]$ for $k=1,2,3,4,5$ where $\mu $ is the sample mean and $\sigma ^2$ is the sample variance of the predictive distribution. \relax }}{62}{figure.caption.33}\protected@file@percent }
\newlabel{fig:confidence}{{8.14}{62}{The figure shows the predictive distribution estimated by use of model 3 in table \ref {tab:deep_models} computed on all datapoints in the training, validation and test data. The black line shows the 95\% line. The crosses indicate measured data with training data shown in blue, validation data shown in orange and test data shown in green. The $y$-axis shows the percentage of all targets lie on the interval $[\mu - k\sigma , \mu + k\sigma ]$ for $k=1,2,3,4,5$ where $\mu $ is the sample mean and $\sigma ^2$ is the sample variance of the predictive distribution. \relax }{figure.caption.33}{}}
\newlabel{fig:confidence@cref}{{[figure][14][8]8.14}{[1][62][]62}}
\newlabel{chap:conclusion}{{8.2.4}{63}{Conclusion}{chapter*.34}{}}
\newlabel{chap:conclusion@cref}{{[subsection][4][8,2]8.2.4}{[1][63][]63}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{63}{chapter*.34}\protected@file@percent }
\citation{nuts,neal2011}
\citation{google_bnn_posteriors}
\citation{google_bnn_posteriors}
\citation{jax}
\@writefile{toc}{\contentsline {chapter}{Appendices}{65}{section*.36}\protected@file@percent }
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{Appendix A}{67}{appendix*.37}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Appendix 1 title }{67}{section.Alph0.1}\protected@file@percent }
\newlabel{sec:appendix_1_label}{{A.1}{67}{Appendix 1 title}{section.Alph0.1}{}}
\newlabel{sec:appendix_1_label@cref}{{[subappendix][1][2147483647,0]A.1}{[1][67][]67}}
\bibcite{nuts}{{1}{}{{}}{{}}}
\bibcite{xsec}{{2}{}{{}}{{}}}
\bibcite{colliderbit}{{3}{}{{}}{{}}}
\bibcite{prospino}{{4}{}{{}}{{}}}
\bibcite{universal_function_approximator}{{5}{}{{}}{{}}}
\bibcite{ADAM}{{6}{}{{}}{{}}}
\bibcite{bayes_theorem}{{7}{}{{}}{{}}}
\bibcite{ml_for_physicists}{{8}{}{{}}{{}}}
\bibcite{conceptual_intro_hmc}{{9}{}{{}}{{}}}
\bibcite{geometric_ergodicity}{{10}{}{{}}{{}}}
\bibcite{rhat}{{11}{}{{}}{{}}}
\bibcite{convergence_diagnostics}{{12}{}{{}}{{}}}
\bibcite{metropolis}{{13}{}{{}}{{}}}
\bibcite{metropolis_two}{{14}{}{{}}{{}}}
\bibcite{classical_mechanics}{{15}{}{{}}{{}}}
\bibcite{leapfrog}{{16}{}{{}}{{}}}
\bibcite{Nesterov2009}{{17}{}{{}}{{}}}
\bibcite{nuts_joonha_park}{{18}{}{{}}{{}}}
\bibcite{tf}{{19}{}{{}}{{}}}
\bibcite{backprop}{{20}{}{{}}{{}}}
\bibcite{swish}{{21}{}{{}}{{}}}
\bibcite{r2_score}{{22}{}{{}}{{}}}
\bibcite{xla}{{23}{}{{}}{{}}}
\bibcite{google_bnn_posteriors}{{24}{}{{}}{{}}}
\bibcite{neal2011}{{25}{}{{}}{{}}}
\bibcite{jax}{{26}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{90}
