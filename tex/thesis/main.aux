\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\babel@aux{UKenglish}{}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Physics Problem}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Computation of Beyond the Standard Model Cross Sections}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Tmp}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Formulation of Machine Learning}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bayesian_ml}{{2}{5}{Bayesian Formulation of Machine Learning}{chapter.2}{}}
\newlabel{chap:bayesian_ml@cref}{{[chapter][2][]2}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Core of Machine Learning}{5}{section.2.1}\protected@file@percent }
\newlabel{eq:model_assumption}{{2.1}{5}{The Core of Machine Learning}{equation.2.1.1}{}}
\newlabel{eq:model_assumption@cref}{{[equation][1][2]2.1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Loss Functions}{5}{subsection.2.1.1}\protected@file@percent }
\newlabel{eq:rss}{{2.2}{5}{Loss Functions}{equation.2.1.2}{}}
\newlabel{eq:rss@cref}{{[equation][2][2]2.2}{[1][5][]5}}
\newlabel{eq:mse}{{2.3}{5}{Loss Functions}{equation.2.1.3}{}}
\newlabel{eq:mse@cref}{{[equation][3][2]2.3}{[1][5][]5}}
\citation{ADAM}
\citation{bayes_theorem}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Regularization}{6}{subsection.2.1.2}\protected@file@percent }
\newlabel{eq:loss_l2_reg}{{2.4}{6}{Regularization}{equation.2.1.4}{}}
\newlabel{eq:loss_l2_reg@cref}{{[equation][4][2]2.4}{[1][6][]6}}
\newlabel{eq:loss_l1_reg}{{2.5}{6}{Regularization}{equation.2.1.5}{}}
\newlabel{eq:loss_l1_reg@cref}{{[equation][5][2]2.5}{[1][6][]6}}
\newlabel{eq:loss_fn}{{2.6}{6}{Regularization}{equation.2.1.6}{}}
\newlabel{eq:loss_fn@cref}{{[equation][6][2]2.6}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Optimization}{6}{subsection.2.1.3}\protected@file@percent }
\newlabel{eq:optimal_param}{{2.8}{6}{Optimization}{equation.2.1.8}{}}
\newlabel{eq:optimal_param@cref}{{[equation][8][2]2.8}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayes' theorem}{6}{section.2.2}\protected@file@percent }
\citation{ml_for_physicists}
\newlabel{eq:bayes_theorem}{{2.10}{7}{Bayes' theorem}{equation.2.2.10}{}}
\newlabel{eq:bayes_theorem@cref}{{[equation][10][2]2.10}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Bayesian Framework for Machine Learning}{7}{section.2.3}\protected@file@percent }
\newlabel{eq:mle}{{2.11}{7}{Bayesian Framework for Machine Learning}{equation.2.3.11}{}}
\newlabel{eq:mle@cref}{{[equation][11][2]2.11}{[1][7][]7}}
\newlabel{eq:map}{{2.12}{7}{Bayesian Framework for Machine Learning}{equation.2.3.12}{}}
\newlabel{eq:map@cref}{{[equation][12][2]2.12}{[1][7][]7}}
\newlabel{eq:likelihood_fn}{{2.17}{8}{Bayesian Framework for Machine Learning}{equation.2.3.17}{}}
\newlabel{eq:likelihood_fn@cref}{{[equation][17][2]2.17}{[1][7][]8}}
\newlabel{eq:gaussian_prior}{{2.19}{8}{Bayesian Framework for Machine Learning}{equation.2.3.19}{}}
\newlabel{eq:gaussian_prior@cref}{{[equation][19][2]2.19}{[1][8][]8}}
\newlabel{eq:posterior_function_of_loss}{{2.22}{8}{Bayesian Framework for Machine Learning}{equation.2.3.22}{}}
\newlabel{eq:posterior_function_of_loss@cref}{{[equation][22][2]2.22}{[1][8][]8}}
\newlabel{eq:loss_function_of_posterior}{{2.24}{8}{Bayesian Framework for Machine Learning}{equation.2.3.24}{}}
\newlabel{eq:loss_function_of_posterior@cref}{{[equation][24][2]2.24}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Bayesian Inference}{8}{section.2.4}\protected@file@percent }
\newlabel{eq:predictive_distribution}{{2.25}{9}{Bayesian Inference}{equation.2.4.25}{}}
\newlabel{eq:predictive_distribution@cref}{{[equation][25][2]2.25}{[1][9][]9}}
\newlabel{eq:predictive_dist_approx}{{2.26}{9}{Bayesian Inference}{equation.2.4.26}{}}
\newlabel{eq:predictive_dist_approx@cref}{{[equation][26][2]2.26}{[1][9][]9}}
\newlabel{eq:bayesian_expval}{{2.27}{9}{Bayesian Inference}{equation.2.4.27}{}}
\newlabel{eq:bayesian_expval@cref}{{[equation][27][2]2.27}{[1][9][]9}}
\citation{conceptual_intro_hmc}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Markov Chain Monte Carlo}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:mcmc}{{3}{11}{Markov Chain Monte Carlo}{chapter.3}{}}
\newlabel{chap:mcmc@cref}{{[chapter][3][]3}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Expectation Values and the Typical Set}{11}{section.3.1}\protected@file@percent }
\newlabel{eq:expval}{{3.1}{11}{Expectation Values and the Typical Set}{equation.3.1.1}{}}
\newlabel{eq:expval@cref}{{[equation][1][3]3.1}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Typical Set}{11}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}The Target Density and Bayesian Applications}{12}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Markov Chains and Markov Transitions}{12}{section.3.2}\protected@file@percent }
\newlabel{eq:detailed_balance}{{3.3}{12}{Markov Chains and Markov Transitions}{equation.3.2.3}{}}
\newlabel{eq:detailed_balance@cref}{{[equation][3][3]3.3}{[1][12][]12}}
\newlabel{eq:mcmc_estimator}{{3.4}{12}{Markov Chains and Markov Transitions}{equation.3.2.4}{}}
\newlabel{eq:mcmc_estimator@cref}{{[equation][4][3]3.4}{[1][12][]12}}
\citation{geometric_ergodicity}
\citation{rhat}
\citation{convergence_diagnostics}
\citation{metropolis,metropolis_two}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Ideal Markov Chains}{13}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Pathologies}{13}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Geometric Ergodicity and Convergence Diagnostics}{13}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Metropolis-Hastings}{13}{section.3.3}\protected@file@percent }
\newlabel{eq:general_acceptance_prob}{{3.5}{14}{Metropolis-Hastings}{equation.3.3.5}{}}
\newlabel{eq:general_acceptance_prob@cref}{{[equation][5][3]3.5}{[1][14][]14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Metropolis-Hastings\relax }}{14}{figure.caption.9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:general_metropolis}{{3.1}{14}{Metropolis-Hastings\relax }{figure.caption.9}{}}
\newlabel{algo:general_metropolis@cref}{{[algorithm][1][3]3.1}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The Proposal Distribution}{14}{subsection.3.3.1}\protected@file@percent }
\newlabel{eq:symmetric_acceptance_prob}{{3.7}{14}{The Proposal Distribution}{equation.3.3.7}{}}
\newlabel{eq:symmetric_acceptance_prob@cref}{{[equation][7][3]3.7}{[1][14][]14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Gibbs Sampling}{15}{section.3.4}\protected@file@percent }
\newlabel{eq:gibbs_sampling}{{3.8}{15}{Gibbs Sampling}{equation.3.4.8}{}}
\newlabel{eq:gibbs_sampling@cref}{{[equation][8][3]3.8}{[1][15][]15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces Gibbs sampling\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{algo:gibbs}{{3.2}{15}{Gibbs sampling\relax }{figure.caption.10}{}}
\newlabel{algo:gibbs@cref}{{[algorithm][2][3]3.2}{[1][15][]15}}
\citation{classical_mechanics}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Hamiltonian Monte Carlo}{17}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hmc}{{4}{17}{Hamiltonian Monte Carlo}{chapter.4}{}}
\newlabel{chap:hmc@cref}{{[chapter][4][]4}{[1][17][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hamiltonian Dynamics}{17}{section.4.1}\protected@file@percent }
\newlabel{sec:hamiltonian_dynamics}{{4.1}{17}{Hamiltonian Dynamics}{section.4.1}{}}
\newlabel{sec:hamiltonian_dynamics@cref}{{[section][1][4]4.1}{[1][17][]17}}
\newlabel{eq:hamiltons_eqs}{{4.1}{17}{Hamiltonian Dynamics}{equation.4.1.1}{}}
\newlabel{eq:hamiltons_eqs@cref}{{[equation][1][4]4.1}{[1][17][]17}}
\newlabel{eq:hamiltonian}{{4.2}{17}{Hamiltonian Dynamics}{equation.4.1.2}{}}
\newlabel{eq:hamiltonian@cref}{{[equation][2][4]4.2}{[1][17][]17}}
\citation{leapfrog}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Leapfrog integration}{18}{subsection.4.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Leapfrog Integration\relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{algo:leapfrog}{{4.1}{18}{Leapfrog Integration\relax }{figure.caption.11}{}}
\newlabel{algo:leapfrog@cref}{{[algorithm][1][4]4.1}{[1][18][]18}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Vectorized Leapfrog Integration\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{algo:vec_leapfrog}{{4.2}{19}{Vectorized Leapfrog Integration\relax }{figure.caption.12}{}}
\newlabel{algo:vec_leapfrog@cref}{{[algorithm][2][4]4.2}{[1][18][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating a Proposal State}{19}{section.4.2}\protected@file@percent }
\newlabel{eq:canonical_coordinate}{{4.5}{19}{Generating a Proposal State}{equation.4.2.5}{}}
\newlabel{eq:canonical_coordinate@cref}{{[equation][5][4]4.5}{[1][19][]19}}
\newlabel{eq:potential_energy}{{4.6}{19}{Generating a Proposal State}{equation.4.2.6}{}}
\newlabel{eq:potential_energy@cref}{{[equation][6][4]4.6}{[1][19][]19}}
\newlabel{eq:K_classical_physics}{{4.9}{19}{Generating a Proposal State}{equation.4.2.9}{}}
\newlabel{eq:K_classical_physics@cref}{{[equation][9][4]4.9}{[1][19][]19}}
\newlabel{eq:canonical_p}{{4.10}{19}{Generating a Proposal State}{equation.4.2.10}{}}
\newlabel{eq:canonical_p@cref}{{[equation][10][4]4.10}{[1][19][]19}}
\newlabel{eq:full_canonical}{{4.11}{20}{Generating a Proposal State}{equation.4.2.11}{}}
\newlabel{eq:full_canonical@cref}{{[equation][11][4]4.11}{[1][20][]20}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces Hamiltonian Monte Carlo\relax }}{21}{figure.caption.13}\protected@file@percent }
\newlabel{algo:hmc}{{4.3}{21}{Hamiltonian Monte Carlo\relax }{figure.caption.13}{}}
\newlabel{algo:hmc@cref}{{[algorithm][3][4]4.3}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Potential Energy Function in Bayesian Machine Learning Applications}{21}{section.4.3}\protected@file@percent }
\newlabel{eq:potential_energy_bayesian}{{4.15}{21}{The Potential Energy Function in Bayesian Machine Learning Applications}{equation.4.3.15}{}}
\newlabel{eq:potential_energy_bayesian@cref}{{[equation][15][4]4.15}{[1][21][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Limitations of Hamiltonian Monte Carlo}{21}{section.4.4}\protected@file@percent }
\citation{nuts}
\citation{Nesterov2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adaptive Hamiltonian Monte Carlo}{23}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:no_u_turn_sampler}{{5}{23}{Adaptive Hamiltonian Monte Carlo}{chapter.5}{}}
\newlabel{chap:no_u_turn_sampler@cref}{{[chapter][5][]5}{[1][23][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The No-U-Turn Sampler}{23}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Generation of States and the Stopping Criterion}{24}{subsection.5.1.1}\protected@file@percent }
\newlabel{eq:stopping_criterion1}{{5.5}{24}{Generation of States and the Stopping Criterion}{equation.5.1.5}{}}
\newlabel{eq:stopping_criterion1@cref}{{[equation][5][5]5.5}{[1][24][]24}}
\newlabel{eq:stopping_criterion2}{{5.6}{25}{Generation of States and the Stopping Criterion}{equation.5.1.6}{}}
\newlabel{eq:stopping_criterion2@cref}{{[equation][6][5]5.6}{[1][25][]25}}
\newlabel{eq:stop}{{5.7}{25}{Generation of States and the Stopping Criterion}{equation.5.1.7}{}}
\newlabel{eq:stop@cref}{{[equation][7][5]5.7}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Selecting Candidate Points}{25}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Efficiently Implementing the No-U-Turn Sampler}{26}{subsection.5.1.3}\protected@file@percent }
\newlabel{eq:nuts_observation}{{5.8}{26}{Efficiently Implementing the No-U-Turn Sampler}{equation.5.1.8}{}}
\newlabel{eq:nuts_observation@cref}{{[equation][8][5]5.8}{[1][26][]26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces Helper Function Used with the NUTS Sampler\relax }}{26}{figure.caption.14}\protected@file@percent }
\newlabel{algo:build_tree}{{5.1}{26}{Helper Function Used with the NUTS Sampler\relax }{figure.caption.14}{}}
\newlabel{algo:build_tree@cref}{{[algorithm][1][5]5.1}{[1][26][]26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.2}{\ignorespaces The NUTS Sampler\relax }}{27}{figure.caption.15}\protected@file@percent }
\newlabel{algo:nuts}{{5.2}{27}{The NUTS Sampler\relax }{figure.caption.15}{}}
\newlabel{algo:nuts@cref}{{[algorithm][2][5]5.2}{[1][26][]27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Computational Cost of The No-U-Turn Sampler}{27}{subsection.5.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Adapting the Step Size}{27}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Adapting a General Parameter Using a Dual Averaging Scheme}{27}{subsection.5.2.1}\protected@file@percent }
\citation{neal2011}
\newlabel{eq:update_rule}{{5.10}{28}{Adapting a General Parameter Using a Dual Averaging Scheme}{equation.5.2.10}{}}
\newlabel{eq:update_rule@cref}{{[equation][10][5]5.10}{[1][28][]28}}
\newlabel{eq:step_size_schedule}{{5.11}{28}{Adapting a General Parameter Using a Dual Averaging Scheme}{equation.5.2.11}{}}
\newlabel{eq:step_size_schedule@cref}{{[equation][11][5]5.11}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Setting the Step Size in Hamiltonian Monte Carlo}{28}{subsection.5.2.2}\protected@file@percent }
\newlabel{eq:HMC_statistic}{{5.13}{28}{Setting the Step Size in Hamiltonian Monte Carlo}{equation.5.2.13}{}}
\newlabel{eq:HMC_statistic@cref}{{[equation][13][5]5.13}{[1][28][]28}}
\newlabel{eq:HMC_statistic2}{{5.14}{28}{Setting the Step Size in Hamiltonian Monte Carlo}{equation.5.2.14}{}}
\newlabel{eq:HMC_statistic2@cref}{{[equation][14][5]5.14}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Adapting the Step Size with the No-U-Turn Sampler}{28}{subsection.5.2.3}\protected@file@percent }
\newlabel{eq:NUTS_statistic}{{5.15}{28}{Adapting the Step Size with the No-U-Turn Sampler}{equation.5.2.15}{}}
\newlabel{eq:NUTS_statistic@cref}{{[equation][15][5]5.15}{[1][28][]28}}
\newlabel{eq:NUTS_statistic2}{{5.16}{29}{Adapting the Step Size with the No-U-Turn Sampler}{equation.5.2.16}{}}
\newlabel{eq:NUTS_statistic2@cref}{{[equation][16][5]5.16}{[1][28][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Generalizing the Tuning Algorithm}{29}{subsection.5.2.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.3}{\ignorespaces Dual Averaging Step Size Adaptation\relax }}{29}{figure.caption.16}\protected@file@percent }
\newlabel{algo:dual_step_size_adaptation}{{5.3}{29}{Dual Averaging Step Size Adaptation\relax }{figure.caption.16}{}}
\newlabel{algo:dual_step_size_adaptation@cref}{{[algorithm][3][5]5.3}{[1][29][]29}}
\citation{tf}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bayesian Neural Networks}{31}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bnn}{{6}{31}{Bayesian Neural Networks}{chapter.6}{}}
\newlabel{chap:bnn@cref}{{[chapter][6][]6}{[1][31][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Neural Networks}{31}{section.6.1}\protected@file@percent }
\newlabel{sec:neural_networks}{{6.1}{31}{Neural Networks}{section.6.1}{}}
\newlabel{sec:neural_networks@cref}{{[section][1][6]6.1}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Basic Mathematical Structure}{31}{subsection.6.1.1}\protected@file@percent }
\citation{backprop}
\citation{ml_for_physicists}
\newlabel{eq:nn_forward_pass}{{6.2}{32}{Basic Mathematical Structure}{equation.6.1.2}{}}
\newlabel{eq:nn_forward_pass@cref}{{[equation][2][6]6.2}{[1][31][]32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Backpropagation}{32}{subsection.6.1.2}\protected@file@percent }
\newlabel{eq:backprop1}{{6.3}{32}{Backpropagation}{equation.6.1.3}{}}
\newlabel{eq:backprop1@cref}{{[equation][3][6]6.3}{[1][32][]32}}
\newlabel{eq:backprop2}{{6.5}{32}{Backpropagation}{equation.6.1.5}{}}
\newlabel{eq:backprop2@cref}{{[equation][5][6]6.5}{[1][32][]32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.1}{\ignorespaces Backpropagation: Forward pass\relax }}{33}{figure.caption.17}\protected@file@percent }
\newlabel{algo:forward_pass}{{6.1}{33}{Backpropagation: Forward pass\relax }{figure.caption.17}{}}
\newlabel{algo:forward_pass@cref}{{[algorithm][1][6]6.1}{[1][32][]33}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.2}{\ignorespaces Backpropagation: Backward pass\relax }}{33}{figure.caption.18}\protected@file@percent }
\newlabel{algo:backward_pass}{{6.2}{33}{Backpropagation: Backward pass\relax }{figure.caption.18}{}}
\newlabel{algo:backward_pass@cref}{{[algorithm][2][6]6.2}{[1][33][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Regularization in Neural Networks}{33}{subsection.6.1.3}\protected@file@percent }
\citation{swish}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Activation Functions}{34}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Sigmoid and Tanh}{34}{subsection.6.2.1}\protected@file@percent }
\newlabel{eq:sigmoid}{{6.10}{34}{Sigmoid and Tanh}{equation.6.2.10}{}}
\newlabel{eq:sigmoid@cref}{{[equation][10][6]6.10}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}ReLU}{34}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Swish}{34}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bayesian learning of Neural Networks using Monte Carlo Samplers}{34}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}What \textit  {is} Bayesian learning of Neural Networks?}{35}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}The Potential Energy Function of Neural Networks}{35}{subsection.6.3.2}\protected@file@percent }
\newlabel{eq:model_priors}{{6.14}{35}{The Potential Energy Function of Neural Networks}{equation.6.3.14}{}}
\newlabel{eq:model_priors@cref}{{[equation][14][6]6.14}{[1][35][]35}}
\newlabel{eq:special_potential_energy}{{6.16}{35}{The Potential Energy Function of Neural Networks}{equation.6.3.16}{}}
\newlabel{eq:special_potential_energy@cref}{{[equation][16][6]6.16}{[1][35][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Practical Training of Bayesian Neural Networks}{35}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Training Algorithm of Bayesian Neural Networks}{36}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Numerical Experiments}{37}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:numerical_experiments}{{7}{37}{Numerical Experiments}{chapter.7}{}}
\newlabel{chap:numerical_experiments@cref}{{[chapter][7][]7}{[1][37][]37}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}The Dataset}{37}{section.7.1}\protected@file@percent }
\newlabel{sec:dataset}{{7.1}{37}{The Dataset}{section.7.1}{}}
\newlabel{sec:dataset@cref}{{[section][1][7]7.1}{[1][37][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Data Generation}{37}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Data Scaling and Transformations}{37}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Performance Metrics}{37}{section.7.2}\protected@file@percent }
\newlabel{seq:perf_metrics}{{7.2}{37}{Performance Metrics}{section.7.2}{}}
\newlabel{seq:perf_metrics@cref}{{[section][2][7]7.2}{[1][37][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Relative Error}{37}{subsection.7.2.1}\protected@file@percent }
\citation{xla}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Standardized Residuals}{38}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{38}{section.7.3}\protected@file@percent }
\newlabel{sec:results}{{7.3}{38}{Results}{section.7.3}{}}
\newlabel{sec:results@cref}{{[section][3][7]7.3}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Computational Performance}{38}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.1}CPU versus GPU Performance}{38}{subsubsection.7.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.2}Prediction Time}{38}{subsubsection.7.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The figure shows the relative measured execution time used per sample using $L = 512$ Leapfrog steps, as a function of number of parameters. The CPU measurements are done using an 8-core M1 CPU (Apple Silicon). The GPU measurements are made with an NVIDIA Tesla P100 GPU. \relax }}{39}{figure.caption.19}\protected@file@percent }
\newlabel{fig:relative_performance}{{7.1}{39}{The figure shows the relative measured execution time used per sample using $L = 512$ Leapfrog steps, as a function of number of parameters. The CPU measurements are done using an 8-core M1 CPU (Apple Silicon). The GPU measurements are made with an NVIDIA Tesla P100 GPU. \relax }{figure.caption.19}{}}
\newlabel{fig:relative_performance@cref}{{[figure][1][7]7.1}{[1][38][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The figure shows the average prediction time to compute a prediction given a single input $x$ using the models in table \ref  {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }}{40}{figure.caption.20}\protected@file@percent }
\newlabel{fig:prediction_time}{{7.2}{40}{The figure shows the average prediction time to compute a prediction given a single input $x$ using the models in table \ref {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }{figure.caption.20}{}}
\newlabel{fig:prediction_time@cref}{{[figure][2][7]7.2}{[1][40][]40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.3}Loading Times}{40}{subsubsection.7.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The figure shows the average prediction time using the built-in GPU on an M1 Apple Silicon system-on-chip to compute a prediction given a single input $x$ using the models in table \ref  {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }}{41}{figure.caption.21}\protected@file@percent }
\newlabel{fig:prediction_time_gpu}{{7.3}{41}{The figure shows the average prediction time using the built-in GPU on an M1 Apple Silicon system-on-chip to compute a prediction given a single input $x$ using the models in table \ref {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }{figure.caption.21}{}}
\newlabel{fig:prediction_time_gpu@cref}{{[figure][3][7]7.3}{[1][40][]41}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces  The figure shows the histograms of measured loading times in seconds using the models in table \ref  {tab:deep_models}. The measurements were performed using {\tt  time.perf\_counter} from Python using an M1 Apple Silicon system-on-chip. The time measurements consist of 1000 measurements for each model. \relax }}{42}{figure.caption.22}\protected@file@percent }
\newlabel{fig:loading_times}{{7.4}{42}{The figure shows the histograms of measured loading times in seconds using the models in table \ref {tab:deep_models}. The measurements were performed using {\tt time.perf\_counter} from Python using an M1 Apple Silicon system-on-chip. The time measurements consist of 1000 measurements for each model. \relax }{figure.caption.22}{}}
\newlabel{fig:loading_times@cref}{{[figure][4][7]7.4}{[1][42][]42}}
\citation{google_bnn_posteriors}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Posterior Distribution of Weights}{43}{subsection.7.3.2}\protected@file@percent }
\newlabel{eq:surrogate_dist}{{7.4}{43}{Posterior Distribution of Weights}{equation.7.3.4}{}}
\newlabel{eq:surrogate_dist@cref}{{[equation][4][7]7.4}{[1][43][]43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Benchmarks of Hyperparameters}{43}{subsection.7.3.3}\protected@file@percent }
\newlabel{subsec:benchmarks}{{7.3.3}{43}{Benchmarks of Hyperparameters}{subsection.7.3.3}{}}
\newlabel{subsec:benchmarks@cref}{{[subsection][3][7,3]7.3.3}{[1][43][]43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.1}The Effect of Number of Burn-in Steps}{43}{subsubsection.7.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.2}The Effect of Pretraining}{43}{subsubsection.7.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.3}The Effect of }{43}{subsubsection.7.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.4}Number of model parameters}{43}{subsubsection.7.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Neutralino-Neutralino Cross Sections}{43}{subsection.7.3.4}\protected@file@percent }
\newlabel{subsec:neuralino_experiments}{{7.3.4}{43}{Neutralino-Neutralino Cross Sections}{subsection.7.3.4}{}}
\newlabel{subsec:neuralino_experiments@cref}{{[subsection][4][7,3]7.3.4}{[1][43][]43}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces  The table shows the training configuration used to sample the models listed in table~\ref  {tab:deep_models}. \relax }}{43}{table.caption.25}\protected@file@percent }
\newlabel{tab:NN_mse_scores}{{7.1}{43}{The table shows the training configuration used to sample the models listed in table~\ref {tab:deep_models}. \relax }{table.caption.25}{}}
\newlabel{tab:NN_mse_scores@cref}{{[table][1][7]7.1}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces The figure shows the projection of the empirical distribution onto the planes spanned by $(W_{2,4}^1, W_{2,5}^1)$ on the left and onto the plane spanned by $(W_{3,7}^1, W_{3, 6}^1)$ on the right, using the samples from model 2 in table \ref  {tab:deep_models}. The distributions are approximated using kernel density estimation. \relax }}{44}{figure.caption.23}\protected@file@percent }
\newlabel{fig:posterior_kernels}{{7.5}{44}{The figure shows the projection of the empirical distribution onto the planes spanned by $(W_{2,4}^1, W_{2,5}^1)$ on the left and onto the plane spanned by $(W_{3,7}^1, W_{3, 6}^1)$ on the right, using the samples from model 2 in table \ref {tab:deep_models}. The distributions are approximated using kernel density estimation. \relax }{figure.caption.23}{}}
\newlabel{fig:posterior_kernels@cref}{{[figure][5][7]7.5}{[1][43][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces The figure shows the measured time in seconds per sample using HMC as a function of Leapfrog steps $L$ using a model with 561 parameters. \relax }}{45}{figure.caption.24}\protected@file@percent }
\newlabel{fig:time_vs_leapfrogsteps}{{7.6}{45}{The figure shows the measured time in seconds per sample using HMC as a function of Leapfrog steps $L$ using a model with 561 parameters. \relax }{figure.caption.24}{}}
\newlabel{fig:time_vs_leapfrogsteps@cref}{{[figure][6][7]7.6}{[1][43][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces The figure shows histograms of the standardized residuals computed for different BNNs. The normal distribution is drawn as dotted line. \relax }}{46}{figure.caption.27}\protected@file@percent }
\newlabel{fig:standardized_residual}{{7.7}{46}{The figure shows histograms of the standardized residuals computed for different BNNs. The normal distribution is drawn as dotted line. \relax }{figure.caption.27}{}}
\newlabel{fig:standardized_residual@cref}{{[figure][7][7]7.7}{[1][43][]46}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces  The table shows the models used in this section. For each model, 1000 sampled networks are used to generate each result shown in this section. The number of nodes per layer is shown in the ``Layers'' column. For each hidden layer, we used $\tanh (x)$ as the activation function. The final layer uses an identity function. \relax }}{47}{table.caption.26}\protected@file@percent }
\newlabel{tab:deep_models}{{7.2}{47}{The table shows the models used in this section. For each model, 1000 sampled networks are used to generate each result shown in this section. The number of nodes per layer is shown in the ``Layers'' column. For each hidden layer, we used $\tanh (x)$ as the activation function. The final layer uses an identity function. \relax }{table.caption.26}{}}
\newlabel{tab:deep_models@cref}{{[table][2][7]7.2}{[1][43][]47}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion}{49}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{8}{51}{Conclusion}{chapter*.28}{}}
\newlabel{chap:conclusion@cref}{{[chapter][8][]8}{[1][51][]51}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{52}{chapter*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendices}{53}{section*.29}\protected@file@percent }
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{Appendix A}{55}{appendix*.30}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Appendix 1 title }{55}{section.Alph0.1}\protected@file@percent }
\newlabel{sec:appendix_1_label}{{A.1}{55}{Appendix 1 title}{section.Alph0.1}{}}
\newlabel{sec:appendix_1_label@cref}{{[subappendix][1][2147483647,0]A.1}{[1][55][]55}}
\bibcite{ADAM}{{1}{}{{}}{{}}}
\bibcite{bayes_theorem}{{2}{}{{}}{{}}}
\bibcite{ml_for_physicists}{{3}{}{{}}{{}}}
\bibcite{conceptual_intro_hmc}{{4}{}{{}}{{}}}
\bibcite{geometric_ergodicity}{{5}{}{{}}{{}}}
\bibcite{rhat}{{6}{}{{}}{{}}}
\bibcite{convergence_diagnostics}{{7}{}{{}}{{}}}
\bibcite{metropolis}{{8}{}{{}}{{}}}
\bibcite{metropolis_two}{{9}{}{{}}{{}}}
\bibcite{classical_mechanics}{{10}{}{{}}{{}}}
\bibcite{leapfrog}{{11}{}{{}}{{}}}
\bibcite{nuts}{{12}{}{{}}{{}}}
\bibcite{Nesterov2009}{{13}{}{{}}{{}}}
\bibcite{neal2011}{{14}{}{{}}{{}}}
\bibcite{tf}{{15}{}{{}}{{}}}
\bibcite{backprop}{{16}{}{{}}{{}}}
\bibcite{swish}{{17}{}{{}}{{}}}
\bibcite{xla}{{18}{}{{}}{{}}}
\bibcite{google_bnn_posteriors}{{19}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{75}
