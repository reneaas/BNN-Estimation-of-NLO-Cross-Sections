\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\babel@aux{UKenglish}{}
\citation{nuts}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Physics Problem}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:physics_problem}{{1}{3}{The Physics Problem}{chapter.1}{}}
\newlabel{chap:physics_problem@cref}{{[chapter][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Computation of Beyond the Standard Model Cross Sections}{3}{section.1.1}\protected@file@percent }
\newlabel{eq:total_events_decomp}{{1.2}{3}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.2}{}}
\newlabel{eq:total_events_decomp@cref}{{[equation][2][1]1.2}{[1][3][]3}}
\citation{prospino}
\citation{xsec}
\newlabel{eq:general_event_eq}{{1.3}{4}{Computation of Beyond the Standard Model Cross Sections}{equation.1.1.3}{}}
\newlabel{eq:general_event_eq@cref}{{[equation][3][1]1.3}{[1][3][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Bayesian Regression as a Substitute}{4}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Formulation of Machine Learning}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bayesian_ml}{{2}{5}{Bayesian Formulation of Machine Learning}{chapter.2}{}}
\newlabel{chap:bayesian_ml@cref}{{[chapter][2][]2}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Core of Machine Learning}{5}{section.2.1}\protected@file@percent }
\newlabel{eq:model_assumption}{{2.1}{5}{The Core of Machine Learning}{equation.2.1.1}{}}
\newlabel{eq:model_assumption@cref}{{[equation][1][2]2.1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Loss Functions}{5}{subsection.2.1.1}\protected@file@percent }
\newlabel{eq:rss}{{2.2}{5}{Loss Functions}{equation.2.1.2}{}}
\newlabel{eq:rss@cref}{{[equation][2][2]2.2}{[1][5][]5}}
\newlabel{eq:mse}{{2.3}{5}{Loss Functions}{equation.2.1.3}{}}
\newlabel{eq:mse@cref}{{[equation][3][2]2.3}{[1][5][]5}}
\citation{ADAM}
\citation{bayes_theorem}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Regularization}{6}{subsection.2.1.2}\protected@file@percent }
\newlabel{eq:loss_l2_reg}{{2.4}{6}{Regularization}{equation.2.1.4}{}}
\newlabel{eq:loss_l2_reg@cref}{{[equation][4][2]2.4}{[1][6][]6}}
\newlabel{eq:loss_l1_reg}{{2.5}{6}{Regularization}{equation.2.1.5}{}}
\newlabel{eq:loss_l1_reg@cref}{{[equation][5][2]2.5}{[1][6][]6}}
\newlabel{eq:loss_fn}{{2.6}{6}{Regularization}{equation.2.1.6}{}}
\newlabel{eq:loss_fn@cref}{{[equation][6][2]2.6}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Optimization}{6}{subsection.2.1.3}\protected@file@percent }
\newlabel{eq:optimal_param}{{2.8}{6}{Optimization}{equation.2.1.8}{}}
\newlabel{eq:optimal_param@cref}{{[equation][8][2]2.8}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayes' theorem}{6}{section.2.2}\protected@file@percent }
\citation{ml_for_physicists}
\newlabel{eq:bayes_theorem}{{2.10}{7}{Bayes' theorem}{equation.2.2.10}{}}
\newlabel{eq:bayes_theorem@cref}{{[equation][10][2]2.10}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Bayesian Framework for Machine Learning}{7}{section.2.3}\protected@file@percent }
\newlabel{eq:mle}{{2.11}{7}{Bayesian Framework for Machine Learning}{equation.2.3.11}{}}
\newlabel{eq:mle@cref}{{[equation][11][2]2.11}{[1][7][]7}}
\newlabel{eq:map}{{2.12}{7}{Bayesian Framework for Machine Learning}{equation.2.3.12}{}}
\newlabel{eq:map@cref}{{[equation][12][2]2.12}{[1][7][]7}}
\newlabel{eq:likelihood_fn}{{2.17}{8}{Bayesian Framework for Machine Learning}{equation.2.3.17}{}}
\newlabel{eq:likelihood_fn@cref}{{[equation][17][2]2.17}{[1][7][]8}}
\newlabel{eq:gaussian_prior}{{2.19}{8}{Bayesian Framework for Machine Learning}{equation.2.3.19}{}}
\newlabel{eq:gaussian_prior@cref}{{[equation][19][2]2.19}{[1][8][]8}}
\newlabel{eq:posterior_function_of_loss}{{2.22}{8}{Bayesian Framework for Machine Learning}{equation.2.3.22}{}}
\newlabel{eq:posterior_function_of_loss@cref}{{[equation][22][2]2.22}{[1][8][]8}}
\newlabel{eq:loss_function_of_posterior}{{2.24}{8}{Bayesian Framework for Machine Learning}{equation.2.3.24}{}}
\newlabel{eq:loss_function_of_posterior@cref}{{[equation][24][2]2.24}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Bayesian Inference}{8}{section.2.4}\protected@file@percent }
\newlabel{eq:predictive_distribution}{{2.25}{9}{Bayesian Inference}{equation.2.4.25}{}}
\newlabel{eq:predictive_distribution@cref}{{[equation][25][2]2.25}{[1][9][]9}}
\newlabel{eq:predictive_dist_approx}{{2.26}{9}{Bayesian Inference}{equation.2.4.26}{}}
\newlabel{eq:predictive_dist_approx@cref}{{[equation][26][2]2.26}{[1][9][]9}}
\newlabel{eq:bayesian_expval}{{2.27}{9}{Bayesian Inference}{equation.2.4.27}{}}
\newlabel{eq:bayesian_expval@cref}{{[equation][27][2]2.27}{[1][9][]9}}
\citation{conceptual_intro_hmc}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Markov Chain Monte Carlo}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:mcmc}{{3}{11}{Markov Chain Monte Carlo}{chapter.3}{}}
\newlabel{chap:mcmc@cref}{{[chapter][3][]3}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Expectation Values and the Typical Set}{11}{section.3.1}\protected@file@percent }
\newlabel{eq:expval}{{3.1}{11}{Expectation Values and the Typical Set}{equation.3.1.1}{}}
\newlabel{eq:expval@cref}{{[equation][1][3]3.1}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Typical Set}{11}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}The Target Density and Bayesian Applications}{12}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Markov Chains and Markov Transitions}{12}{section.3.2}\protected@file@percent }
\newlabel{eq:detailed_balance}{{3.3}{12}{Markov Chains and Markov Transitions}{equation.3.2.3}{}}
\newlabel{eq:detailed_balance@cref}{{[equation][3][3]3.3}{[1][12][]12}}
\newlabel{eq:mcmc_estimator}{{3.4}{12}{Markov Chains and Markov Transitions}{equation.3.2.4}{}}
\newlabel{eq:mcmc_estimator@cref}{{[equation][4][3]3.4}{[1][12][]12}}
\citation{geometric_ergodicity}
\citation{rhat}
\citation{convergence_diagnostics}
\citation{metropolis,metropolis_two}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Ideal Markov Chains}{13}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Pathologies}{13}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Geometric Ergodicity and Convergence Diagnostics}{13}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Metropolis-Hastings}{13}{section.3.3}\protected@file@percent }
\newlabel{eq:general_acceptance_prob}{{3.5}{14}{Metropolis-Hastings}{equation.3.3.5}{}}
\newlabel{eq:general_acceptance_prob@cref}{{[equation][5][3]3.5}{[1][14][]14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Metropolis-Hastings\relax }}{14}{figure.caption.9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:general_metropolis}{{3.1}{14}{Metropolis-Hastings\relax }{figure.caption.9}{}}
\newlabel{algo:general_metropolis@cref}{{[algorithm][1][3]3.1}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The Proposal Distribution}{14}{subsection.3.3.1}\protected@file@percent }
\newlabel{eq:symmetric_acceptance_prob}{{3.7}{14}{The Proposal Distribution}{equation.3.3.7}{}}
\newlabel{eq:symmetric_acceptance_prob@cref}{{[equation][7][3]3.7}{[1][14][]14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Gibbs Sampling}{15}{section.3.4}\protected@file@percent }
\newlabel{eq:gibbs_sampling}{{3.8}{15}{Gibbs Sampling}{equation.3.4.8}{}}
\newlabel{eq:gibbs_sampling@cref}{{[equation][8][3]3.8}{[1][15][]15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces Gibbs sampling\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{algo:gibbs}{{3.2}{15}{Gibbs sampling\relax }{figure.caption.10}{}}
\newlabel{algo:gibbs@cref}{{[algorithm][2][3]3.2}{[1][15][]15}}
\citation{classical_mechanics}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Hamiltonian Monte Carlo}{17}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hmc}{{4}{17}{Hamiltonian Monte Carlo}{chapter.4}{}}
\newlabel{chap:hmc@cref}{{[chapter][4][]4}{[1][17][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hamiltonian Dynamics}{17}{section.4.1}\protected@file@percent }
\newlabel{sec:hamiltonian_dynamics}{{4.1}{17}{Hamiltonian Dynamics}{section.4.1}{}}
\newlabel{sec:hamiltonian_dynamics@cref}{{[section][1][4]4.1}{[1][17][]17}}
\newlabel{eq:hamiltons_eqs}{{4.1}{17}{Hamiltonian Dynamics}{equation.4.1.1}{}}
\newlabel{eq:hamiltons_eqs@cref}{{[equation][1][4]4.1}{[1][17][]17}}
\newlabel{eq:hamiltonian}{{4.2}{17}{Hamiltonian Dynamics}{equation.4.1.2}{}}
\newlabel{eq:hamiltonian@cref}{{[equation][2][4]4.2}{[1][17][]17}}
\citation{leapfrog}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Leapfrog integration}{18}{subsection.4.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Leapfrog Integration\relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{algo:leapfrog}{{4.1}{18}{Leapfrog Integration\relax }{figure.caption.11}{}}
\newlabel{algo:leapfrog@cref}{{[algorithm][1][4]4.1}{[1][18][]18}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Vectorized Leapfrog Integration\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{algo:vec_leapfrog}{{4.2}{19}{Vectorized Leapfrog Integration\relax }{figure.caption.12}{}}
\newlabel{algo:vec_leapfrog@cref}{{[algorithm][2][4]4.2}{[1][18][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating a Proposal State}{19}{section.4.2}\protected@file@percent }
\newlabel{eq:canonical_coordinate}{{4.5}{19}{Generating a Proposal State}{equation.4.2.5}{}}
\newlabel{eq:canonical_coordinate@cref}{{[equation][5][4]4.5}{[1][19][]19}}
\newlabel{eq:potential_energy}{{4.6}{19}{Generating a Proposal State}{equation.4.2.6}{}}
\newlabel{eq:potential_energy@cref}{{[equation][6][4]4.6}{[1][19][]19}}
\newlabel{eq:K_classical_physics}{{4.9}{19}{Generating a Proposal State}{equation.4.2.9}{}}
\newlabel{eq:K_classical_physics@cref}{{[equation][9][4]4.9}{[1][19][]19}}
\newlabel{eq:canonical_p}{{4.10}{19}{Generating a Proposal State}{equation.4.2.10}{}}
\newlabel{eq:canonical_p@cref}{{[equation][10][4]4.10}{[1][19][]19}}
\newlabel{eq:full_canonical}{{4.11}{20}{Generating a Proposal State}{equation.4.2.11}{}}
\newlabel{eq:full_canonical@cref}{{[equation][11][4]4.11}{[1][20][]20}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces Hamiltonian Monte Carlo\relax }}{21}{figure.caption.13}\protected@file@percent }
\newlabel{algo:hmc}{{4.3}{21}{Hamiltonian Monte Carlo\relax }{figure.caption.13}{}}
\newlabel{algo:hmc@cref}{{[algorithm][3][4]4.3}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Potential Energy Function in Bayesian Machine Learning Applications}{21}{section.4.3}\protected@file@percent }
\newlabel{eq:potential_energy_bayesian}{{4.15}{21}{The Potential Energy Function in Bayesian Machine Learning Applications}{equation.4.3.15}{}}
\newlabel{eq:potential_energy_bayesian@cref}{{[equation][15][4]4.15}{[1][21][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Limitations of Hamiltonian Monte Carlo}{21}{section.4.4}\protected@file@percent }
\citation{nuts}
\citation{Nesterov2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adaptive Hamiltonian Monte Carlo}{23}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:no_u_turn_sampler}{{5}{23}{Adaptive Hamiltonian Monte Carlo}{chapter.5}{}}
\newlabel{chap:no_u_turn_sampler@cref}{{[chapter][5][]5}{[1][23][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The No-U-Turn Sampler}{23}{section.5.1}\protected@file@percent }
\citation{nuts}
\citation{nuts}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The figure shows an example of a trajectory generated by the NUTS sampler. The balanced binary tree structure is drawn in on the trajectory as well as illustrated at the bottom. The numbering displays the order in which the states are generated by Leapfrog integration. The black node is the initial node. The first doubling is forwards in time and yields the rightmost node of the first binary tree. The second doubling is backwards in time and is initiated from the black node, yielding a new tree of height 2 where the left subtree is the new states (the yellow nodes). The next doubling is also backwards in time, and the Leapfrog integrator is initiated from the tail (the leftmost yellow node) for four Leapfrog steps generating a subtree which becomes the left half of the next tree (blue nodes). The final doubling in the figure is forwards in time with $L = 8$ Leapfrog steps are taken from the orange node (which was the leftmost leaf of the tree befoure the final doubling) which yields the green nodes. The figure is a modified version of a diagram in \cite  {nuts}.\relax }}{24}{figure.caption.14}\protected@file@percent }
\newlabel{fig:nuts_trajectory}{{5.1}{24}{The figure shows an example of a trajectory generated by the NUTS sampler. The balanced binary tree structure is drawn in on the trajectory as well as illustrated at the bottom. The numbering displays the order in which the states are generated by Leapfrog integration. The black node is the initial node. The first doubling is forwards in time and yields the rightmost node of the first binary tree. The second doubling is backwards in time and is initiated from the black node, yielding a new tree of height 2 where the left subtree is the new states (the yellow nodes). The next doubling is also backwards in time, and the Leapfrog integrator is initiated from the tail (the leftmost yellow node) for four Leapfrog steps generating a subtree which becomes the left half of the next tree (blue nodes). The final doubling in the figure is forwards in time with $L = 8$ Leapfrog steps are taken from the orange node (which was the leftmost leaf of the tree befoure the final doubling) which yields the green nodes. The figure is a modified version of a diagram in \cite {nuts}.\relax }{figure.caption.14}{}}
\newlabel{fig:nuts_trajectory@cref}{{[figure][1][5]5.1}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Stopping Criterion}{24}{subsection.5.1.1}\protected@file@percent }
\newlabel{eq:change_in_distance}{{5.1}{25}{Stopping Criterion}{equation.5.1.1}{}}
\newlabel{eq:change_in_distance@cref}{{[equation][1][5]5.1}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Generation of States and the Stopping Criterion}{25}{subsection.5.1.2}\protected@file@percent }
\citation{tf}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bayesian Neural Networks}{27}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bnn}{{6}{27}{Bayesian Neural Networks}{chapter.6}{}}
\newlabel{chap:bnn@cref}{{[chapter][6][]6}{[1][27][]27}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Neural Networks}{27}{section.6.1}\protected@file@percent }
\newlabel{sec:neural_networks}{{6.1}{27}{Neural Networks}{section.6.1}{}}
\newlabel{sec:neural_networks@cref}{{[section][1][6]6.1}{[1][27][]27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Basic Mathematical Structure}{27}{subsection.6.1.1}\protected@file@percent }
\citation{backprop}
\citation{ml_for_physicists}
\newlabel{eq:nn_forward_pass}{{6.2}{28}{Basic Mathematical Structure}{equation.6.1.2}{}}
\newlabel{eq:nn_forward_pass@cref}{{[equation][2][6]6.2}{[1][27][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Backpropagation}{28}{subsection.6.1.2}\protected@file@percent }
\newlabel{eq:backprop1}{{6.3}{28}{Backpropagation}{equation.6.1.3}{}}
\newlabel{eq:backprop1@cref}{{[equation][3][6]6.3}{[1][28][]28}}
\newlabel{eq:backprop2}{{6.5}{28}{Backpropagation}{equation.6.1.5}{}}
\newlabel{eq:backprop2@cref}{{[equation][5][6]6.5}{[1][28][]28}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.1}{\ignorespaces Backpropagation: Forward pass\relax }}{29}{figure.caption.15}\protected@file@percent }
\newlabel{algo:forward_pass}{{6.1}{29}{Backpropagation: Forward pass\relax }{figure.caption.15}{}}
\newlabel{algo:forward_pass@cref}{{[algorithm][1][6]6.1}{[1][28][]29}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.2}{\ignorespaces Backpropagation: Backward pass\relax }}{29}{figure.caption.16}\protected@file@percent }
\newlabel{algo:backward_pass}{{6.2}{29}{Backpropagation: Backward pass\relax }{figure.caption.16}{}}
\newlabel{algo:backward_pass@cref}{{[algorithm][2][6]6.2}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Regularization in Neural Networks}{29}{subsection.6.1.3}\protected@file@percent }
\citation{swish}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Activation Functions}{30}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Sigmoid and Tanh}{30}{subsection.6.2.1}\protected@file@percent }
\newlabel{eq:sigmoid}{{6.10}{30}{Sigmoid and Tanh}{equation.6.2.10}{}}
\newlabel{eq:sigmoid@cref}{{[equation][10][6]6.10}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}ReLU}{30}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Swish}{30}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bayesian learning of Neural Networks using Monte Carlo Samplers}{30}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}What \textit  {is} Bayesian learning of Neural Networks?}{31}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}The Potential Energy Function of Neural Networks}{31}{subsection.6.3.2}\protected@file@percent }
\newlabel{eq:model_priors}{{6.14}{31}{The Potential Energy Function of Neural Networks}{equation.6.3.14}{}}
\newlabel{eq:model_priors@cref}{{[equation][14][6]6.14}{[1][31][]31}}
\newlabel{eq:special_potential_energy}{{6.16}{31}{The Potential Energy Function of Neural Networks}{equation.6.3.16}{}}
\newlabel{eq:special_potential_energy@cref}{{[equation][16][6]6.16}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Practical Training of Bayesian Neural Networks}{31}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:practical_bnn}{{6.3.3}{31}{Practical Training of Bayesian Neural Networks}{subsection.6.3.3}{}}
\newlabel{sec:practical_bnn@cref}{{[subsection][3][6,3]6.3.3}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Training Algorithm of Bayesian Neural Networks}{32}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Numerical Experiments}{35}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:numerical_experiments}{{7}{35}{Numerical Experiments}{chapter.7}{}}
\newlabel{chap:numerical_experiments@cref}{{[chapter][7][]7}{[1][35][]35}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}The Dataset}{35}{section.7.1}\protected@file@percent }
\newlabel{sec:dataset}{{7.1}{35}{The Dataset}{section.7.1}{}}
\newlabel{sec:dataset@cref}{{[section][1][7]7.1}{[1][35][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Data Generation}{35}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Data Scaling and Transformations}{35}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Data Splitting}{35}{subsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Methodology}{36}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Implementation}{36}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Selection of Models and Hyperparameters}{36}{subsection.7.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces  The table shows the models used in this section. For each model, 1000 sampled networks were sampled to collectively represent each BNN model. We used 2500 warm-up steps (20\% burn-in and 80\% adaptation). We skipped 10 samples for each sampled network. We used 1000 pretraining epochs with a batch size of 32. The kernel used for each model was the NUTS kernel with a maximum of $L = 4096$ Leapfrog steps. The number of nodes per layer is shown in the ``Layers'' column. For each hidden layer, we used $\tanh (x)$ as the activation function. The final layer uses an identity function. \relax }}{36}{table.caption.17}\protected@file@percent }
\newlabel{tab:deep_models}{{7.1}{36}{The table shows the models used in this section. For each model, 1000 sampled networks were sampled to collectively represent each BNN model. We used 2500 warm-up steps (20\% burn-in and 80\% adaptation). We skipped 10 samples for each sampled network. We used 1000 pretraining epochs with a batch size of 32. The kernel used for each model was the NUTS kernel with a maximum of $L = 4096$ Leapfrog steps. The number of nodes per layer is shown in the ``Layers'' column. For each hidden layer, we used $\tanh (x)$ as the activation function. The final layer uses an identity function. \relax }{table.caption.17}{}}
\newlabel{tab:deep_models@cref}{{[table][1][7]7.1}{[1][36][]36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Performance Metrics}{36}{subsection.7.2.3}\protected@file@percent }
\newlabel{sec:perf_metrics}{{7.2.3}{36}{Performance Metrics}{subsection.7.2.3}{}}
\newlabel{sec:perf_metrics@cref}{{[subsection][3][7,2]7.2.3}{[1][36][]36}}
\citation{xla}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3.1}Relative Error}{37}{subsubsection.7.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3.2}Standardized Residuals}{37}{subsubsection.7.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{37}{section.7.3}\protected@file@percent }
\newlabel{sec:results}{{7.3}{37}{Results}{section.7.3}{}}
\newlabel{sec:results@cref}{{[section][3][7]7.3}{[1][37][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Computational Performance}{37}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.1}CPU v. GPU Performance}{37}{subsubsection.7.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The figure shows the relative measured execution time used per sample using $L = 512$ Leapfrog steps, as a function of number of parameters. The CPU measurements are done using an 8-core M1 CPU (Apple Silicon). The GPU measurements are made with an NVIDIA Tesla P100 GPU. \relax }}{38}{figure.caption.18}\protected@file@percent }
\newlabel{fig:relative_performance}{{7.1}{38}{The figure shows the relative measured execution time used per sample using $L = 512$ Leapfrog steps, as a function of number of parameters. The CPU measurements are done using an 8-core M1 CPU (Apple Silicon). The GPU measurements are made with an NVIDIA Tesla P100 GPU. \relax }{figure.caption.18}{}}
\newlabel{fig:relative_performance@cref}{{[figure][1][7]7.1}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.2}Prediction Time}{38}{subsubsection.7.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The figure shows the average prediction time to compute a prediction given a single input $x$ using the models in table \ref  {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }}{39}{figure.caption.19}\protected@file@percent }
\newlabel{fig:prediction_time}{{7.2}{39}{The figure shows the average prediction time to compute a prediction given a single input $x$ using the models in table \ref {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }{figure.caption.19}{}}
\newlabel{fig:prediction_time@cref}{{[figure][2][7]7.2}{[1][39][]39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1.3}Loading Times}{39}{subsubsection.7.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The figure shows the average prediction time using the built-in GPU on an M1 Apple Silicon system-on-chip to compute a prediction given a single input $x$ using the models in table \ref  {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }}{40}{figure.caption.20}\protected@file@percent }
\newlabel{fig:prediction_time_gpu}{{7.3}{40}{The figure shows the average prediction time using the built-in GPU on an M1 Apple Silicon system-on-chip to compute a prediction given a single input $x$ using the models in table \ref {tab:deep_models}. The average time used is measured in ms and is averaged over 1000 randomly sampled points. The measured time includes computation of the sample mean and sample error. \relax }{figure.caption.20}{}}
\newlabel{fig:prediction_time_gpu@cref}{{[figure][3][7]7.3}{[1][39][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces  The figure shows the histograms of measured loading times in seconds using the models in table \ref  {tab:deep_models}. The measurements were performed using {\tt  time.perf\_counter} from Python using an M1 Apple Silicon system-on-chip. The time measurements consist of 1000 measurements for each model. \relax }}{41}{figure.caption.21}\protected@file@percent }
\newlabel{fig:loading_times}{{7.4}{41}{The figure shows the histograms of measured loading times in seconds using the models in table \ref {tab:deep_models}. The measurements were performed using {\tt time.perf\_counter} from Python using an M1 Apple Silicon system-on-chip. The time measurements consist of 1000 measurements for each model. \relax }{figure.caption.21}{}}
\newlabel{fig:loading_times@cref}{{[figure][4][7]7.4}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Posterior Distribution of Weights}{41}{subsection.7.3.2}\protected@file@percent }
\newlabel{eq:surrogate_dist}{{7.4}{41}{Posterior Distribution of Weights}{equation.7.3.4}{}}
\newlabel{eq:surrogate_dist@cref}{{[equation][4][7]7.4}{[1][41][]41}}
\citation{google_bnn_posteriors}
\citation{nuts}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Benchmarks of Hyperparameters}{42}{subsection.7.3.3}\protected@file@percent }
\newlabel{subsec:benchmarks}{{7.3.3}{42}{Benchmarks of Hyperparameters}{subsection.7.3.3}{}}
\newlabel{subsec:benchmarks@cref}{{[subsection][3][7,3]7.3.3}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.1}The Effect of Number of Burn-in Steps and Step Size Adaptation}{42}{subsubsection.7.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.2}The Effect of Pretraining}{42}{subsubsection.7.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces The figure shows the projection of the empirical distribution onto the planes spanned by $(W_{2,4}^1, W_{2,5}^1)$ on the left and onto the plane spanned by $(W_{3,7}^1, W_{3, 6}^1)$ on the right, using the samples from model 3 in table \ref  {tab:deep_models}. The distributions are approximated using kernel density estimation. \relax }}{43}{figure.caption.22}\protected@file@percent }
\newlabel{fig:posterior_kernels}{{7.5}{43}{The figure shows the projection of the empirical distribution onto the planes spanned by $(W_{2,4}^1, W_{2,5}^1)$ on the left and onto the plane spanned by $(W_{3,7}^1, W_{3, 6}^1)$ on the right, using the samples from model 3 in table \ref {tab:deep_models}. The distributions are approximated using kernel density estimation. \relax }{figure.caption.22}{}}
\newlabel{fig:posterior_kernels@cref}{{[figure][5][7]7.5}{[1][42][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces The figure at the top shows the measured time in seconds per sample using HMC as a function of Leapfrog steps $L$ using a model with 561 parameters. The figure at the bottom shows the time in seconds per sample with the same sampler with a fixed number of Leapfrog steps $L = 512$ as a function of number of parameters in the BNN model. \relax }}{44}{figure.caption.23}\protected@file@percent }
\newlabel{fig:time_vs_leapfrogsteps}{{7.6}{44}{The figure at the top shows the measured time in seconds per sample using HMC as a function of Leapfrog steps $L$ using a model with 561 parameters. The figure at the bottom shows the time in seconds per sample with the same sampler with a fixed number of Leapfrog steps $L = 512$ as a function of number of parameters in the BNN model. \relax }{figure.caption.23}{}}
\newlabel{fig:time_vs_leapfrogsteps@cref}{{[figure][6][7]7.6}{[1][42][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces The figure shows the standardized residuals computed on the testset. The model architechture used is a model with layers 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In the top figure, we have used the HMC sampler with a fixed number of Leapfrog steps $L = 512$. In the bottom figure, we have used the NUTS sampler with a maximum tree depth of $12$ corresponding to a maximum of $L = 2^{12} = 4096$ Leapfrog steps. The remaining important hyperparameters were 2500 pretraining epochs with a batch size of 32 using the ADAM optimizer. In total a 1000 neural networks were sampled in each case with a thinning-amount of 10 steps between each sample. \relax }}{45}{figure.caption.24}\protected@file@percent }
\newlabel{fig:standardized_residuals_vs_burn_in_steps}{{7.7}{45}{The figure shows the standardized residuals computed on the testset. The model architechture used is a model with layers 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In the top figure, we have used the HMC sampler with a fixed number of Leapfrog steps $L = 512$. In the bottom figure, we have used the NUTS sampler with a maximum tree depth of $12$ corresponding to a maximum of $L = 2^{12} = 4096$ Leapfrog steps. The remaining important hyperparameters were 2500 pretraining epochs with a batch size of 32 using the ADAM optimizer. In total a 1000 neural networks were sampled in each case with a thinning-amount of 10 steps between each sample. \relax }{figure.caption.24}{}}
\newlabel{fig:standardized_residuals_vs_burn_in_steps@cref}{{[figure][7][7]7.7}{[1][42][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The figure shows the average number of Leapfrog steps $L$ as a function of number of warm-up steps used by the NUTS sampler when sampling the models shown in the bottom of figure \ref  {fig:standardized_residuals_vs_burn_in_steps}. We have included a few more measurements to showcase how fluctuating the average number can be. \relax }}{46}{figure.caption.25}\protected@file@percent }
\newlabel{fig:avg_leapfrog_steps_vs_burn_in}{{7.8}{46}{The figure shows the average number of Leapfrog steps $L$ as a function of number of warm-up steps used by the NUTS sampler when sampling the models shown in the bottom of figure \ref {fig:standardized_residuals_vs_burn_in_steps}. We have included a few more measurements to showcase how fluctuating the average number can be. \relax }{figure.caption.25}{}}
\newlabel{fig:avg_leapfrog_steps_vs_burn_in@cref}{{[figure][8][7]7.8}{[1][42][]46}}
\citation{ml_for_physicists}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces The figure shows the standardized residuals of a model with the architecture 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In this case the varying number of the number of epochs run with pretraining starting from 32 all the way up to 8192. The batch size used was 32, the number of warm-up steps was 1000 (200 of which were burn-in steps and 800 were adaptation steps). We fixed the Leapfrog steps to $L = 512$ using the HMC sampler. The ADAM optimizer was used for the pretraining phase. As usual we sampled 1000 neural networks with 10 steps between each sample. \relax }}{47}{figure.caption.26}\protected@file@percent }
\newlabel{fig:std_residual_vs_pretraining}{{7.9}{47}{The figure shows the standardized residuals of a model with the architecture 5-20-20-1 with $\tanh (x)$ as the hidden activation function. In this case the varying number of the number of epochs run with pretraining starting from 32 all the way up to 8192. The batch size used was 32, the number of warm-up steps was 1000 (200 of which were burn-in steps and 800 were adaptation steps). We fixed the Leapfrog steps to $L = 512$ using the HMC sampler. The ADAM optimizer was used for the pretraining phase. As usual we sampled 1000 neural networks with 10 steps between each sample. \relax }{figure.caption.26}{}}
\newlabel{fig:std_residual_vs_pretraining@cref}{{[figure][9][7]7.9}{[1][47][]47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3.3}Effect of Number of Parameters}{47}{subsubsection.7.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces  The figure shows the distribution of the standardized residuals computed on the test data using the models listed in table \ref  {tab:deep_models}. The Normal distribution is drawn in with a dotted black line for benchmarking reference. The figure is meant to illustrate the performance of the models with respect to the number of parameters in the models. The models were trained with 2500 warm-up steps (20\% burn-in and 80\% adaptation), gathering 1000 neural networks with 10 steps between each sample. We used 1000 pretraining epochs with a batch size of 32. The kernel used was the NUTS kernel with a maximum of $L = 4096$ Leapfrog steps. \relax }}{48}{figure.caption.27}\protected@file@percent }
\newlabel{fig:standardized_residual_vs_params}{{7.10}{48}{The figure shows the distribution of the standardized residuals computed on the test data using the models listed in table \ref {tab:deep_models}. The Normal distribution is drawn in with a dotted black line for benchmarking reference. The figure is meant to illustrate the performance of the models with respect to the number of parameters in the models. The models were trained with 2500 warm-up steps (20\% burn-in and 80\% adaptation), gathering 1000 neural networks with 10 steps between each sample. We used 1000 pretraining epochs with a batch size of 32. The kernel used was the NUTS kernel with a maximum of $L = 4096$ Leapfrog steps. \relax }{figure.caption.27}{}}
\newlabel{fig:standardized_residual_vs_params@cref}{{[figure][10][7]7.10}{[1][47][]48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Predictive Distributions}{48}{subsection.7.3.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces  The table shows the training configuration used to sample the models listed in table~\ref  {tab:deep_models}. \relax }}{48}{table.caption.29}\protected@file@percent }
\newlabel{tab:NN_mse_scores}{{7.2}{48}{The table shows the training configuration used to sample the models listed in table~\ref {tab:deep_models}. \relax }{table.caption.29}{}}
\newlabel{tab:NN_mse_scores@cref}{{[table][2][7]7.2}{[1][48][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces  The figure shows the predictive distribution estimated by use of model 3 in table \ref  {tab:deep_models} for to randomly chosen points from the test set. The red line shows the true target and the black line shows the predicted sample mean obtained from the distribution. The figure on top demonstrates a case where the sample mean is approximately the same as the target, while the figure at the bottom demonstrates a case where the true target lies entirely outside the predictive distrbution. \relax }}{49}{figure.caption.28}\protected@file@percent }
\newlabel{fig:predictive_distributions}{{7.11}{49}{The figure shows the predictive distribution estimated by use of model 3 in table \ref {tab:deep_models} for to randomly chosen points from the test set. The red line shows the true target and the black line shows the predicted sample mean obtained from the distribution. The figure on top demonstrates a case where the sample mean is approximately the same as the target, while the figure at the bottom demonstrates a case where the true target lies entirely outside the predictive distrbution. \relax }{figure.caption.28}{}}
\newlabel{fig:predictive_distributions@cref}{{[figure][11][7]7.11}{[1][48][]49}}
\newlabel{chap:conclusion}{{7.3.4}{51}{Conclusion}{chapter*.30}{}}
\newlabel{chap:conclusion@cref}{{[subsection][4][7,3]7.3.4}{[1][51][]51}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{52}{chapter*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendices}{53}{section*.31}\protected@file@percent }
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{Appendix A}{55}{appendix*.32}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Appendix 1 title }{55}{section.Alph0.1}\protected@file@percent }
\newlabel{sec:appendix_1_label}{{A.1}{55}{Appendix 1 title}{section.Alph0.1}{}}
\newlabel{sec:appendix_1_label@cref}{{[subappendix][1][2147483647,0]A.1}{[1][55][]55}}
\bibcite{nuts}{{1}{}{{}}{{}}}
\bibcite{prospino}{{2}{}{{}}{{}}}
\bibcite{xsec}{{3}{}{{}}{{}}}
\bibcite{ADAM}{{4}{}{{}}{{}}}
\bibcite{bayes_theorem}{{5}{}{{}}{{}}}
\bibcite{ml_for_physicists}{{6}{}{{}}{{}}}
\bibcite{conceptual_intro_hmc}{{7}{}{{}}{{}}}
\bibcite{geometric_ergodicity}{{8}{}{{}}{{}}}
\bibcite{rhat}{{9}{}{{}}{{}}}
\bibcite{convergence_diagnostics}{{10}{}{{}}{{}}}
\bibcite{metropolis}{{11}{}{{}}{{}}}
\bibcite{metropolis_two}{{12}{}{{}}{{}}}
\bibcite{classical_mechanics}{{13}{}{{}}{{}}}
\bibcite{leapfrog}{{14}{}{{}}{{}}}
\bibcite{Nesterov2009}{{15}{}{{}}{{}}}
\bibcite{tf}{{16}{}{{}}{{}}}
\bibcite{backprop}{{17}{}{{}}{{}}}
\bibcite{swish}{{18}{}{{}}{{}}}
\bibcite{xla}{{19}{}{{}}{{}}}
\bibcite{google_bnn_posteriors}{{20}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{76}
