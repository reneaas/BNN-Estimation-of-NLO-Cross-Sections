
\section{Bayesian Inference}
The foundation for Bayesian inference is Bayes' theorem, which can be formulated as
\begin{equation}
	P(\theta | D) = \frac{P(D|\theta)P(\theta)}{P(D)}.
\end{equation}
where $D$ is observed data and $\theta$ are the model parameters.
Some useful terminology is in order. $P(\theta)$ is called the \textit{prior} distribution and embodies our prior knowledge of $\theta$ before any new observations are considered. $P(D|\theta)$ is called the \textit{likelihood} function and provides information about $\theta$ learned from observing the data $D$. The \textit{posterior} distribution $P(\theta|D)$ models our belief about $\theta$ after the data $D$ is observed. More succinctly, we can write Bayes' theorem as
\begin{equation}
  P(\theta|D) \propto P(\theta|D)P(\theta),
\end{equation}
because its rarely of interest, or tractable, to compute $P(D)$, known as the \textit{evidence}.

The objective of Bayesian inference is to compute the \textit{predictive} distribution for an unseen datapoint $y^*$, which can be expressed as
the integral over all parameters of the posterior distribution weighted by the likelihood. Given a dataset of observations $D = \{y^{(1)}, \ldots, y^{(n)}\}$, this implies
\begin{equation}
  P(y^*|D) = \int P(y^*, \theta|D)\dd\theta =\int P(y^*|\theta, D)P(\theta|D)\dd \theta = \int P(y^*|\theta)P(\theta|D)\dd\theta,
\end{equation}
which loosely describes the probability of observing $y^*$ given the observations in $D$.


\section{Bayesian Framework for Neural Networks}

We can specialize the equations used in Bayesian inference for neural networks in the context of regression.
The predictive distribution $P(y|x, \theta)$ seeks to model a function $f : \mathbb{R}^p \to \mathbb{R}^d$ that for a given $x \in \mathbb{R}^p$ produces an output $y \in \mathbb{R}^d$. In the infinite data limit, the distribution should be a Dirac delta function. For finite datasets, however, we instead seek a distribution of outputs given the input features.

Consider a set of observations $D = \{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\}$, where $x^{(i)} \in \mathbb{R}^p$ are the input features and $y^{(i)} \in \mathbb{R}^d$ are the targets. The equation for the predictive distribution of $y^*$ given an input $x^*$ changes to
\begin{equation}
  P(y^*|x^*, D) = \int P(y^*|x^*, \theta)P(\theta|D)\dd \theta.
\end{equation}
Assuming that the the observations in $D$ are drawn independently, the likelihood function can be expressed as
\begin{equation}
  P(D|\theta) = \prod_{i=1}^n P(y^{(i)}|x^{(i)}, \theta).
\end{equation}
In the context of regression, the likelihood for a given observation $(x,y)$ is commonly chosen to be
\begin{comment}
  \begin{equation}
    P(y|x, \theta) \propto \exp\left(-\frac{(y - f(x; \theta))^2}{2\sigma^2}\right),
  \end{equation}
\end{comment}
\begin{equation}\label{eq:nn_likelihood}
  P(y|x, \theta) \propto \exp\left(-\frac{\lambda}{2}\norm{y - f(x;\theta)}_2^2\right).
\end{equation}
where $f(x; \theta)$ is the output of the neural network and $\lambda$ is a hyperparameter typically chosen to be identical for all inputs $(x, y)$ during training. The likelihood found eq.~\eqref{eq:nn_likelihood} is equivalent to using $L^2$-norm as a loss function with regularization strength $\lambda$ when framed as a minimization problem, which we will see shortly.

In practice, however, we instead sample a set of network parameters $\{\theta_1, ..., \theta_n\}$ from the posterior distribution
\begin{equation}
  P(\theta|D) \propto P(D|\theta)P(\theta),
\end{equation}
from which we can produce a set of predictions $\{\hat{y}_1,\ldots \hat{y}_n\}$ from the neural network model 
\begin{equation}
  \hat{y}_i = f(x, \theta_i).
\end{equation}
Given this set of predictions, we can compute the sample mean
\begin{equation}
  \hat{y}_\text{MLE} = \frac{1}{n}\sum_i f(x, \theta_i),
\end{equation}
which is an approximation to the \textit{maximum likelihood estimate} (MLE). Furthermore, an estimate of the error is provided by the sample \textit{covariance}
\begin{equation}\label{eq:covariance}
  \text{Cov}(\hat{y}) = \frac{1}{n-1}\sum_i (\hat{y}^{(i)} - \hat{y}_\text{MLE})(\hat{y}^{(i)} - \hat{y}_\text{MLE})^T.
\end{equation}
The diagonal terms of eq.~\eqref{eq:covariance} yields the sample \textit{variance} of the components of $\{\hat{y}^{(i)}\}_{i=1}^n$.

\begin{comment}
\section{Sources of Uncertainty}
The main motivation utilizing Bayesian inference to neural networks is to be able to quantify uncertainty. 
Uncertainty can be further divided into two categories:
\begin{itemize}
  \item \textbf{Epistemic uncertainty}: the systematic uncertainty. It practice, this uncertainty can be quantified by 
  the posterior of the model $P(\theta|D)$. Thus it quantifies the uncertainty in the model itself.
  \item \textbf{Aleatoric uncertainty}: the statistical uncertainty, related to the conditional distribution $P(y|x, \theta)$.
  This uncertainty quantifies the error due to random chance, and measures the random noise in the data.
\end{itemize}
In principle, in the infinite data limit, both errors goes to zero as there's only a single parametrization $\theta$ that explains all data $D$,
and there's only a single output $y$ for a given $x$ and $\theta$. However, neural networks as so-called \textit{over-parametrized}
models in the sense that they have many equivalent parametrizations. These occur due to underlying symmetries of the model, 
two of which are \textit{weight-space symmetry} and \textit{scaling symmetry}. The former means that we can permute parameters
in two adjacent and produce the same output as before the permutation. 
The latter case occur when the network uses non-linear activation that obey $\sigma(\alpha x) = \alpha \sigma(x)$. 
This relation implies that we can rescale the parameters 
of two adjacent layers by a factor of $\alpha$ and $1/\alpha$ respectively and produce the same output. Thus a parametrization
that explains a set of observations does not exist for sufficiently complex models.
\end{comment}

\section{Bayesian learning using HMC}
To learn from the data $D$ using HMC, we need to define a potential energy function $V(q)$ 
and a kinetic energy function $K(p)$. In the case of a neural network, the potential energy can be specified in the generic form
\begin{equation}\label{eq:generic_potential_energy}
  V(W, b) = - \log Z - \sum_{\ell}\log P(W^\ell) - \sum_{\ell}\log P(b^\ell) - \sum_i \log P(y^{(i)}| x^{(i)}, W, b).
\end{equation}
where $W$ and $b$ collectively denotes all the kernels and biases of the model, respectively.
We thus need to specify a prior for kernels $W^\ell$ and the biases $b^\ell$, as well as a likelihood given an input $(x, y)$.
The priors are typically chosen to be Gaussian,
\begin{equation}\label{eq:model_priors}
  P(W^\ell) \propto \exp\left(-\frac{\lambda_W}{2}\norm{W^\ell}_2^2\right), \qquad P(b^\ell) \propto \exp\left(-\frac{\lambda_b}{2}\norm{b^\ell}_2^2\right).
\end{equation}
The likelihood for regressions tasks was defined in eq.~\eqref{eq:nn_likelihood}. Combining this and eq.~\eqref{eq:model_priors},
we can write down an explicit expression for the potential energy as
\begin{equation}\label{eq:special_potential_energy}
  V(W, b) = -\log Z + \frac{\lambda_W}{2} \sum_{\ell} \norm{W^\ell}_2^2 + \frac{\lambda_b}{2}\sum_{\ell} \norm{b^\ell}_2^2 + \frac{\lambda}{2}\sum_{i} \norm{y^{(i)} - f(x^{(i)}; W, b)}_2^2,  
\end{equation}
given a set of datapoints $D = \{x^{(i)}, y^{(i)}\}_{i=1}^n$.
Here $Z$ denotes the normalization constant of the distribution $P \propto \exp(-V(W, b))$, 
but is of no importance for the sampling procedure because it either vanishes when the gradient of $V$ is computed,
or when we compute the energy difference between the initial state $(W, b)$ and the proposed state $(W^*, b^*)$.
Finally, we need to specify a distribution for the kinetic energy. Since we interpret the Hamiltonian as the negative log likelihood
of the target distribution, we have implicitly defined the distribution as Gaussian
\begin{equation}
  P(p) \propto \prod_\ell \exp\left(-\frac{(p^\ell)^T (M^\ell)^{-1} p^{\ell}}{2}\right),
\end{equation}
where $M^\ell$ is the diagonal mass matrix of layer $\ell$ with its mass elements corresponding to each individual momentum and $p^\ell$
denotes the generalized momenta of layers $\ell$.


With the ingredients introduced hitherto, we can proceed to sample from a network with an arbitrary network architecture.
The procedure is a follows. 

\begin{enumerate}
  \item Initialize the weights of the network sampled from the priors.
  \item Train the network using the backpropagation algorithm to reach an initial network state in the high probability region of the posterior.
  This is done to reduce the number of burn-in steps needed for the the Markov chain to reach the stationary distribution.
  \item Perform a small number of burn-in steps. This step may be redundant as the network is minimized using eq.~\eqref{eq:special_potential_energy},
  which will place us in a high probability region once the loss is minimized. 
  \item Sample a set of neural network parameters from the posterior distribution.
\end{enumerate}
