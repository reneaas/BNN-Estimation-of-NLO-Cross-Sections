
\section{Bayesian Framework for Neural Networks}

We can specialize the equations used in Bayesian inference for neural networks in the context of regression.
The predictive distribution $P(y|x, \theta)$ seeks to model a function $f : \mathbb{R}^p \to \mathbb{R}^d$ that for a given $x \in \mathbb{R}^p$ produces an output $y \in \mathbb{R}^d$. In the infinite data limit, the distribution should be a Dirac delta function. For finite datasets, however, we instead seek a distribution of outputs given the input features.

Consider a set of observations $D = \{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\}$, where $x^{(i)} \in \mathbb{R}^p$ are the input features and $y^{(i)} \in \mathbb{R}^d$ are the targets. The equation for the predictive distribution of $y^*$ given an input $x^*$ changes to
\begin{equation}
  P(y^*|x^*, D) = \int P(y^*|x^*, \theta)P(\theta|D)\dd \theta.
\end{equation}
Assuming that the the observations in $D$ are drawn independently, the likelihood function can be expressed as
\begin{equation}
  P(D|\theta) = \prod_{i=1}^n P(y^{(i)}|x^{(i)}, \theta).
\end{equation}
In the context of regression, the likelihood for a given observation $(x,y)$ is commonly chosen to be
\begin{comment}
  \begin{equation}
    P(y|x, \theta) \propto \exp\left(-\frac{(y - f(x; \theta))^2}{2\sigma^2}\right),
  \end{equation}
\end{comment}
\begin{equation}\label{eq:nn_likelihood}
  P(y|x, \theta) \propto \exp\left(-\frac{\lambda}{2}\norm{y - f(x;\theta)}_2^2\right).
\end{equation}
where $f(x; \theta)$ is the output of the neural network and $\lambda$ is a hyperparameter typically chosen to be identical for all inputs $(x, y)$ during training. The likelihood found eq.~\eqref{eq:nn_likelihood} is equivalent to using $L^2$-norm as a loss function with regularization strength $\lambda$ when framed as a minimization problem, which we will see shortly.

In practice, however, we instead sample a set of network parameters $\{\theta_1, ..., \theta_n\}$ from the posterior distribution
\begin{equation}
  P(\theta|D) \propto P(D|\theta)P(\theta),
\end{equation}
from which we can produce a set of predictions $\{\hat{y}_1,\ldots \hat{y}_n\}$ from the neural network model 
\begin{equation}
  \hat{y}_i = f(x, \theta_i).
\end{equation}
Given this set of predictions, we can compute the sample mean
\begin{equation}
  \hat{y}_\text{MLE} = \frac{1}{n}\sum_i f(x, \theta_i),
\end{equation}
which is an approximation to the \textit{maximum likelihood estimate} (MLE). Furthermore, an estimate of the error is provided by the sample \textit{covariance}
\begin{equation}\label{eq:covariance}
  \text{Cov}(\hat{y}) = \frac{1}{n-1}\sum_i (\hat{y}^{(i)} - \hat{y}_\text{MLE})(\hat{y}^{(i)} - \hat{y}_\text{MLE})^T.
\end{equation}
The diagonal terms of eq.~\eqref{eq:covariance} yields the sample \textit{variance} of the components of $\{\hat{y}^{(i)}\}_{i=1}^n$.


\section{Bayesian learning using HMC}
To learn from the data $D$ using HMC, we need to define a potential energy function $V(q)$ 
and a kinetic energy function $K(p)$. In the case of a neural network, the potential energy can be specified in the generic form
\begin{equation}\label{eq:generic_potential_energy}
  V(W, b) = - \log Z - \sum_{\ell}\log P(W^\ell) - \sum_{\ell}\log P(b^\ell) - \sum_i \log P(y^{(i)}| x^{(i)}, W, b).
\end{equation}
where $W$ and $b$ collectively denotes all the kernels and biases of the model, respectively.
We thus need to specify a prior for kernels $W^\ell$ and the biases $b^\ell$, as well as a likelihood given an input $(x, y)$.
The priors are typically chosen to be Gaussian,
\begin{equation}\label{eq:model_priors}
  P(W^\ell) \propto \exp\left(-\frac{\lambda_W}{2}\norm{W^\ell}_2^2\right), \qquad P(b^\ell) \propto \exp\left(-\frac{\lambda_b}{2}\norm{b^\ell}_2^2\right).
\end{equation}
The likelihood for regressions tasks was defined in eq.~\eqref{eq:nn_likelihood}. Combining this and eq.~\eqref{eq:model_priors},
we can write down an explicit expression for the potential energy as
\begin{equation}\label{eq:special_potential_energy}
  V(W, b) = -\log Z + \frac{\lambda_W}{2} \sum_{\ell} \norm{W^\ell}_2^2 + \frac{\lambda_b}{2}\sum_{\ell} \norm{b^\ell}_2^2 + \frac{\lambda}{2}\sum_{i} \norm{y^{(i)} - f(x^{(i)}; W, b)}_2^2,  
\end{equation}
given a set of datapoints $D = \{x^{(i)}, y^{(i)}\}_{i=1}^n$.
Here $Z$ denotes the normalization constant of the distribution $P \propto \exp(-V(W, b))$, 
but is of no importance for the sampling procedure because it either vanishes when the gradient of $V$ is computed,
or when we compute the energy difference between the initial state $(W, b)$ and the proposed state $(W^*, b^*)$.
Finally, we need to specify a distribution for the kinetic energy. Since we interpret the Hamiltonian as the negative log likelihood
of the target distribution, we have implicitly defined the distribution as Gaussian
\begin{equation}
  P(p) \propto \prod_\ell \exp\left(-\frac{(p^\ell)^T (M^\ell)^{-1} p^{\ell}}{2}\right),
\end{equation}
where $M^\ell$ is the diagonal mass matrix of layer $\ell$ with its mass elements corresponding to each individual momentum and $p^\ell$
denotes the generalized momenta of layers $\ell$.


With the ingredients introduced hitherto, we can proceed to sample from a network with an arbitrary network architecture.
The procedure is a follows. 

\begin{enumerate}
  \item Initialize the weights of the network sampled from the priors.
  \item Train the network using the backpropagation algorithm to reach an initial network state in the high probability region of the posterior.
  This is done to reduce the number of burn-in steps needed for the the Markov chain to reach the stationary distribution.
  \item Perform a small number of burn-in steps. This step may be redundant as the network is minimized using eq.~\eqref{eq:special_potential_energy},
  which will place us in a high probability region once the loss is minimized. 
  \item Sample a set of neural network parameters from the posterior distribution.
\end{enumerate}
