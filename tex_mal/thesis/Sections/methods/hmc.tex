In this chapter, we will study the details of the Hamiltonian Monte Carlo (HMC) method.
It is a Markov Chain Monte Carlo (MCMC) sampling technique that merges Gibbs sampling, Hamiltonian dynamics with a final Metropolis-Hastings update.
It avoids random walk behavior by performing gradient-informed exploration of the typical set. 
We will begin with a survey of Hamiltonian dynamics followed by a description of
the \textit{Leapfrog} integrator which is used to simulate Hamiltonian systems. 
Once these are established, we will describe how HMC is constructed.

\section{Hamiltonian dynamics}\label{sec:hamiltonian_dynamics}
Hamiltonian dynamics \cite{classical_mechanics} plays a central part in the HMC algorithm. 
\begin{comment}
  \subsection{Lagrangian Mechanics}
Assume a set of \textit{generalized coordinates} $q = (q_1, \ldots, q_d)$. Generally, the Lagrangian can be written as
\begin{equation}
  L(q, \dot{q}, t) = K(q, \dot{q}, t) - V(q, \dot{q}, t),
\end{equation}
where $K$ is the kinetic energy and $V$ is the potential energy of the system. We shall restrict the treatment to the case where there is no explicit dependence on time $t$. The solutions $q(t)$ can be found by solving the \textit{Euler--Lagrange} equations given by
\begin{equation}
  \dv{}{t}\pdv{L}{\dot{q}_i} - \pdv{L}{q_i} = 0.
\end{equation}

\subsection{Hamiltonian Mechanics}
The Hamiltonian is constructed by the Legendre transformation,
\begin{equation}
  H(q, p, t) = \sum_i p_i \dot{q}_i(q, p) - L(q, \dot{q}(q, p), t),
\end{equation}
where
\begin{equation}
  p_i = \pdv{L}{\dot{q}_i},
\end{equation}
for which we introduce a collective notation $p = (p_1, \ldots, p_d)$.
The equations of motion, known as \textit{Hamilton's} equations, are given by
\begin{equation}\label{eq:eom}
  \dv{q_i}{t} = \pdv{H}{p_i}, \qquad \dv{p_i}{t} = - \pdv{H}{q_i}.
\end{equation}

For the purpose of utilizing this framework in the context of HMC, it's assumed that the form of the Lagrangian is
\begin{equation}
  L(q, \dot{q}) = K(\dot{q}) - V(q),
\end{equation}
where
\begin{equation}\label{eq:kinetic_energy}
  K(\dot{q}) = \sum_i \frac{1}{2}m_i\dot{q}^2_i.
\end{equation}
The generalized momentum of coordinate $q_i$ is
\begin{equation}
  p_i = \pdv{K}{\dot{q}_i} = m\dot{q}_i,
\end{equation}
from which it follows that the Legendre transformed kinetic energy can be written as
\begin{equation}\label{eq:kinetic_energy}
  K(p) = \sum_i \frac{p_i^2}{2m_i}.
\end{equation}
Finally, we can write down the Hamiltonian as
\begin{equation}\label{eq:hamiltonian}
  H(q, p) = K(p) + V(q) = \sum_i \frac{p_i^2}{2m_i} + V(q).
\end{equation}
\end{comment}
Let us assume that $q = (q_1, \ldots, q_d)$ are the \textit{generalized coordinates} and 
$p = (p_1, \ldots, p_d)$ and \textit{generalized momenta} of a physical system. We will further
assume that the Hamiltonian can be decomposed as
\begin{equation}\label{eq:general_hamiltonian}
  H(q, p) = V(q) + K(p),
\end{equation}
where $V$ denotes the \textit{potential} energy and $K$ denotes the \textit{kinetic} energy of the system. The time evolution of $q$ and $p$ are
found solving \textit{Hamilton's} equations,
\begin{equation}\label{eq:eom}
  \dv{q_i}{t} = \pdv{H}{p_i}, \qquad \dv{p_i}{t} = - \pdv{H}{q_i}.
\end{equation}
From Hamilton's equations in eq.~\eqref{eq:eom}, we can easily show that the Hamiltonian is conserved in time $t$ by
\begin{equation}
  \dv{H}{t} = \sum_i\left(\dv{q_i}{t}\pdv{H}{q_i} + \dv{p_i}{t}\pdv{H}{p_i}  \right)
  = \sum_i\left(\pdv{H}{p_i}\pdv{H}{q_i} - \pdv{H}{q_i}\pdv{H}{p_i}  \right) = 0.
\end{equation}
A numerical integrator must thus conserve this property. The class of numerical solvers that conserve the Hamiltonian is
called \textit{symplectic} integrators. The particular kind that HMC uses is called the \textit{Leapfrog} integrator which 
we will discuss next.

\subsection{Leapfrog integration}
The Leapfrog integrator\cite{leapfrog} is used in HMC to integrate eq.~\eqref{eq:eom} to generate new proposal states.
Assume we approximate the true coordinates and momenta by $(\hat{q}, \hat{p})$. A single Leapfrog integration step can then be written as in algorithm \ref{algo:leapfrog}. 
Here $\epsilon$ represents the stepsize used in the algorithm.
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{Leapfrog integration (single step)}\label{algo:leapfrog}
		\begin{algorithmic}
      \Procedure{LEAPFROG}{$q, p, \epsilon$} 
      \For{$i=1,\ldots, d$} \\
			\State $\hat{p}_i(t + \epsilon/2) = \hat{p}_i(t) - \displaystyle{\frac{\epsilon}{2}}\pdv*{V(\hat{q}(t))}{q_i} $\\
			\State $\hat{q_i}(t+\epsilon) = \hat{q}_i(t) + \displaystyle{\frac{\epsilon}{m_i}} \hat{p}_i(t+\epsilon/2)$\\
      \EndFor
      \For{$i=1,\ldots, d$} \\
      \State $\hat{p_i}(t+\epsilon) = \hat{p}_i(t + \epsilon/2) -  \displaystyle{\frac{\epsilon}{2}}\pdv*{V(\hat{q}(t+\epsilon))}{q_i}$ \\
      \EndFor
      \EndProcedure
		\end{algorithmic}
	\end{algorithm}
\end{figure}
It is common to set all masses $m_i = 1$, which simplifies Hamilton's equations somewhat and allows us to straight forwardly \textit{vectorize} our
leapfrog integrator.
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{Leapfrog integration (single step)}\label{algo:leapfrog}
		\begin{algorithmic}
      \Procedure{LEAPFROG}{$q, p, \epsilon$} 
      \\
			\State 1. $\hat{p}(t + \epsilon/2) = \hat{p}(t) - \displaystyle{\frac{\epsilon}{2}}\nabla_q V(\hat{q}(t))$\\
			\State 2. $\hat{q}(t+\epsilon) = \hat{q}(t) + \epsilon \hat{p}(t+\epsilon/2)$\\
			\State 3. $\hat{p}(t+\epsilon) = \hat{p}(t + \epsilon/2) -  \displaystyle{\frac{\epsilon}{2}}\nabla_q V(\hat{q}(t+\epsilon))$
      \EndProcedure
		\end{algorithmic}
	\end{algorithm}
\end{figure}

\section{HMC}
Hamiltonian Monte Carlo is largely developed by Radford Neal \cite{hmc}. We seek to sample from some target distribution which we express
in terms of a canonical distribution over the generalized coordinates
\begin{equation}\label{eq:canonical_q}
  \pi(q) = \exp\left(-V(q)\right).
\end{equation}
In other words, we can generically express any potential energy function as the negative logarithm of the target distribution,
\begin{equation}\label{eq:general_potential}
  V(q) = -\log \pi(q).
\end{equation}
To utilize the framework of Hamiltonian dynamics, we introduce \textit{auxilliary}
momentum variables $p$. This allows us to construct a canonical distribution
over \textit{phase-space} as
\begin{equation}
  \pi(q, p) = \exp\left(-H(q, p)\right).
\end{equation}
Using the form of the Hamiltonian introduced in eq.~\eqref{eq:general_hamiltonian}, we can recast the canonical distribution over phase-space as
\begin{equation}
  \pi(q, p) = \exp\left(-V(q)\right)\exp\left(-K(p)\right).
\end{equation}
The most common choice for the canonincal distribution over momentum space is a Gaussian which corresponds to the kinetic energy function
from classical physics
\begin{equation}\label{eq:kinetic_energy}
  K(p) = \sum_{i=1}^d \frac{p_i^2}{2m_i}.
\end{equation}
Before we summarize the algorithm in pseudocode, we should look at conceptual description of the algorithm.
\begin{enumerate}
  \item Given an initial state $q$, we randomly sample $p$ from a Gaussian distribution.
  \item The next step is to choose a direction in phase space, in which we sample $v \sim \text{Uniform}(\{-1, 1\})$.
  \item Now we perform $L$ leapfrog steps along the $v$ direction, for a total trajectory length of $v\epsilon L$.
  \item At this point, we have reached a proposal state in phase-space $(q^*, p^*)$ which we perform Metropolis correction,
  that is, we feed the proposal state and the initial state through the Metropolis-Hastings algorithm and accept the new
  state with a given acceptance probability.
\end{enumerate}
This sums up a single step in HMC which returns a position state. To sample multiple such points, we simply feed the returned position state $q$ back in to the machinery 
and redo the procedure.
The HMC scheme is summarized in algorithm \ref{algo:hmc}.
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{Hamiltonian Monte Carlo}\label{algo:hmc}
		\begin{algorithmic}
      \Procedure{HMCstep}{$L, q, p$}
      \State Sample $v \sim \text{Uniform}(\{-1, 1\})$.
      \State $(q^*, p^*) \leftarrow (q, p)$    \Comment{Start from initial state.}
      \State $p^* \leftarrow \text{GIBBS}(p^*)$
      \For{$l = 1, ..., L$} \Comment{$L$ Leapfrog steps.}
        \State $(q^*, p^*) \leftarrow$ LEAPFROG$(q^*, p^*, v)$
      \EndFor
      \State $P = \min \left(1, \exp\left[-\left(H(q^*,p^*) - H(q, p)\right)\right]\right)$
      \State Sample $u \sim U(0,1)$ \Comment{Uniform distribution on $(0,1)$.}
      \If {$P \geq u$}
        \State $(q, p) \gets (q^*, p^*)$ \Comment{Accept proposed state.}
      \Else
        \State $(q, p) \gets (q, p)$ \Comment{Reject proposed state.}
      \EndIf
      \EndProcedure
		\end{algorithmic}
	\end{algorithm}
\end{figure}

\section{The Potential Energy Function in Bayesian ML Applications}
We seek to use HMC in a Bayesian ML application. It is therefore important to
discuss a general way to construct the potential energy function in such applications.
First, recall from chapter \ref{chap:bayesian_ml} in eq.~\eqref{eq:posterior_function_of_loss} that the posterior could in general be written as
\begin{equation}
  p(\theta|D) \propto \exp\left(-\mathcal{L}(\theta)\right),
\end{equation}
where $\mathcal{L}$ was some loss function in the classical ML sense.
However, we do not need the evidence term and simply sample from the target distribution $\pi(\theta) = p(D|\theta)p(\theta)$ instead.
Comparison with eq.~\eqref{eq:canonical_q} makes it clear that the potential energy
function simply is $\mathcal{L}$. Combining this with eq.~\eqref{eq:loss_function_of_posterior}, lets us conclude that the general expression
for the potential energy is
\begin{equation}
  \mathcal{L} = -\log Z - \log p(D|\theta) - \log p(\theta),
\end{equation}
and if we assume all datapoints are i.i.d we can recast it as eq.~\eqref{eq:loss_function_of_posterior}, that is
\begin{equation}
  \mathcal{L} = -\log Z - \sum_{i=1}^N \log p(y^{(i)}|x^{(i)}, \theta) - \log p(\theta),
\end{equation}

\section{Limitations of HMC}
Although HMC is effective at exploring the state space we wish to sample from, it suffers from the need to hand-tune
the trajectory length $\epsilon L$. Poor choices of $\epsilon$ and $L$ can lead to a poor results.
On one hand, if the trajectory length is too short, exploration of the state space will be limited 
which makes HMC behave like a random-walk. Suppose we fix the trajectory length to a finite,
but sufficiently large value.
If the step size $\epsilon$ is too large,
it can lead to instabilities in the leapfrog integrator, while if its chosen to be too small, it will perform far too many
iterations to make the algorithm to be worthwhile. 
Tuning these parameters requires preliminary runs for a given problem. 

In the next chapter, we will study algorithms that adaptively sets the trajectory length of HMC,
namely the No-U-Turn sampler combined with dual-averaging of the step size, which allows us to overcome these
limitations and more effectively sample from the target distribution without the need for hand-tuning.

