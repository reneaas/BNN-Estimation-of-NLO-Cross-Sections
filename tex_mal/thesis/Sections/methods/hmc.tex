In this chapter, we will study the details of the Hamiltonian Monte Carlo (HMC) method.
It is a Markov Chain Monte Carlo (MCMC) sampling technique that merges Gibbs sampling, Hamiltonian dynamics with a final Metropolis-Hastings update.
It avoids random walk behaviour with a systematical exploration of the state space 
and generates successive samples with smaller correlation. We will begin with a survey of Lagrangian and Hamiltonian dynamics followed by a description of
the \textit{Leapfrog} integrator which is used to simulate the Hamiltonian systems. Once these are established, we will summarize the 
HMC method in a generic manner - applicable to any continuous distribution. Moreover, we will discuss important properties like conservation of the Hamiltonian
and local phase space volume.


\section{Hamiltonian dynamics}\label{sec:hamiltonian_dynamics}
Hamiltonian dynamics \cite{classical_mechanics} plays a central part in the HMC algorithm. 
For completeness, we will first survey Lagrangian mechanics from which we derive the Hamiltonian. 
The Hamiltonian then lays the foundation for Hamiltonian dynamics.

\subsection{Lagrangian Mechanics}
Assume a set of \textit{generalized coordinates} $q = (q_1, \ldots, q_d)$. Generally, the Lagrangian can be written as
\begin{equation}
  L(q, \dot{q}, t) = K(q, \dot{q}, t) - V(q, \dot{q}, t),
\end{equation}
where $K$ is the kinetic energy and $V$ is the potential energy of the system. We shall restrict the treatment to the case where there is no explicit dependence on time $t$. The solutions $q(t)$ can be found by solving the \textit{Euler--Lagrange} equations given by
\begin{equation}
  \dv{}{t}\pdv{L}{\dot{q}_i} - \pdv{L}{q_i} = 0.
\end{equation}

\subsection{Hamiltonian Mechanics}
The Hamiltonian is constructed by the Legendre transformation,
\begin{equation}
  H(q, p, t) = \sum_i p_i \dot{q}_i(q, p) - L(q, \dot{q}(q, p), t),
\end{equation}
where
\begin{equation}
  p_i = \pdv{L}{\dot{q}_i},
\end{equation}
for which we introduce a collective notation $p = (p_1, \ldots, p_d)$.
The equations of motion, known as \textit{Hamilton's} equations, are given by
\begin{equation}\label{eq:eom}
  \dv{q_i}{t} = \pdv{H}{p_i}, \qquad \dv{p_i}{t} = - \pdv{H}{q_i}.
\end{equation}

For the purpose of utilizing this framework in the context of HMC, it's assumed that the form of the Lagrangian is
\begin{equation}
  L(q, \dot{q}) = K(\dot{q}) - V(q),
\end{equation}
where
\begin{equation}\label{eq:kinetic_energy}
  K(\dot{q}) = \sum_i \frac{1}{2}m_i\dot{q}^2_i.
\end{equation}
The generalized momentum of coordinate $q_i$ is
\begin{equation}
  p_i = \pdv{K}{\dot{q}_i} = m\dot{q}_i,
\end{equation}
from which it follows that the Legendre transformed kinetic energy can be written as
\begin{equation}\label{eq:kinetic_energy}
  K(p) = \sum_i \frac{p_i^2}{2m_i}.
\end{equation}
Finally, we can write down the Hamiltonian as
\begin{equation}\label{eq:hamiltonian}
  H(q, p) = K(p) + V(q) = \sum_i \frac{p_i^2}{2m_i} + V(q).
\end{equation}
From Hamilton's equations in eq.~\ref{eq:eom}, we can easily show that the Hamiltonian is conserved in time $t$ by
\begin{equation}
  \dv{H}{t} = \sum_i\left(\dv{q_i}{t}\pdv{H}{q_i} + \dv{p_i}{t}\pdv{H}{p_i}  \right)
  = \sum_i\left(\pdv{H}{p_i}\pdv{H}{q_i} - \pdv{H}{q_i}\pdv{H}{p_i}  \right) = 0.
\end{equation}
This motives the need for a symplectic integrator, which we will discuss next.

\subsection{Leapfrog integration}
To run one step of HMC, we need to compute the time evolution of a Hamiltonian system of the form discussed in the former section,
where the neural network parameters will play the role as the generalized coordinates $q$.
The common choice of algorithm to integrate the equations of motion in eq.~\eqref{eq:eom} is \textit{Leapfrog} integration \cite{leapfrog}. This integrator is \textit{symplectic}, which means it conserves local volumes in phase space. This effectively translates to an approximately conserved value of $H(q,p)$ throughout a simulation, with slight oscillations about a mean value.

Assume we approximate the true coordinates and momenta by $(\hat{q}, \hat{p})$. A single Leapfrog integration step can then be written as in algorithm \ref{algo:leapfrog}. 
Here $\epsilon$ represents the stepsize used in the algorithm.
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{Leapfrog integration (single step)}\label{algo:leapfrog}
		\begin{algorithmic}
      \Procedure{LEAPFROG}{$q, p, \epsilon$} 
      \For{$i=1,\ldots, d$} \\
			\State 1. $\hat{p}_i(t + \epsilon/2) = \hat{p}_i(t) - \displaystyle{\frac{\epsilon}{2}}\pdv*{V(\hat{q}(t))}{q_i} $\\
			\State 2. $\hat{q_i}(t+\epsilon) = \hat{q}_i(t) + \displaystyle{\frac{\epsilon}{m_i}} \hat{p}_i(t+\epsilon/2)$\\
			\State 3. $\hat{p_i}(t+\epsilon) = \hat{p}_i(t + \epsilon/2) -  \displaystyle{\frac{\epsilon}{2}}\pdv*{V(\hat{q}(t+\epsilon))}{q_i}$ \\
      \EndFor
      \EndProcedure
		\end{algorithmic}
	\end{algorithm}
\end{figure}
It is common to set all masses $m_i = 1$, which simplifies Hamilton's equations somewhat an allows us to straight forwardly \textit{vectorize} our
leapfrog integrator.
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{Leapfrog integration (single step)}\label{algo:leapfrog}
		\begin{algorithmic}
      \Procedure{LEAPFROG}{$q, p, \epsilon$} 
      \\
			\State 1. $\hat{p}(t + \epsilon/2) = \hat{p}(t) - \displaystyle{\frac{\epsilon}{2}}\nabla_q V(\hat{q}(t))$\\
			\State 2. $\hat{q}(t+\epsilon) = \hat{q}(t) + \epsilon \hat{p}(t+\epsilon/2)$\\
			\State 3. $\hat{p}(t+\epsilon) = \hat{p}(t + \epsilon/2) -  \displaystyle{\frac{\epsilon}{2}}\nabla_q V(\hat{q}(t+\epsilon))$
      \EndProcedure
		\end{algorithmic}
	\end{algorithm}
\end{figure}

\subsection{Conservation of Local Phase-Space Volume}

\section{Hamiltonian Monte Carlo}
Hamiltonian Monte Carlo is largely developed and expanded upon by Radford Neal \cite{hmc}. The target distribution we wish sample from
is expressed in terms the Canonical distribution
\begin{equation}
  P(q) \propto \exp(-V(q)),
\end{equation}
where $q$ represents the parameters of the model. $V(q)$ can then always be expressed as
\begin{equation}
  V(q) = -\log Z - \log P(q)
\end{equation}
for some normalization constant $Z$. 
This constant plays no actual role in the sampling procedure as we only need to compute the difference $V(q') - V(q)$ at two points $q'$ and $q$.
We also need the gradient of $V$ with respect to $q$ where the constant term vanishes.
To utilize the framework of Hamiltonian dynamics explained in section \ref{sec:hamiltonian_dynamics}, we introduce momentum variables
$p_i$ such that we can express the total Hamiltonian as
\begin{equation}
  H(q, p) = K(p) + V(q),
\end{equation}
which we interpret as the negative log likelihood (up to a constant) of the canonical distribution over phase-space
\begin{equation}
  P(q, p) \propto \exp\left(-H(q,p)\right).
\end{equation}
Before we summarize the algorithm in pseudocode, we should look at conceptual description of the algorithm.
\begin{enumerate}
  \item Given an initial state $q$, we randomly sample $p$ from a Gaussian distribution.
  \item The next step is to choose a direction in phase space, in which we sample $v \sim \text{Uniform}(\{-1, 1\})$.
  \item Now we perform $L$ leapfrog steps along the $v$ direction, for a total trajectory length of $v\epsilon L$.
  \item At this point, we have reached a proposal state in phase-space $(q^*, p^*)$ which we perform Metropolis correction,
  that is, we feed the proposal state and the initial state through the Metropolis-Hastings algorithm and accept the new
  state with a given acceptance probability.
\end{enumerate}
This sums up a single step in HMC which returns a position state. To sample multiple such points, we simply feed the returned position state $q$ back in to the machinery 
and redo the procedure.
The HMC scheme is summarized in algorithm \ref{algo:hmc}.
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{Hamiltonian Monte Carlo}\label{algo:hmc}
		\begin{algorithmic}
      \Procedure{HMCstep}{$L, q, p$}
      \State Sample $v \sim \text{Uniform}(\{-1, 1\})$.
      \State $(q^*, p^*) \leftarrow (q, p)$    \Comment{Start from initial state.}
      \State $p^* \leftarrow \text{GIBBS}(p^*)$
      \For{$l = 1, ..., L$} \Comment{$L$ Leapfrog steps.}
        \State $(q^*, p^*) \leftarrow$ LEAPFROG$(q^*, p^*, v)$
      \EndFor
      \State $P = \min \left(1, \exp\left[-\left(H(q^*,p^*) - H(q, p)\right)\right]\right)$
      \State Sample $u \sim U(0,1)$ \Comment{Uniform distribution on $(0,1)$.}
      \If {$P \geq u$}
        \State $(q, p) \leftarrow (q^*, p^*)$ \Comment{Accept proposed state.}
      \Else
        \State $(q, p) \leftarrow (q, p)$ \Comment{Reject proposed state.}
      \EndIf
      \EndProcedure
		\end{algorithmic}
	\end{algorithm}
\end{figure}

\section{Limitations of HMC}
Although HMC is effective at exploring the state space we wish to sample from, it suffers from the need to hand-tune
the trajectory length $\epsilon L$. Poor choices of $\epsilon$ and $L$ can lead to a poor results.
On one hand, if the trajectory length is too short, exploration of the state space will be limited 
which makes HMC behave like a random-walk. Suppose we fix the trajectory length to a finite,
but sufficiently large value.
If the step size $\epsilon$ is too large,
it can lead to instabilities in the leapfrog integrator, while if its chosen to be too small, it will perform far too many
iterations to make the algorithm to be worthwhile. 
Tuning these parameters requires preliminary runs for a given problem. 

In the next chapter, we will study algorithms that adaptively sets the trajectory length of HMC,
namely the No-U-Turn sampler combined with dual-averaging of the step size, which allows us to overcome these
limitations and more effectively sample from the target distribution without the need for hand-tuning.

