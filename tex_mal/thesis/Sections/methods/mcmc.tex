In this chapter, we will review preliminary, general theory behind Markov chain Monte Carlo (MCMC) methods. We will start with an abstract view before we delve into
specific algorithms such as \textit{Gibbs} sampling and \textit{Metropolis-Hastings}. We will then discuss convergence diagnostics and metrics to assess the
quality of the samples obtained by the MCMC chain. The field is vast, so we cannot cover every nook and cranny. We will instead
focus on the parts that lay the foundation for the main algorithms used in this thesis, namely Hamiltonian Monte Carlo and the No-U-Turn sampler. 
Thus we will restrict our attention to samples from continuous distributions and ignore the theory for discrete spaces entirely.
This choice is one born out of healthy pragmatism.
We will not describe these algorithms in this chapter since the require some extra care and thus will have their own chapters devoted to them.

\section{Markov Chain Monte Carlo}\label{sec:mcmc}
\begin{comment}
  The treatment of Markov chains largely follows the presentation in \cite{markov_chains}. A Markov process is a scheme that from a given state (or value) $\mu$ generates a new state $\nu$ with a \textit{transition} probability $T(\nu|\mu)$. A Markov process has the following properties
\begin{itemize}
  \item The transition probabilities $T(\nu|\mu)$ are time-independent.
  \item $T(\nu|\mu)$ only depend on the states $\mu$ and $\nu$.
  \item The transition $\mu \to \mu$ is allowed, thus $T(\mu|\mu) > 0$. Hence, the new generated state may just be the same as the previous state.
  \item The transition probabilties must sum to unity, hence $\int_{\nu} T(\nu|\mu) = 1$. In other words, the procedure must generate some new state.
\end{itemize}

Two important principles, originally found by Metropolis et. al \cite{metropolis}, were introduced to make the MCMC chains computationally feasible. 
\begin{enumerate}
  \item \textbf{Ergodicity}: Any state $\nu$ can be reached from any other state $\mu$ given a long enough MCMC chain.
  \item \textbf{Detailed balance}: If the transition probabilities $T(\nu|\mu)$ obey the detailed balance equation, $$P(\mu)T(\nu|\mu) = P(\nu)T(\mu|\nu),$$
      then the Markov chain is guaranteed to be ergodic. The interpretation of this is that on average, the system makes the transition $\mu \to \nu$ just as often as the transition $\nu \to \mu$.
\end{enumerate}
\end{comment}

A \textit{Monte Carlo Markov chain} (MCMC) method is a scheme to sample points $\theta$ proportional to a distribution $\pi(\theta)$. It generates a new point $\theta_i$ given a point $\theta_{i-1}$. 
A \textit{Markov chain} is a sequence of points $\theta_1, \theta_2, \ldots,$ that are possibly dependent, but occur in the sequence in proportion to $\pi(\theta)$. Note that $\pi(\theta)$ here is not an exact probability distribution because it need not be normalized to unity.
However, suppose $P(\theta)$ is the underlying probability distribution, then $\pi(\theta) \propto P(\theta)$. Typically, in Bayesian applications, we have a prior $P(\theta)$ and a likelihood $P(D|\theta)$. In this case $\pi(\theta) = P(D|\theta)P(\theta)$ and 
$\pi(\theta) \propto P(\theta|D)$, that is, it's proportional to the posterior distribution.


A few important properties of the Markov chain, originally introduced by Metropolis et. al and built upon by Hastings \cite{metropolis}
is worth mentioning:
\begin{enumerate}
  \item \textbf{Ergodicity}. Each point $\theta_i$ is chosen from a distribution that only depends on the previous point in the sequence, $\theta_{i-1}$. For this, we introduce a transition probability that is  $T(\theta_i|\theta_{i-1})$. This ensures that any point $\theta$ can eventually be reached given a long enough sequence of samples \cite{numerical_recipies}. 
  \item \textbf{Detailed balance}. The transition probability is chosen to obey $$\pi(\theta)T(\theta'|\theta) = \pi(\theta')T(\theta|\theta'),$$ which ensures that the Markov chain is ergodic. Mathematically, we can express this condition as
  $$\pi(\theta') = \int \pi(\theta)T(\theta'|\theta)\dd\theta.$$
  \item We allow the transition $\theta \to \theta$, hence the transition probability $T(\theta|\theta)$ may be non-zero.
  \item The transition probablities integrate to unity, thus $$\int T(\theta'|\theta)\dd\theta = 1,$$
  reflecting the notion that some transition is guaranteed to occur.
  \item Finally, the transition probabilities are required to be time--independent.
\end{enumerate}


\section{Gibbs sampling}
Gibbs sampling \cite{gibbs} is a sampling technique used to generate a Markov chain sequence from an underlying multivariate distribution $P(\gamma)$ for a multi-dimensional parameter $\gamma = (\gamma_1, \ldots \gamma_d) \in \mathbb{R}^d$, for $d > 1$.
Suppose $\gamma^{(t)}$ represents the parameters at iteration $t$. Then the parameters $\gamma^{(t+1)}$ at iteration $t+1$ are generated from $\gamma^{(t)}$ by the following procedure.
\begin{figure}[H]
  \begin{algorithm}[H]
    \caption{Gibbs sampling}
    \begin{algorithmic}
      \Procedure{GIBBS}{$\gamma^{(t)}$}\\
        \State Sample $\gamma^{(t+1)}_1 \sim P(\gamma_1|\gamma_2^{(t)},...,\gamma_d^{(t)})$ \\
        \State Sample $\gamma^{(t+1)}_2 \sim P(\gamma_2|\gamma_1^{(t+1)},...,\gamma_d^{(t)})$\\
        \State $\vdots$ \qquad  \qquad $\vdots$ \qquad  \qquad $\vdots$ \qquad \qquad $\vdots$\\
        \State Sample $\gamma^{(t+1)}_d \sim P(\gamma_d|\gamma_1^{(t+1)},...,\gamma_{d-1}^{(t+1)})$\\
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{figure}
\noindent Thus each new sample $\gamma^{(t+1)}_i$ is only dependent on the prior state of the other parameters through
\begin{equation}
  \gamma_i^{(t+1)} \sim P(\gamma_i|\gamma_{1}^{(t+1)}, \ldots \gamma_{i-1}^{(t+1)}, \gamma_{i+1}^{(t)}, \ldots, \gamma_{d}^{(t)}),
\end{equation}
which by definition makes it a Markov chain.

\section{Metropolis-Hastings}
The Metropolis-Hastings algorithm \cite{metropolis} is a sampling algorithm based on random walks in parameter space used in MCMC chains to generate a new point $\theta'$ given a point $\theta$. Albeit efficient for some
problems, it's not a suitable sampling technique in the context of neural networks. 
The parameter space of neural networks is high-dimensional. Random walk exploration of said space will
yield highly correlated parameters per iteration. The random walk behaviour does not efficiently explore the parameter space.
However, a rudimentary understanding of the algorithm will be useful before we embark upon the HMC sampling algorithm, 
because the final update of the algorithm is by application of the Metropolis-Hastings algorithm.

The transition probability in the Metropolis algorithm is chosen to be
\begin{equation}
  T(\theta'|\theta) = q(\theta'|\theta)A(\theta,\theta'),
\end{equation}
where $q(\theta'|\theta)$ is called the proposal distribution and $A(\theta, \theta')$ is the acceptance probability given by
\begin{equation}
  A(\theta, \theta') = \min \left(1, \frac{\pi(\theta')q(\theta|\theta')}{\pi(\theta)q(\theta'|\theta)}\right).
\end{equation}
In the Metropolis-Hastings algorithm, a symmetry constraint is imposed on the proposal distribution such that 
$q(\theta'|\theta) = q(\theta|\theta')$. Thus the acceptance probability reduces to
\begin{equation}
  A(\theta, \theta') = \min \left(1, \frac{\pi(\theta')}{\pi(\theta)}\right).
\end{equation}
The point $\theta'$ is accepted with probability $A(\theta, \theta')$. 
\begin{figure}[H]
  \begin{algorithm}[H]
    \caption{Metropolis-Hastings}
    \begin{algorithmic}
      \Procedure{METROPOLIS-HASTINGS}{$\theta$}\\
      \State Sample $\theta' \sim q(\theta'|\theta)$\\
      \State $p \leftarrow \min \left(1, \frac{\pi(\theta')}{\pi(\theta)}\right)$\\
      \State Sample $u$ uniformly on $(0,1)$. \\
      \If {$p \geq u$}
        \State $\theta \leftarrow \theta'$ \Comment{Accept transition}
      \Else
        \State $\theta \leftarrow \theta$  \Comment{Reject transition}
      \EndIf\\
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{figure}



\section{Convergence diagnostics and strategies}
\subsection{Burn-in}
\subsection{Autocorrelation}
\subsection{Mixing}
\subsection{The Potential Scale Reduction Factor $\hat{R}$}

