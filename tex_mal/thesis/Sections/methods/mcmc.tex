In this chapter, we will review preliminary, general theory behind Markov chain Monte Carlo (MCMC) methods. We will start with an abstract view before we delve into
specific algorithms such as \textit{Gibbs} sampling and \textit{Metropolis-Hastings}. We will then discuss convergence diagnostics and metrics to assess the
quality of the samples obtained by the MCMC chain. The field is vast, so we cannot cover every nook and cranny. We will instead
focus on the parts that lay the foundation for the main algorithms used in this thesis, namely Hamiltonian Monte Carlo and the No-U-Turn sampler. 
Thus we will restrict our attention to samples from continuous distributions and ignore the theory for discrete spaces entirely.
This choice is one born out of healthy pragmatism.
We will not describe these algorithms in this chapter since they require some extra care and thus will have their own chapters devoted to them.

\begin{comment}
  \section{Curse of Dimensionality}
The \textit{curse of dimensionality} can be understood from a geometric perspective. 
Consider a \textit{density} $\pi(\theta)$ over a space $d$-dimensional space $Q$. Denote $\theta$ as any point in $Q$ and expectation of any function
$f(\theta)$ as $\expval{f}_\pi$ with respect to the density. Restricting our attention to smooth sample space, the expectation can be written as
\begin{equation}
  \expval{f}_\theta = \int \dd \theta \pi(\theta) f(\theta). 
\end{equation}  
\end{comment}

\section{Curse of Dimensionality}
The \textit{curse of dimensionality} is a pervasive problem in high-dimensional problems that can be understood from a geometric perspective. 
Consider a \textit{density} $\pi(\theta)$ over a $d$-dimensional sample space $Q$. Denote $\theta$ as any point in $Q$ and expectation of any function
$f(\theta)$ as $\expval{f}_\pi$ with respect to the density. Restricting our attention to smooth sample space, the expectation can be written as
\begin{equation}
  \expval{f}_\pi = \int \dd \theta \pi(\theta) f(\theta),
\end{equation}  
for some parametrization of $\theta$. The expectation itself is independent of the choice of parametrization. 



\section{Markov Chain Monte Carlo}\label{sec:mcmc}
\begin{comment}
  The treatment of Markov chains largely follows the presentation in \cite{markov_chains}. A Markov process is a scheme that from a given state (or value) $\mu$ generates a new state $\nu$ with a \textit{transition} probability $T(\nu|\mu)$. A Markov process has the following properties
\begin{itemize}
  \item The transition probabilities $T(\nu|\mu)$ are time-independent.
  \item $T(\nu|\mu)$ only depend on the states $\mu$ and $\nu$.
  \item The transition $\mu \to \mu$ is allowed, thus $T(\mu|\mu) > 0$. Hence, the new generated state may just be the same as the previous state.
  \item The transition probabilties must sum to unity, hence $\int_{\nu} T(\nu|\mu) = 1$. In other words, the procedure must generate some new state.
\end{itemize}

Two important principles, originally found by Metropolis et. al \cite{metropolis}, were introduced to make the MCMC chains computationally feasible. 
\begin{enumerate}
  \item \textbf{Ergodicity}: Any state $\nu$ can be reached from any other state $\mu$ given a long enough MCMC chain.
  \item \textbf{Detailed balance}: If the transition probabilities $T(\nu|\mu)$ obey the detailed balance equation, $$P(\mu)T(\nu|\mu) = P(\nu)T(\mu|\nu),$$
      then the Markov chain is guaranteed to be ergodic. The interpretation of this is that on average, the system makes the transition $\mu \to \nu$ just as often as the transition $\nu \to \mu$.
\end{enumerate}
\end{comment}

A \textit{Monte Carlo Markov chain} (MCMC) method is a scheme to sample points $\theta$ proportional to a distribution $\pi(\theta)$. It generates a new point $\theta_i$ given a point $\theta_{i-1}$. 
A \textit{Markov chain} is a sequence of points $\theta_1, \theta_2, \ldots,$ that are possibly dependent, but occur in the sequence in proportion to $\pi(\theta)$. Note that $\pi(\theta)$ here is not an exact probability distribution because it need not be normalized to unity.
However, suppose $P(\theta)$ is the underlying probability distribution, then $\pi(\theta) \propto P(\theta)$. Typically, in Bayesian applications, we have a prior $P(\theta)$ and a likelihood $P(D|\theta)$. In this case $\pi(\theta) = P(D|\theta)P(\theta)$ and 
$\pi(\theta) \propto P(\theta|D)$, that is, it's proportional to the posterior distribution.


A few important properties of the Markov chain, originally introduced by Metropolis et. al and built upon by Hastings \cite{metropolis}
is worth mentioning:
\begin{enumerate}
  \item \textbf{Ergodicity}. Each point $\theta_i$ is chosen from a distribution that only depends on the previous point in the sequence, $\theta_{i-1}$. For this, we introduce a transition probability that is  $T(\theta_i|\theta_{i-1})$. This ensures that any point $\theta$ can eventually be reached given a long enough sequence of samples \cite{numerical_recipies}. 
  \item \textbf{Detailed balance}. The transition probability is chosen to obey $$\pi(\theta)T(\theta'|\theta) = \pi(\theta')T(\theta|\theta'),$$ which ensures that the Markov chain is ergodic. Mathematically, we can express this condition as
  $$\pi(\theta') = \int \pi(\theta)T(\theta'|\theta)\dd\theta.$$
  \item We allow the transition $\theta \to \theta$, hence the transition probability $T(\theta|\theta)$ may be non-zero.
  \item The transition probablities integrate to unity, thus $$\int T(\theta'|\theta)\dd\theta = 1,$$
  reflecting the notion that some transition is guaranteed to occur.
  \item Finally, the transition probabilities are required to be time--independent.
\end{enumerate}


\section{Gibbs sampling}
Gibbs sampling \cite{gibbs} is a sampling technique used to generate a Markov chain sequence from an underlying multivariate distribution $P(\gamma)$ for a multi-dimensional parameter $\gamma = (\gamma_1, \ldots \gamma_d) \in \mathbb{R}^d$, for $d > 1$.
Suppose $\gamma^{(t)}$ represents the parameters at iteration $t$. Then the parameters $\gamma^{(t+1)}$ at iteration $t+1$ are generated from $\gamma^{(t)}$ by the following procedure.
\begin{figure}[H]
  \begin{algorithm}[H]
    \caption{Gibbs sampling}
    \begin{algorithmic}
      \Procedure{GIBBS}{$\gamma^{(t)}$}\\
        \State Sample $\gamma^{(t+1)}_1 \sim P(\gamma_1|\gamma_2^{(t)},...,\gamma_d^{(t)})$ \\
        \State Sample $\gamma^{(t+1)}_2 \sim P(\gamma_2|\gamma_1^{(t+1)},...,\gamma_d^{(t)})$\\
        \State $\vdots$ \qquad  \qquad $\vdots$ \qquad  \qquad $\vdots$ \qquad \qquad $\vdots$\\
        \State Sample $\gamma^{(t+1)}_d \sim P(\gamma_d|\gamma_1^{(t+1)},...,\gamma_{d-1}^{(t+1)})$\\
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{figure}
\noindent Thus each new sample $\gamma^{(t+1)}_i$ is only dependent on the prior state of the other parameters through
\begin{equation}
  \gamma_i^{(t+1)} \sim P(\gamma_i|\gamma_{1}^{(t+1)}, \ldots \gamma_{i-1}^{(t+1)}, \gamma_{i+1}^{(t)}, \ldots, \gamma_{d}^{(t)}),
\end{equation}
which by definition makes it a Markov chain.

\section{Metropolis-Hastings}
The Metropolis-Hastings algorithm \cite{metropolis} is a sampling algorithm based on random walks in parameter space used in MCMC chains to generate a new point $\theta'$ given a point $\theta$. Albeit efficient for some
problems, it's not a suitable sampling technique in the context of neural networks. 
The parameter space of neural networks is high-dimensional. Random walk exploration of said space will
yield highly correlated parameters per iteration. The random walk behaviour does not efficiently explore the parameter space.
However, a rudimentary understanding of the algorithm will be useful before we embark upon the HMC sampling algorithm, 
because the final update of the algorithm is by application of the Metropolis-Hastings algorithm.

The transition probability in the Metropolis algorithm is chosen to be
\begin{equation}
  T(\theta'|\theta) = q(\theta'|\theta)A(\theta,\theta'),
\end{equation}
where $q(\theta'|\theta)$ is called the proposal distribution and $A(\theta, \theta')$ is the acceptance probability given by
\begin{equation}
  A(\theta, \theta') = \min \left(1, \frac{\pi(\theta')q(\theta|\theta')}{\pi(\theta)q(\theta'|\theta)}\right).
\end{equation}
In the Metropolis-Hastings algorithm, a symmetry constraint is imposed on the proposal distribution such that 
$q(\theta'|\theta) = q(\theta|\theta')$. Thus the acceptance probability reduces to
\begin{equation}
  A(\theta, \theta') = \min \left(1, \frac{\pi(\theta')}{\pi(\theta)}\right).
\end{equation}
The point $\theta'$ is accepted with probability $A(\theta, \theta')$. 
\begin{figure}[H]
  \begin{algorithm}[H]
    \caption{Metropolis-Hastings}
    \begin{algorithmic}
      \Procedure{METROPOLIS-HASTINGS}{$\theta$}\\
      \State Sample $\theta' \sim q(\theta'|\theta)$\\
      \State $p \leftarrow \min \left(1, \frac{\pi(\theta')}{\pi(\theta)}\right)$\\
      \State Sample $u$ uniformly on $(0,1)$. \\
      \If {$p \geq u$}
        \State $\theta \leftarrow \theta'$ \Comment{Accept transition}
      \Else
        \State $\theta \leftarrow \theta$  \Comment{Reject transition}
      \EndIf\\
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{figure}



\section{Convergence diagnostics and strategies}
\subsection{Burn-in}
The concept of \textit{burn-in} plays a central role in MCMC methods. A MCMC chain is initiated from an initial point $\theta$, typically sampled at random. 
But this point may be far away from a high probability region, so it is customary to discard a finite number of samples from head of the chain
because it may not be particularly representative of the stationary distribution we wish to sample from. The number of samples discarded is called the \textit{burn-in time}. The remaining samples generated from
the chain, which we may call the \textit{tail} of the chain, is used to compute statistics of interest. 
\subsection{Mixing}
\textit{Mixing} is a concept used to describe whether the MCMC chain has converged to its target distribution. The \textit{mixing time} is the time it takes for the chain to be sufficiently close to its target distribution. As far as the I know, there exist no definite way to measure if a MCMC chain has converged, but there exist a metric that tells you when it has not, known as the \textit{potential scale reduction factor} $\hat{R}$.
\subsubsection{The Potential Scale Reduction Factor $\hat{R}$}
The prescription to compute $\hat{R}$ is a as follows: 
\begin{enumerate}
  \item Run $M$ independent chains where the $m$'th chain generates a sequence $\theta_1^m, \ldots, \theta_{N_m}^m$ once burn-in samples are discarded.
  \item Compute the posterior means of each chain given by
  \begin{equation}
    \hat{\theta}_m = \frac{1}{N_m}\sum_{i=1}^{N_m} \theta_i^m.
  \end{equation}
  \item Compute the variance of each chain:
  \begin{equation}
    \sigma_m^2 = \frac{1}{N_m - 1}\sum_{i=1}^{N_m}(\theta_i^m - \hat{\theta}_m)^2.
  \end{equation}
  \item Calculate the mean of the mean estimates from each chain,
  \begin{equation}
    \hat{\theta} = \frac{1}{M}\sum_{i=1}^M \hat{\theta}_m.
  \end{equation}
  \item Compute the variance about the joint mean:
  \begin{equation}
    B = \frac{N}{M-1}\sum_{m=1}^M (\hat{\theta}_m - \hat{\theta})^2.
  \end{equation}
  \item Compute the average of the variance of all $M$ chains,
  \begin{equation}
    W = \frac{1}{M}\sum_{m=1}^M \sigma_m^2.
  \end{equation}
  \item Define the estimator,
  \begin{equation}
    \hat{V} = \frac{N-1}{N}W + \frac{M+1}{MN}B.
  \end{equation}
  \item The potential scale reduction $\hat{R}$ is then
  \begin{equation}
    \hat{R} = \sqrt{\frac{\hat{V}}{W}}.
  \end{equation} 
\end{enumerate}
as the length of each chain $N_m \to \infty$, we expect $\hat{R} \to 1$, indicating that each independent chain has the same distribution.
Thus if $\hat{R} \approx 1$, one typically assumes the chain has converged. However, the diagnostic can really only be used to determine if the chains has not converged. From a practical point of view, a rule-of-thumb is used where if $\hat{R} < 1.1$, the chains are assumed to have converged. If $\hat{R} > 1.1$, a longer chain must be run to achieve the desired mixing.

\subsection{Correlation and Thinning}
Suppose burn-in samples are discared and we are left with a MCMC chain $\theta_1, \ldots, \theta_N$. Typically, the points in the chain are statistically \textit{correlated}, meaning they are not drawn independently from the target distribution. A common strategy
is to apply \textit{thinning}, effectively discarding a subset of samples inbetween points. The justification for this is
that nearby points in the chain provide sufficiently identical contributions to any computed statistic. In pratice, these points 
are never stored by defining a finite number of samples to skip between each stored sample.

\subsection{Common Practices}


