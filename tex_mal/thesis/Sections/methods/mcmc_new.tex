In this chapter, we will discuss fundamental ideas pertaining to \textit{Markov Chain Monte Carlo} (MCMC) methods.
We shall confine the discussion to continuous sample spaces which is the category of sample space focused on in this thesis.
We will commence with a discussion of expectation values and an important notion called the \textit{typical set}. 
We will then define and discuss Markov chains and Markov transitions after which we shall
discuss Metropolis-Hastings sampling and its limitations. 
We will adopt a geometric view to provide a natural transition to Hamiltonian Monte Carlo and the No-U-Turn sampler
in the two following chapters. The treatment will closely follow \cite{conceptual_intro_hmc}

\section{Expectation Values and the Typical Set}
Consider a probability density function $\pi(q)$ and a $d$-dimensional sample space $Q$ where $q \in Q$. Consider $f(q)$ to be an arbitrary smooth
function of $q$. The \textit{expectation value} of a \textit{target function} $f(q)$ with respect to the density $\pi(q)$ is then defined as
\begin{equation}\label{eq:expval}
    \mathbb{E}_\pi[f] = \int \dd q \ \pi(q) f(q).
\end{equation}
We shall interchangably refer to expectation values simply as \textit{expectations}.
For all but a few simple low-dimensional densities, computing eq.~\eqref{eq:expval} is impossible to evaluate analytically. 
For high-dimensional spaces, evaluating the expectation over the entire sample space is computationally infeasible.
Moreover, it is unlikely that the entire sample space contribute significantly to the expectation. Efficiently
evaluating the expectations may thus only require evaluation of the integrand in specific regions of the sample space.
For most purposes,the we are interested in the expectation of more than a single target function. For example, in Bayesian applications,
we are often interested in both mean and variance of a quantity which introduces the need for several target functions. 
Thus the numerical method should not depend on the target function in question. 
Instead the focus is laid on the contribution from $\pi(q)\dd q$ to
the integrand and which region of sample space that makes this quantity non-neglible. This region of sample space
is called the \textit{typical set}. Numerical methods that efficiently sample points from the typical set are what
we refer to as MCMC methods.

\subsection{The Typical Set}
MCMC methods are devised to efficiently sample points $q$ of the typical set. For simplicity, we can divide
sample space into three regions
\begin{enumerate}
    \item High-probability density region. These are regions in the neighborhood of any mode of the target density.
    \item The typical set. This refer to the regions in which $\pi(q) \dd q$ provides a non-neglible contribution
    to any expectation. This may be thought of as the high-probability region of the sample space
    since $\pi(q) \dd q$ is proportional to probability of a state $q$ in a volume $\dd q$. 
    \item Low-probability density regions. These are regions far away from any mode of the density.
\end{enumerate}
The first and third region will yield neglible contributions to an expectation.
The point to stress here is that the interesting part of sample space to explore is not the high-probability region, and is
not the part MCMC methods sample from. Instead, they sample from the region in which $\pi(q) \dd q$ is large.
Although the notion of a typical set can be formalized precisely, we will intentionally 
operate with this somewhat imprecise definition. For our purposes, it suffices to use it merely as
a conceptual notion to evaluate the quality of the samples stored from a MCMC chain. 


\section{Markov Chains and Markov Transitions}
Since evaluation of eq.~\eqref{eq:expval} in most interesting cases is intractible, we seek to evaluate them
from \textit{Markov chains}. A Markov chain is a sequence of points $q_1, ..., q_N$ generated sequentially
using a random map called a \textit{Markov transition}. A Markov transition is a conditional probability density
$T(q'|q)$ that yields the probability of transition from a point $q$ to $q'$. The Markov transition is often called
the \textit{transition kernel}, which is a term we will adopt. 

An arbitrary transition kernel is not useful because the generated Markov chain is unlikely to have any relation to the target distribution
of interest. To generate a useful Markov chain, we must use a transition kernel that preserves the target distribution. 
The condition imposed to ensure this is expressed as
\begin{equation}\label{eq:detailed_balance}
    \pi(q) = \int dq' \pi(q')T(q|q').
\end{equation}
The condition is formally called \textit{detailed balance}. The interpretation of the condition is that the Markov chain is reversible.
We can start from any $q$ and use the transition kernel to produce a set of new states. The distribution generated by the Markov chain
should be distributed according the target distribution regardless of which point we used to generate the chain from. A more important
fact is that as long as this condition is satisfied, the Markov chain will converge to and stay within the typical set.
The standard approach to approximate eq.~\eqref{eq:expval} is with the MCMC \textit{estimator}
\begin{equation}
    \hat{f}_N = \frac{1}{N}\sum_{i=1}^N f(q_i).
\end{equation}
For large enough $N$, the estimator will converge to the true expectation such that $\lim_{N\to\infty} \hat{f}_N = \mathbb{E}_\pi[f]$.
Obviously, the knowledge that the estimator will asymptotically converge to the true expectations are of limited use
when restricted to a practical computation in which only a finite chain can be generated. 
We are thus more interested in the properties of finite Markov chains.

\subsection{Ideal Markov Chains}
An ideal Markov chain can be divided into three phases.
\begin{enumerate}
    \item A convergence phase. The Markov chain is initiated from some point $q$ and the initially generated sequence
    lies in a region outside the typical set. Estimators evaluated using this part of the sequence are highly biased.
    \item An exploration phase. The Markov chain has reached the typical set and begins its first traversal of it.
    In this phase, estimators will rapidly converge towards their true values.
    \item A saturation phase. At this point, the Markov chain has explored most of the typical set and convergence
    of the estimators slow down significantly.
\end{enumerate}
The ideal evaluation of estimators thus only use the parts of Markov chain generated in the second and third phase, discarding the
the chain generated in the first phase. The notion of discarding the points from the first phase is called \textit{burn-in}.

\subsection{Pathologies}
Unfortunately, many target distributions embody typical sets with pathological regions where any 
transition kernel that obeys eq.~\eqref{eq:detailed_balance} is not sufficient to efficiently explore the typical set.
Geometrically, this can be regions in which the target distribution rapidly changes in certain localized regions of
the typical set.
The pathological regions can be completely ignored by the chain for much of the exploration,
leading to poor convergence and thus biased estimators. But as long as the transition kernel
satisfies detailed balance, we know for a fact that the estimators \textit{must} converge eventually.
Consequentially, the Markov chain partially be stuck near pathological regions for long periods to compensate
before it rapidly explores other parts of the typical set. This behaviour is repeated, 
which makes estimators oscillate. Regardless of when the MCMC chain is terminated, the estimator will likely be biased due
to this oscillating behaviour.

\subsection{Geometric Ergodicity}
Generation of ideal Markov chains is a certainty if the transition kernel satisfies \textit{geometric ergodicity} \cite{geometric_ergodicity}.
However, in most cases it is impossible to check that the condition is satisfied. Instead one uses a statistical quantitiy known
as the \textit{potential scale reduction factor} $\hat{R}$. The ideal value is $\hat{R} = 1$. For values far away from this target,
it is unlikely that geometric ergodicity is satisfied.



\section{Metropolis-Hastings}
Construction of a transition kernel that ensures convergence to the typical set of the target distribution is a non-trivial problem in general.
Fortunately, the Metropolis-Hastings algorithm provides a solution that lets us construct 
a transition kernel with this property \cite{metropolis,metropolis_two}. 
The algorithm consist of two components, a proposal of a new state and a correction step.
Given a state $q$, we propose a new state $q'$ by adding a random perturbation to the initial state.
The correction step rejects an proposed state that ends moves away from the typical set of the target distribution.
The proposed state is formally sampled from a proposal distribution $Q(q'|q)$. The so-called \textit{acceptance probability} $a(q'| q)$, 
that is the probability of accepting a proposed state $q'$ given an initial state $q$, is given by
\begin{equation}\label{eq:general_acceptance_prob}
    a(q'|q) = \min \left(1, \frac{Q(q|q')\pi(q')}{Q(q'|q)\pi (q)}\right).
\end{equation}

\begin{figure}[H]
    \begin{algorithm}[H]
      \caption{Metropolis-Hastings}\label{algo:general_metropolis}
      \begin{algorithmic}
        \Procedure{METROPOLIS-HASTINGS}{$q$}\\
        \State Sample $q' \sim Q(q'|q)$\\
        \State $\displaystyle{a(q'|q) \leftarrow \min \left(1, \frac{Q(q|q')\pi(q')}{Q(q'|q)\pi (q)}\right)}$\\
        \State Sample $u \sim \text{Uniform}(0,1)$. \\
        \If {$a(q'|q) \geq u$}
          \State $\theta \leftarrow \theta'$ \Comment{Accept transition}
        \Else
          \State $\theta \leftarrow \theta$  \Comment{Reject transition}
        \EndIf\\
        \EndProcedure
      \end{algorithmic}
    \end{algorithm}
  \end{figure}

There are many valid choices for proposal distribution. A common choice is a Gaussian distribution $Q(q'|q) = \mathcal{N}(q'|q, \Sigma)$
which we will refer to as \textit{random walk Metropolis}.
More precisely, this means that a proposed state is given by
\begin{equation}
    q' = q + r
\end{equation}
where $r \sim \mathcal{N}(0, I)$. This distribution is symmetric such that $Q(q'|q) = Q(q|q')$, implying that the acceptance probability
reduces to
\begin{equation}\label{eq:symmetric_acceptance_prob}
    a(q'|q) = \min \left(1, \frac{\pi(q')}{\pi(q)}\right).
\end{equation}
Hence, evaluation of the acceptance probability only requires evaluating the target distribution at the initial state and the proposed state.

\subsection{Random Walk Metropolis}
The random walk Metropolis algorithm does suffer from slow convergence to, and exploration of, the typical set in high-dimensional spaces.
This can be understood because of the following: a random perturbation will likely cause the proposed state
to lie outside the typical set, which leads to rejection of the proposed state. We can compensate
for this flaw by reducing the standard deviation of each dimension, but this will naturally lead to slow movement through
sample space. The slow exploration also leads to a Markov chain where consecutive embody
a relatively large measure of correlation. The effect is that the effective sample size grows slowly
and efficient evaluation of eq.~\eqref{eq:expval} becomes difficult.
Fortunately, there exists a solution; \textit{gradient-informed} exploration of the sample space, manifested in
the form of \textit{Hamiltonian Monte Carlo}. This algorithm is a special case of a Metropolis-Hastings algorithm
in which the proposal distribution $Q(q'|q)$ is a special one utilizing Hamiltonian dynamics and Gibbs sampling
to produce a new proposal state $q'$. This is the topic of the next chapter.

\section{Gibbs Sampling}
The final standard MCMC algorithm we need is the \textit{Gibbs} sampler. It plays a small
part of the sampling in HMC and so we should therefore briefly discuss it.
It is a MCMC sampling method used for multi-variate probability densities, and
so is only meaningful to discuss for $d > 1$ dimensions.
Suppose $\gamma^{(t)}$ represents the parameters at iteration $t$. 
The next sample $\gamma^{(t+1)}$ in the Markov chain is drawn according to a
conditional distribution as follows.
\begin{equation}\label{eq:gibbs_sampling}
  \gamma_i^{(t+1)} \sim P(\gamma_i|\gamma_{1}^{(t+1)}, \ldots \gamma_{i-1}^{(t+1)}, \gamma_{i+1}^{(t)}, \ldots, \gamma_{d}^{(t)}).
\end{equation}
We may summarize this as a function in algorithm~\ref{algo:gibbs} which 
given an initial state $\gamma^{(t)}$ returns a new state sampled according to eq.~\eqref{eq:gibbs_sampling}.
\begin{figure}[H]
  \begin{algorithm}[H]
    \caption{Gibbs sampling}\label{algo:gibbs}
    \begin{algorithmic}
      \Function{GIBBS}{$\gamma^{(t)}$}
        \For{$i=1,\ldots, d$}
          \State Sample $\gamma_i^{(t+1)} \sim P(\gamma_i|\gamma_{1}^{(t+1)}, \ldots \gamma_{i-1}^{(t+1)}, \gamma_{i+1}^{(t)}, \ldots, \gamma_{d}^{(t)}).$
        \EndFor \\
        \Return $\gamma^{(t+1)} = \left(\gamma_1^{(t+1)}, \ldots, \gamma_d^{(t+1)}\right)$.
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
\end{figure}


