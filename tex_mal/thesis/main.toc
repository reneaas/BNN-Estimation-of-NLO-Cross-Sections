\babel@toc {UKenglish}{}
\contentsline {chapter}{Introduction}{1}{chapter*.4}%
\contentsline {chapter}{\numberline {1}The Physics Problem}{3}{chapter.1}%
\contentsline {chapter}{\numberline {2}Bayesian Formulation of Machine Learning}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}The Core of Machine Learning}{5}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Loss Functions}{5}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Regularization}{6}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Optimization}{6}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Bayes' theorem}{6}{section.2.2}%
\contentsline {section}{\numberline {2.3}Bayesian Framework for Machine Learning}{7}{section.2.3}%
\contentsline {section}{\numberline {2.4}Bayesian Inference}{8}{section.2.4}%
\contentsline {chapter}{\numberline {3}Markov Chain Monte Carlo}{11}{chapter.3}%
\contentsline {section}{\numberline {3.1}Expectation Values and the Typical Set}{11}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}The Typical Set}{11}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}The Target Density and Bayesian Applications}{12}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Markov Chains and Markov Transitions}{12}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Ideal Markov Chains}{12}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Pathologies}{13}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Geometric Ergodicity and Convergence Diagnostics}{13}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Metropolis-Hastings}{13}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}The Proposal Distribution}{14}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Random Walk Metropolis}{14}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Gibbs Sampling}{14}{section.3.4}%
\contentsline {chapter}{\numberline {4}Hamiltonian Monte Carlo}{17}{chapter.4}%
\contentsline {section}{\numberline {4.1}Hamiltonian dynamics}{17}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Leapfrog integration}{17}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}HMC}{18}{section.4.2}%
\contentsline {section}{\numberline {4.3}The Potential Energy Function in Bayesian ML Applications}{19}{section.4.3}%
\contentsline {section}{\numberline {4.4}Limitations of HMC}{20}{section.4.4}%
\contentsline {chapter}{\numberline {5}The No-U-Turn Sampler}{21}{chapter.5}%
\contentsline {section}{\numberline {5.1}Preliminary definitions}{21}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Generation of Candidate Points and Stopping Criterion}{22}{subsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.1.1}Choosing candidate points}{22}{subsubsection.5.1.1.1}%
\contentsline {section}{\numberline {5.2}The Naive NUTS Algorithm}{23}{section.5.2}%
\contentsline {section}{\numberline {5.3}Efficient NUTS}{24}{section.5.3}%
\contentsline {section}{\numberline {5.4}Dual-Averaging Step Size Adaptation}{25}{section.5.4}%
\contentsline {section}{\numberline {5.5}NUTS with Dual-Averaging Step Size Adaptation}{25}{section.5.5}%
\contentsline {chapter}{\numberline {6}Bayesian Learning for Neural Networks}{27}{chapter.6}%
\contentsline {section}{\numberline {6.1}Neural Networks}{27}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Basic Mathematical Structure}{27}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Backpropagation}{28}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Loss Function for Regression}{28}{subsection.6.1.3}%
\contentsline {subsection}{\numberline {6.1.4}Regularization in Neural Networks}{29}{subsection.6.1.4}%
\contentsline {section}{\numberline {6.2}Activation Functions}{30}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Sigmoid}{30}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}ReLU}{30}{subsection.6.2.2}%
\contentsline {subsection}{\numberline {6.2.3}Swish}{30}{subsection.6.2.3}%
\contentsline {section}{\numberline {6.3}Bayesian Framework for Neural Networks}{30}{section.6.3}%
\contentsline {section}{\numberline {6.4}Bayesian learning using HMC}{31}{section.6.4}%
\contentsline {chapter}{\numberline {7}Numerical Experiments}{33}{chapter.7}%
\contentsline {section}{\numberline {7.1}The Dataset}{33}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Data Generation}{33}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Data Scaling and Transformations}{33}{subsection.7.1.2}%
\contentsline {section}{\numberline {7.2}Performance Metrics}{33}{section.7.2}%
\contentsline {section}{\numberline {7.3}Results}{33}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Benchmarks of Hyperparameters}{33}{subsection.7.3.1}%
\contentsline {subsubsection}{\numberline {7.3.1.1}Baseline Model}{33}{subsubsection.7.3.1.1}%
\contentsline {subsubsection}{\numberline {7.3.1.2}Pretraining}{33}{subsubsection.7.3.1.2}%
\contentsline {subsubsection}{\numberline {7.3.1.3}Burn-in length}{33}{subsubsection.7.3.1.3}%
\contentsline {subsubsection}{\numberline {7.3.1.4}Number of model parameters}{33}{subsubsection.7.3.1.4}%
\contentsline {subsection}{\numberline {7.3.2}Neutralino-Neutralino Cross Sections}{33}{subsection.7.3.2}%
\contentsline {chapter}{\numberline {8}Discussion}{35}{chapter.8}%
\contentsline {chapter}{Conclusion}{38}{chapter*.15}%
\contentsline {chapter}{Appendices}{39}{section*.16}%
\contentsline {chapter}{Appendix A}{41}{appendix*.17}%
\contentsline {section}{\numberline {A.1}Appendix 1 title }{41}{section.Alph0.1}%
